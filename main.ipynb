{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liberaries init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mne\n",
    "import torch\n",
    "from torch.utils.data import DataLoader ,  Dataset\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#global use\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participant_files_from_dir(dir_path: Path) -> dict[str ,  List[Path] ]:\n",
    "    \"\"\"\n",
    "    from participants directory returns files path orginized in relation to task and run \n",
    "    exemple \n",
    "    dic = {\n",
    "    \"contrastchangeDetection_run-1\" : [path1,path2,path3],\n",
    "    ...   \n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    dir_path = os.path.join(dir_path , \"eeg\")\n",
    "    participant_files = {}\n",
    "    for file in os.listdir(dir_path):\n",
    "        elements = file.split(\"_\")\n",
    "        if len(elements ) == 3 :\n",
    "            elements.insert(2 , \"run-1\")\n",
    "        elements[1] += \"_\"+ elements[2]\n",
    "        del elements[2]\n",
    "        if elements[1] not in participant_files:\n",
    "            participant_files[elements[1]] = [Path(os.path.join(dir_path , file))]\n",
    "        else:\n",
    "            participant_files[elements[1]].append(Path(os.path.join(dir_path , file)))\n",
    "    # aka each task has the events , channels , eeg json and eeg raw\n",
    "    for key in participant_files  :\n",
    "        assert len(participant_files[key]) == 4 \n",
    "    return participant_files\n",
    "def prepare_ccd_events(events_fp : Path) -> DataFrame:\n",
    "    \"\"\"\n",
    "    from events file returns a dataframe with trial start , trial end , stimulas start , action onset , RT AND SUCCESS\n",
    "\n",
    "    \"\"\"\n",
    "    assert os.path.splitext(events_fp)[1] == \".tsv\"\n",
    "    events = pd.read_csv(events_fp , sep = \"\\t\")\n",
    "    events[\"onset\"] = pd.to_numeric(events[\"onset\"],errors=\"raise\")   \n",
    "    events = events.reset_index(drop=True)\n",
    "    events = events.sort_values(by=\"onset\" , ascending=True)\n",
    "    trials = events[ events[\"value\"] == \"contrastTrial_start\"].copy()\n",
    "    trials[\"trial_start\"] = trials[\"onset\"]\n",
    "\n",
    "    trials[\"trial_end\"] = trials[\"onset\"].shift(-1) \n",
    "    stimulas = events [ events[\"value\"].isin([\"right_target\" ,\"left_target\"])].copy()\n",
    "    action = events [ events[\"value\"].isin([\"right_buttonPress\" ,\"left_buttonPress\"])].copy()\n",
    "    results = []\n",
    "    for i in range(0 ,len(trials)-1 ):\n",
    "        #get the stimulas onset in the trial i duration\n",
    "        stimulas_row = stimulas[ (stimulas[\"onset\"] >= trials[\"trial_start\"].iloc[i]) & (stimulas[\"onset\"] < trials[\"trial_end\"].iloc[i]) ]\n",
    "        if stimulas_row.empty:\n",
    "            continue\n",
    "        stimulas_start = float(stimulas_row[\"onset\"].iloc[0])\n",
    "\n",
    "        action_rows = action[ (action[\"onset\"] >= stimulas_start) & (action[\"onset\"] < trials[\"trial_end\"].iloc[i]) ]\n",
    "        # if theres no action , theres no rt , theres no success\n",
    "        if action_rows.empty:\n",
    "            \n",
    "            continue\n",
    "        action_row = action_rows.iloc[0]\n",
    "        action_onset = float(action_row[\"onset\"])\n",
    "        rt = action_onset - stimulas_start\n",
    "        success = 1 if action_row[\"feedback\"] == \"smiley_face\" else 0\n",
    "        result ={\n",
    "        \"trial_start\" : float(trials[\"trial_start\"].iloc[i]) ,\n",
    "        \"trial_end\" :float(trials[\"trial_end\"].iloc[i]) ,\n",
    "        \"stimulas_start\" : stimulas_start,\n",
    "        \"action_onset\" :action_onset  ,\n",
    "        \"rt\" : rt ,\n",
    "        \"success\" : success\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_participants_ccd_data(data_dir: Path) -> Dict[str , Dict[str , Tuple[DataFrame , Path]]]:\n",
    "\n",
    "    #dictionary that will have for each  participant (path as key) a dictionary with the ccd-run (key) and values as the df , and path to raw eeg file \n",
    "    results = {}\n",
    "    for release in os.listdir(data_dir):\n",
    "        release_dir_path = os.path.join(data_dir , release)\n",
    "        #go throught the participants directory\n",
    "        for file in os.listdir(release_dir_path):\n",
    "            \n",
    "            if not  file.split(\"-\")[0] == \"sub\" :\n",
    "                continue\n",
    "\n",
    "            participant_id = file\n",
    "            participant_dir_path = os.path.join(release_dir_path , file)\n",
    "\n",
    "            results[participant_dir_path] = {}\n",
    "            participant_files = load_participant_files_from_dir(participant_dir_path)\n",
    "            filtered_participant_files = {}\n",
    "            # filter for ccd and sus data\n",
    "            for key in participant_files:\n",
    "                if key.split(\"_\")[0].lower() == \"task-contrastchangedetection\" :\n",
    "                    filtered_participant_files[key] = participant_files[key]\n",
    "            \n",
    "            for task , files in filtered_participant_files.items():\n",
    "                events_path = [path for path in files if \"events\" in str(path)]  \n",
    "                assert len(events_path) == 1\n",
    "                events_path = events_path[0]\n",
    "                eeg_path = [path for path in files if \".set\" in str(path)]\n",
    "\n",
    "                assert len(eeg_path) == 1\n",
    "                df = prepare_ccd_events(events_path)\n",
    "                results[participant_dir_path][task] = (df , eeg_path[0])\n",
    "    return results\n",
    "\n",
    "def participants_ccd_data_to_list(data : dict) -> List[Tuple[DataFrame , Path]]:\n",
    "    results = []\n",
    "    for participant in data:\n",
    "        for task in data[participant]:\n",
    "            results.append(data[participant][task])\n",
    "    return results\n",
    "            \n",
    "def participants_ccd_list_to_trial_rt_pair(data : List[Tuple[DataFrame , Path]]) -> List[Tuple[Path ,Tuple[float,float] , float]]:\n",
    "    results = []\n",
    "    for participant in data:\n",
    "        df , eeg_path = participant\n",
    "        for i in range(0 , len(df)):\n",
    "            results.append((eeg_path , (df[\"stimulas_start\"].iloc[i]+0.5 , df[\"stimulas_start\"].iloc[i]+2.5 ) , df[\"rt\"].iloc[i]))\n",
    "    return results\n",
    "\n",
    "def train_val_test_split_by_subject(data : List[Tuple[Path ,Tuple[float,float] , float]] , test_size : float = 0.1 , val_size : float = 0.1) -> Tuple[List[Tuple[Path ,Tuple[float,float] , float]] , List[Tuple[Path ,Tuple[float,float] , float]] , List[Tuple[Path ,Tuple[float,float] , float]]]:\n",
    "    subjects = []\n",
    "    for element in data:\n",
    "        path = element[0]\n",
    "        subject = str(path).split('\\\\')[-3]\n",
    "        if subject not in subjects:\n",
    "            subjects.append(subject)\n",
    "    train_subjects , test_subjects = train_test_split(subjects , test_size=test_size +val_size)\n",
    "    test_subjects , val_subjects = train_test_split(test_subjects , test_size=val_size/(test_size +val_size))\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    val_data = []\n",
    "    for element in data:\n",
    "        path = element[0]\n",
    "        subject = str(path).split('\\\\')[-3]\n",
    "        if subject in train_subjects:\n",
    "            train_data.append(element)\n",
    "        elif subject in test_subjects:\n",
    "            test_data.append(element)\n",
    "        elif subject in val_subjects:\n",
    "            val_data.append(element)\n",
    "    return train_data , val_data , test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727\n"
     ]
    }
   ],
   "source": [
    "data_path= r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\data\"\n",
    "data = prepare_participants_ccd_data(data_path)\n",
    "\n",
    "data = participants_ccd_data_to_list(data)\n",
    "data = participants_ccd_list_to_trial_rt_pair(data)\n",
    "train , val , test = train_val_test_split_by_subject(data)\n",
    "print(len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting the raw eeg windows data and setting them up for fast import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_to_fast_loading_shards(data : List[Tuple[Path ,Tuple[float,float] , float]] , split = \"train\" , shard_size : int = 5000):\n",
    "    shards_path = \"shards_dir\"\n",
    "    if not os.path.exists(shards_path):\n",
    "        os.makedirs(shards_path)\n",
    "\n",
    "    split_path = os.path.join(shards_path , split)\n",
    "    if not os.path.exists(split_path):\n",
    "        os.makedirs(split_path)\n",
    "    else:\n",
    "        return split_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self , data ):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        eeg_path , (start , end) , rt = self.data[index]\n",
    "        eeg = mne.io.read_raw_eeglab(eeg_path , preload=True)\n",
    "        eeg = eeg.crop(start , end+0.2)\n",
    "        raw = eeg.get_data()\n",
    "        raw = torch.tensor(raw , dtype=torch.float)[:,:200]\n",
    "        rt = torch.tensor(rt , dtype=torch.float)\n",
    "\n",
    "        return raw , rt\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 200])\n",
      "tensor(1.9680)\n"
     ]
    }
   ],
   "source": [
    "data_dir = data_path\n",
    "orig_participants_data= prepare_participants_ccd_data(data_dir)\n",
    "participants_data = participants_ccd_data_to_list(orig_participants_data)\n",
    "trial_rt_pairs = participants_ccd_list_to_trial_rt_pair(participants_data)\n",
    "\n",
    "train_pairs , val_pairs , test_pairs = train_val_test_split_by_subject(trial_rt_pairs)\n",
    "train_data = EEGDataset(train_pairs)\n",
    "test_data = EEGDataset(test_pairs)\n",
    "val_data = EEGDataset(val_pairs)\n",
    "eeg , rt=train_data[0]\n",
    "print(eeg.shape)\n",
    "print(rt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse_over_data(model, dataloader, device):\n",
    "    model.eval()\n",
    "    se_sum = 0.0     # sum of squared errors\n",
    "    sum_y = 0.0      # sum of y\n",
    "    sum_y2 = 0.0     # sum of y^2\n",
    "    n = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x).view_as(y)\n",
    "            diff = y_pred - y\n",
    "\n",
    "            se_sum += diff.pow(2).sum().item()\n",
    "            sum_y  += y.sum().item()\n",
    "            sum_y2 += y.pow(2).sum().item()\n",
    "            n += y.numel()\n",
    "\n",
    "    rmse = (se_sum / n) ** 0.5\n",
    "    var  = (sum_y2 / n) - (sum_y / n) ** 2\n",
    "    std  = var ** 0.5\n",
    "    return rmse / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCCDmodel(nn.Module):\n",
    "    def __init__(self , nb_channels = 129 , nb_times= 200 , nb_output = 1):\n",
    "        super().__init__()\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nb_channels * nb_times , 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128 , 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64 , 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32 , nb_output)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        return self.classification_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "\n",
    "train_dataloader = DataLoader(train_data , batch_size=batch_size , shuffle=True )\n",
    "test_dataloader = DataLoader(test_data , batch_size=batch_size , shuffle=True )\n",
    "val_dataloader = DataLoader(val_data , batch_size=batch_size , shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blmodel = BaselineCCDmodel().to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(blmodel.parameters() , lr=lr )\n",
    "loss_f = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 62/68 [03:49<00:22,  3.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m blmodel.train()\n\u001b[32m     12\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m , index):\n\u001b[32m      9\u001b[39m     eeg_path , (start , end) , rt = \u001b[38;5;28mself\u001b[39m.data[index]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     eeg = \u001b[43mmne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_raw_eeglab\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     eeg = eeg.crop(start , end+\u001b[32m0.2\u001b[39m)\n\u001b[32m     12\u001b[39m     raw = eeg.get_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\mne\\io\\eeglab\\eeglab.py:328\u001b[39m, in \u001b[36mread_raw_eeglab\u001b[39m\u001b[34m(input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;129m@fill_doc\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_raw_eeglab\u001b[39m(\n\u001b[32m    286\u001b[39m     input_fname,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     verbose=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    292\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mRawEEGLAB\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Read an EEGLAB .set file.\u001b[39;00m\n\u001b[32m    294\u001b[39m \n\u001b[32m    295\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m \u001b[33;03m    .. versionadded:: 0.11.0\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRawEEGLAB\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43meog\u001b[49m\u001b[43m=\u001b[49m\u001b[43meog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43muint16_codec\u001b[49m\u001b[43m=\u001b[49m\u001b[43muint16_codec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmontage_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmontage_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-269>:12\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\mne\\io\\eeglab\\eeglab.py:459\u001b[39m, in \u001b[36mRawEEGLAB.__init__\u001b[39m\u001b[34m(self, input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    453\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe number of trials is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meeg.trials\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. It must be 1 for raw\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    454\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m files. Please use `mne.io.read_epochs_eeglab` if\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m the .set file contains epochs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m     )\n\u001b[32m    458\u001b[39m last_samps = [eeg.pnts - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m info, eeg_montage, _ = \u001b[43m_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meog\u001b[49m\u001b[43m=\u001b[49m\u001b[43meog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmontage_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmontage_units\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# read the data\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eeg.data, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\mne\\io\\eeglab\\eeglab.py:230\u001b[39m, in \u001b[36m_get_info\u001b[39m\u001b[34m(eeg, eog, montage_units)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eeg_has_ch_names_info:\n\u001b[32m    229\u001b[39m     has_pos = _eeg_has_montage_information(eeg)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     ch_names, ch_types, eeg_montage = \u001b[43m_get_montage_information\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmontage_units\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmontage_units\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     update_ch_names = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# if eeg.chanlocs is empty, we still need default chan names\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\mne\\io\\eeglab\\eeglab.py:210\u001b[39m, in \u001b[36m_get_montage_information\u001b[39m\u001b[34m(eeg, get_pos, montage_units)\u001b[39m\n\u001b[32m    201\u001b[39m         _check_head_radius(mean_radius, add_info=additional_info)\n\u001b[32m    203\u001b[39m     montage = make_dig_montage(\n\u001b[32m    204\u001b[39m         ch_pos=\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ch_names, pos_array)),\n\u001b[32m    205\u001b[39m         coord_frame=\u001b[33m\"\u001b[39m\u001b[33mhead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m         nasion=nasion,\n\u001b[32m    209\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[43m_ensure_fiducials_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmontage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    212\u001b[39m     montage = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\mne\\_fiff\\_digitization.py:278\u001b[39m, in \u001b[36m_ensure_fiducials_head\u001b[39m\u001b[34m(dig)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m fids:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m radius \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    277\u001b[39m         radius = [\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m             \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dig\n\u001b[32m    280\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[33m\"\u001b[39m\u001b[33mcoord_frame\u001b[39m\u001b[33m\"\u001b[39m] == FIFF.FIFFV_COORD_HEAD\n\u001b[32m    281\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isnan(d[\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m]).any()\n\u001b[32m    282\u001b[39m         ]\n\u001b[32m    283\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m radius:\n\u001b[32m    284\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# can't complete, no head points\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2744\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2742\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2743\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2744\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "blmodel = BaselineCCDmodel().to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(blmodel.parameters() , lr=lr )\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    blmodel.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = blmodel(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_dataloader)} , nRMSE : {nrmse_over_data(blmodel , train_dataloader ,device)}\")\n",
    "    blmodel.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = blmodel(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_dataloader)} , nRMSE : {nrmse_over_data(blmodel , test_dataloader,device)}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Baseline model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
