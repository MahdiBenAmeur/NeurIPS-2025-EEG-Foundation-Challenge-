{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liberaries init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mne\n",
    "import torch\n",
    "from torch.utils.data import DataLoader ,  Dataset\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR ,    MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#global use\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_AND_CHECKPOINTS_PATH = r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\models_and_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make it deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deterministic(seed: int = 42):\n",
    "    import os, random, numpy as np, torch\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # CUDA determinism\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "make_deterministic(42)\n",
    "g = torch.Generator().manual_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ccd Data preproccessing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participant_files_from_dir(dir_path: Path) -> dict[str ,  List[Path] ]:\n",
    "    \"\"\"\n",
    "    from participants directory returns files path orginized in relation to task and run \n",
    "    exemple \n",
    "    dic = {\n",
    "    \"contrastchangeDetection_run-1\" : [path1,path2,path3],\n",
    "    ...   \n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    dir_path = os.path.join(dir_path , \"eeg\")\n",
    "    participant_files = {}\n",
    "    for file in os.listdir(dir_path):\n",
    "        elements = file.split(\"_\")\n",
    "        if len(elements ) < 3 :\n",
    "            print(elements)\n",
    "            continue\n",
    "        if len(elements ) == 3 :\n",
    "            elements.insert(2 , \"run-1\")\n",
    "        elements[1] += \"_\"+ elements[2]\n",
    "        del elements[2]\n",
    "        if elements[1] not in participant_files:\n",
    "            participant_files[elements[1]] = [Path(os.path.join(dir_path , file))]\n",
    "        else:\n",
    "            participant_files[elements[1]].append(Path(os.path.join(dir_path , file)))\n",
    "    # aka each task has the events , channels , eeg json and eeg raw\n",
    "    for key in participant_files  :\n",
    "        assert len(participant_files[key]) == 4 \n",
    "    return participant_files\n",
    "def prepare_ccd_events(events_fp : Path) -> DataFrame:\n",
    "    \"\"\"\n",
    "    from events file returns a dataframe with trial start , trial end , stimulas start , action onset , RT AND SUCCESS\n",
    "\n",
    "    \"\"\"\n",
    "    assert os.path.splitext(events_fp)[1] == \".tsv\"\n",
    "    events = pd.read_csv(events_fp , sep = \"\\t\")\n",
    "    events[\"onset\"] = pd.to_numeric(events[\"onset\"],errors=\"raise\")   \n",
    "    events = events.reset_index(drop=True)\n",
    "    events = events.sort_values(by=\"onset\" , ascending=True)\n",
    "    trials = events[ events[\"value\"] == \"contrastTrial_start\"].copy()\n",
    "    trials[\"trial_start\"] = trials[\"onset\"]\n",
    "\n",
    "    trials[\"trial_end\"] = trials[\"onset\"].shift(-1) \n",
    "    stimulas = events [ events[\"value\"].isin([\"right_target\" ,\"left_target\"])].copy()\n",
    "    action = events [ events[\"value\"].isin([\"right_buttonPress\" ,\"left_buttonPress\"])].copy()\n",
    "    results = []\n",
    "    for i in range(0 ,len(trials)-1 ):\n",
    "        #get the stimulas onset in the trial i duration\n",
    "        stimulas_row = stimulas[ (stimulas[\"onset\"] >= trials[\"trial_start\"].iloc[i]) & (stimulas[\"onset\"] < trials[\"trial_end\"].iloc[i]) ]\n",
    "        if stimulas_row.empty:\n",
    "            continue\n",
    "        stimulas_start = float(stimulas_row[\"onset\"].iloc[0])\n",
    "\n",
    "        action_rows = action[ (action[\"onset\"] >= stimulas_start) & (action[\"onset\"] < trials[\"trial_end\"].iloc[i]) ]\n",
    "        # if theres no action , theres no rt , theres no success\n",
    "        if action_rows.empty:\n",
    "            \n",
    "            continue\n",
    "        action_row = action_rows.iloc[0]\n",
    "        action_onset = float(action_row[\"onset\"])\n",
    "        rt = action_onset - stimulas_start\n",
    "        success = 1 if action_row[\"feedback\"] == \"smiley_face\" else 0\n",
    "        result ={\n",
    "        \"trial_start\" : float(trials[\"trial_start\"].iloc[i]) ,\n",
    "        \"trial_end\" :float(trials[\"trial_end\"].iloc[i]) ,\n",
    "        \"stimulas_start\" : stimulas_start,\n",
    "        \"action_onset\" :action_onset  ,\n",
    "        \"rt\" : rt ,\n",
    "        \"success\" : success\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_participants_ccd_data(data_dir: Path) -> Dict[str , Dict[str , Tuple[DataFrame , Path]]]:\n",
    "\n",
    "    #dictionary that will have for each  participant (path as key) a dictionary with the ccd-run (key) and values as the df , and path to raw eeg file \n",
    "    results = {}\n",
    "    for release in os.listdir(data_dir):\n",
    "        release_dir_path = os.path.join(data_dir , release)\n",
    "        #go throught the participants directory\n",
    "        for file in os.listdir(release_dir_path):\n",
    "            \n",
    "            if not  file.split(\"-\")[0] == \"sub\" :\n",
    "                continue\n",
    "\n",
    "            participant_id = file\n",
    "            participant_dir_path = os.path.join(release_dir_path , file)\n",
    "\n",
    "            results[participant_dir_path] = {}\n",
    "            participant_files = load_participant_files_from_dir(participant_dir_path)\n",
    "            filtered_participant_files = {}\n",
    "            # filter for ccd and sus data\n",
    "            for key in participant_files:\n",
    "                if key.split(\"_\")[0].lower() == \"task-contrastchangedetection\" :\n",
    "                    filtered_participant_files[key] = participant_files[key]\n",
    "            \n",
    "            for task , files in filtered_participant_files.items():\n",
    "                events_path = [path for path in files if \"events\" in str(path)]  \n",
    "                assert len(events_path) == 1\n",
    "                events_path = events_path[0]\n",
    "                eeg_path = [path for path in files if \".bdf\" in str(path)]\n",
    "\n",
    "                assert len(eeg_path) == 1\n",
    "                df = prepare_ccd_events(events_path)\n",
    "                results[participant_dir_path][task] = (df , eeg_path[0])\n",
    "    return results\n",
    "\n",
    "def participants_ccd_data_to_list(data : dict) -> List[Tuple[DataFrame , Path]]:\n",
    "    results = []\n",
    "    for participant in data:\n",
    "        for task in data[participant]:\n",
    "            results.append(data[participant][task])\n",
    "    return results\n",
    "            \n",
    "def participants_ccd_list_to_trial_rt_pair(data : List[Tuple[DataFrame , Path]]) -> List[Tuple[Path ,Tuple[float,float] , float]]:\n",
    "    results = []\n",
    "    for participant in data:\n",
    "        df , eeg_path = participant\n",
    "        for i in range(0 , len(df)):\n",
    "            results.append((eeg_path , (df[\"stimulas_start\"].iloc[i]+0.5 , df[\"stimulas_start\"].iloc[i]+2.5 ) , df[\"rt\"].iloc[i]))\n",
    "    return results\n",
    "\n",
    "def train_val_test_split_by_subject(data : List[Tuple[Path ,Tuple[float,float] , float]] , test_size : float = 0.1 , val_size : float = 0.1) -> Tuple[List[Tuple[Path ,Tuple[float,float] , float]] , List[Tuple[Path ,Tuple[float,float] , float]] , List[Tuple[Path ,Tuple[float,float] , float]]]:\n",
    "    subjects = []\n",
    "    for element in data:\n",
    "        path = element[0]\n",
    "        subject = str(path).split('\\\\')[-3]\n",
    "        if subject not in subjects:\n",
    "            subjects.append(subject)\n",
    "    train_subjects , test_subjects = train_test_split(subjects , test_size=test_size +val_size)\n",
    "    test_subjects , val_subjects = train_test_split(test_subjects , test_size=val_size/(test_size +val_size))\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    val_data = []\n",
    "    for element in data:\n",
    "        path = element[0]\n",
    "        subject = str(path).split('\\\\')[-3]\n",
    "        if subject in train_subjects:\n",
    "            train_data.append(element)\n",
    "        elif subject in test_subjects:\n",
    "            test_data.append(element)\n",
    "        elif subject in val_subjects:\n",
    "            val_data.append(element)\n",
    "    return train_data , val_data , test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sedQjjK1A']\n",
      "84341\n"
     ]
    }
   ],
   "source": [
    "data_path= r\"D:\\projects\\pytorch_training\\data_1\"\n",
    "data = prepare_participants_ccd_data(data_path)\n",
    "\n",
    "data = participants_ccd_data_to_list(data)\n",
    "data = participants_ccd_list_to_trial_rt_pair(data)\n",
    "train , val , test = train_val_test_split_by_subject(data)\n",
    "print(len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting the raw eeg windows data and setting them up for fast import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_to_fast_loading_shards(data : List[Tuple[Path ,Tuple[float,float] , float]] , split = \"train\" , shard_size : int = 1000 ,shards_path= \"ccd_shards_dir\"):\n",
    "    split_path = os.path.join(shards_path , split)\n",
    "\n",
    "    if not os.path.exists(shards_path):\n",
    "        os.makedirs(shards_path)\n",
    "\n",
    "    if not os.path.exists(split_path):\n",
    "        os.makedirs(split_path)\n",
    "    else:\n",
    "        return split_path\n",
    "\n",
    "\n",
    "    shard_index=0\n",
    "    window_shard_path = os.path.join(split_path , f\"window_shard_{shard_index}.npy\")\n",
    "    rt_shard_path = os.path.join(split_path , f\"rt_shard_{shard_index}.npy\")\n",
    "    windows =[]\n",
    "    rts =[]\n",
    "    print(f\"proccessing {split} data , length = {len(data)}\")\n",
    "    for index , element in tqdm(enumerate(data)):\n",
    "        if index!=0 and index % shard_size == 0:\n",
    "            window_array = np.array(windows)\n",
    "            rt_array = np.array(rts)\n",
    "            X = np.lib.format.open_memmap(window_shard_path , dtype='float32' , mode='w+' , shape=(window_array.shape[0] , window_array.shape[1] , window_array.shape[2]))\n",
    "            X[:] = window_array\n",
    "            del X\n",
    "            Y = np.lib.format.open_memmap(rt_shard_path , dtype='float32' , mode='w+' , shape=(rt_array.shape[0] ,))\n",
    "            Y[:] = rt_array\n",
    "            del Y\n",
    "\n",
    "            print(\"saved_file\")\n",
    "            shard_index+=1\n",
    "            window_shard_path = os.path.join(split_path , f\"window_shard_{shard_index}.npy\")\n",
    "            rt_shard_path = os.path.join(split_path , f\"rt_shard_{shard_index}.npy\")\n",
    "            windows =[]\n",
    "            rts =[]\n",
    "        eeg_path , (start , end) , rt = element\n",
    "        eeg = mne.io.read_raw_bdf(eeg_path, preload=True, verbose=False)\n",
    "        eeg = eeg.crop(start , end)\n",
    "        raw = eeg.get_data()[: ,:200]\n",
    "        windows.append(raw)\n",
    "        rts.append(rt)\n",
    "    window_array = np.array(windows)\n",
    "    rt_array = np.array(rts)\n",
    "    if len(window_array) > 0:\n",
    "        X = np.lib.format.open_memmap(window_shard_path , dtype='float32' , mode='w+' , shape=(window_array.shape[0] , window_array.shape[1] , window_array.shape[2]))\n",
    "        X[:] = window_array\n",
    "        del X\n",
    "        Y = np.lib.format.open_memmap(rt_shard_path , dtype='float32' , mode='w+' , shape=(rt_array.shape[0] ,))\n",
    "        Y[:] = rt_array\n",
    "        del Y\n",
    "    return split_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m shard_size = \u001b[32m1000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pairs_to_fast_loading_shards(\u001b[43mtrain\u001b[49m , \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m ,shard_size )\n\u001b[32m      3\u001b[39m pairs_to_fast_loading_shards(test , \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,shard_size )\n\u001b[32m      4\u001b[39m pairs_to_fast_loading_shards(val , \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m,shard_size )\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "shard_size = 1000\n",
    "pairs_to_fast_loading_shards(train , \"train\" ,shard_size )\n",
    "pairs_to_fast_loading_shards(test , \"test\",shard_size )\n",
    "pairs_to_fast_loading_shards(val , \"val\",shard_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self , shards_path , shard_size  =1000 , device = \"cuda\"  , split = \"train\"):\n",
    "        self.shards_path = os.path.join(shards_path , split)\n",
    "        self.numbers_of_shards = int( len(os.listdir(self.shards_path)) / 2)\n",
    "        self.shard_size = shard_size\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        length = 0\n",
    "        for shard_path in os.listdir(self.shards_path):\n",
    "            if \"window\" in shard_path:\n",
    "                window_shard_path = os.path.join(self.shards_path , shard_path)\n",
    "                shard = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "                size = shard.shape[0]\n",
    "                length += size\n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        shard_pos = index // self.shard_size \n",
    "        window_shard_path = os.path.join(self.shards_path , f\"window_shard_{shard_pos}.npy\")\n",
    "        X = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "        rt_shard_path = os.path.join(self.shards_path , f\"rt_shard_{shard_pos}.npy\")\n",
    "        Y = np.lib.format.open_memmap(rt_shard_path , mode=\"r\")\n",
    "        raw = X[index % self.shard_size]\n",
    "        rt = Y[index % self.shard_size]\n",
    "        tensor_raw = torch.tensor(raw , dtype=torch.float32).to(device)\n",
    "        tensor_rt = torch.tensor(rt , dtype=torch.float32).to(device)\n",
    "        return tensor_raw , tensor_rt\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 200])\n",
      "tensor(2.1300, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shard_size = 1000\n",
    "shards_path= r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\ccd_shards_dir_full\"\n",
    "\"\"\"train_pairs_shards_path = pairs_to_fast_loading_shards([] , \"train\" , shard_size)\n",
    "test_pairs_shards_path = pairs_to_fast_loading_shards([] , \"test\" ,  shard_size)\n",
    "val_pairs_shards_path =pairs_to_fast_loading_shards([] , \"val\" , shard_size)\"\"\"\n",
    "train_ccd_data = EEGDataset(shards_path ,  shard_size ,device , split=\"train\")\n",
    "test_ccd_data = EEGDataset(shards_path , shard_size , device , split=\"test\")\n",
    "val_ccd_data = EEGDataset(shards_path ,shard_size , device  , split=\"val\")\n",
    "eeg , rt=train_ccd_data[0]\n",
    "print(eeg.shape)\n",
    "print(rt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nrmse_over_data(model, dataloader, device):\n",
    "    model.eval()\n",
    "    se_sum = 0.0     # sum of squared errors\n",
    "    sum_y = 0.0      # sum of y\n",
    "    sum_y2 = 0.0     # sum of y^2\n",
    "    n = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=False):\n",
    "            index = 0 \n",
    "\n",
    "            for x, y in tqdm(dataloader):\n",
    "\n",
    "                x = x.to(device, dtype=torch.float32)\n",
    "                y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "                y_pred = model(x).view_as(y)\n",
    "                diff = y_pred - y\n",
    "\n",
    "                se_sum += diff.pow(2).sum().item()\n",
    "                sum_y  += y.sum().item()\n",
    "                sum_y2 += y.pow(2).sum().item()\n",
    "                n += y.numel()\n",
    "\n",
    "    rmse = (se_sum / n) ** 0.5\n",
    "    var  = (sum_y2 / n) - (sum_y / n) ** 2\n",
    "    std  = var ** 0.5\n",
    "    return rmse / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_ccd_dataloader = DataLoader(train_ccd_data , batch_size=batch_size , shuffle=True )#, generator=g )\n",
    "test_ccd_dataloader = DataLoader(test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "val_ccd_dataloader = DataLoader(val_ccd_data , batch_size=batch_size , shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(200)\n",
    "\n",
    "        self.channel_pooling = nn.Sequential(\n",
    "            nn.Linear(129 , 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64,32)\n",
    "\n",
    "        )\n",
    "        self.pooling = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5),\n",
    "            nn.SiLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 32, kernel_size=5),\n",
    "            nn.SiLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 *47 , 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512 , 1)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.channel_pooling(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.pooling(x)\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blmodel = BaseLineModel().to(device)\n",
    "\n",
    "lr = 5e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(blmodel.parameters() , lr=lr   )\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer , patience=1 )\n",
    "\n",
    "loss_f = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1051 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:25<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 1 , loss : 0.19554375654545486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:10<00:00, 12.62it/s]\n",
      "100%|██████████| 134/134 [00:06<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.16729712636390728 , nRMSE : 1.0018320363860007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:42<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 2 , loss : 0.17730106859265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:10<00:00, 12.67it/s]\n",
      "100%|██████████| 134/134 [00:06<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , loss : 0.1683270329637314 , nRMSE : 1.0048415389932366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:26<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 3 , loss : 0.17403457830567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:10<00:00, 12.67it/s]\n",
      "100%|██████████| 134/134 [00:06<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 3 , loss : 0.16762836488770017 , nRMSE : 1.002736276817248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:37<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 4 , loss : 0.17082426015404606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:10<00:00, 13.37it/s]\n",
      "100%|██████████| 134/134 [00:06<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 4 , loss : 0.1695336680025307 , nRMSE : 1.0085948037244818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:38<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 5 , loss : 0.1708341516256616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:12<00:00, 10.33it/s]\n",
      "100%|██████████| 134/134 [00:09<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 5 , loss : 0.1695660641277904 , nRMSE : 1.0085880018699778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 486/1051 [00:48<00:56,  9.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m blmodel.train()\n\u001b[32m     10\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ccd_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     21\u001b[39m X = np.lib.format.open_memmap(window_shard_path , mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m rt_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrt_shard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m Y = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_memmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrt_shard_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m raw = X[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m     25\u001b[39m rt = Y[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:971\u001b[39m, in \u001b[36mopen_memmap\u001b[39m\u001b[34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001b[39m\n\u001b[32m    968\u001b[39m         offset = fp.tell()\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Read the header of the file first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_check_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#print(\"before training\")\n",
    "#print(f\"train nRMSE : {nrmse_over_data(blmodel , train_ccd_dataloader ,device)}\")\n",
    "#print(f\"test nRMSE : {nrmse_over_data(blmodel , test_ccd_dataloader ,device)}\")\n",
    "\n",
    "epochs = 30\n",
    "with torch.autocast(device_type=\"cuda\", enabled=False):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        blmodel.train()\n",
    "        cumulative_loss = 0\n",
    "        for  batch in tqdm(train_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = blmodel(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #nrmse_train = nrmse_over_data(blmodel , train_ccd_dataloader ,device)\n",
    "        #if lr!= optimizer.param_groups[0]['lr']:\n",
    "        #    print(f\"new lr : {optimizer.param_groups[0]['lr']}\")\n",
    "        #    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)}\")# , nRMSE : {nrmse_train}\")\n",
    "        blmodel.eval()\n",
    "        with torch.inference_mode():\n",
    "            cumulative_loss = 0\n",
    "            for batch in tqdm(val_ccd_dataloader):\n",
    "                x , y = batch\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = blmodel(x)\n",
    "                loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "                cumulative_loss += loss.item()\n",
    "            nrmse_over_val = nrmse_over_data(blmodel , val_ccd_dataloader ,device)\n",
    "            scheduler1.step(nrmse_over_val)\n",
    "            print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(val_ccd_dataloader)} , nRMSE : {nrmse_over_val}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:07<00:00, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.000476755054369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nrmse_test=nrmse_over_data(blmodel , test_ccd_dataloader ,device)\n",
    "print(f\"test nRMSE : {nrmse_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.000476755054369\n"
     ]
    }
   ],
   "source": [
    "print(f\"test nRMSE : {nrmse_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final test on r5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_r5 = r\"D:\\projects\\pytorch_training\\test_data\"\n",
    "data = prepare_participants_ccd_data(path_to_r5)\n",
    "\n",
    "data = participants_ccd_data_to_list(data)\n",
    "data = participants_ccd_list_to_trial_rt_pair(data)\n",
    "\n",
    "final_r5_test =pairs_to_fast_loading_shards(data ,\"final_r5_test\" , shard_size)\n",
    "r5_test_data = EEGDataset(final_r5_test ,  shard_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 test nRMSE : 1.0011487058935713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r5_test_dataloader = DataLoader(r5_test_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(blmodel , r5_test_dataloader,device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:01<00:00, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 test RMSE : 0.3627938385318386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_rmse(blmodel , dataloader , device):\n",
    "    blmodel.eval()\n",
    "    se_sum = 0.0   # sum of squared errors\n",
    "    n = 0          # total number of elements\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for x, y in tqdm(dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = blmodel(x).squeeze(-1)\n",
    "            diff = y_pred - y\n",
    "\n",
    "            se_sum += (diff ** 2).sum().item()\n",
    "            n += y.numel()\n",
    "\n",
    "    rmse = np.sqrt(se_sum / n)\n",
    "    return rmse\n",
    "\n",
    "print(f\"r5 test RMSE : {calculate_rmse(blmodel , r5_test_dataloader,device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full data preproccessing for encoder training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:53: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_15092\\4045812165.py:53: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def load_participant_eeg_files_from_dir(dir_path: Path) -> dict[str ,  List[Path] ]:\n",
    "    \"\"\"\n",
    "    from participants directory returns files path orginized in relation to task and run \n",
    "    exemple \n",
    "    dic = {\n",
    "    \"contrastchangeDetection_run-1\" : [path1,path2,path3],\n",
    "    ...   \n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    dir_path = os.path.join(dir_path , \"eeg\")\n",
    "    participant_files = {}\n",
    "    for file in os.listdir(dir_path):\n",
    "        if os.path.splitext(file)[1] != \".bdf\":\n",
    "            continue\n",
    "        elements = file.split(\"_\")\n",
    "        if len(elements ) == 3 :\n",
    "            elements.insert(2 , \"run-1\")\n",
    "        elements[1] += \"_\"+ elements[2]\n",
    "        del elements[2]\n",
    "        participant_files[elements[1]] = Path(os.path.join(dir_path , file))\n",
    "\n",
    "\n",
    "    return participant_files\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_participants_eeg_data(data_dir: Path) -> Dict[str , Dict[str , Tuple[DataFrame , Path]]]:\n",
    "\n",
    "    #dictionary that will have for each  participant (path as key) a dictionary with the ccd-run (key) and values as the df , and path to raw eeg file \n",
    "    results = {}\n",
    "    for release in os.listdir(data_dir):\n",
    "        release_dir_path = os.path.join(data_dir , release)\n",
    "        #go throught the participants directory\n",
    "        for file in os.listdir(release_dir_path):\n",
    "            \n",
    "            if not  file.split(\"-\")[0] == \"sub\" :\n",
    "                continue\n",
    "\n",
    "            participant_id = file\n",
    "            participant_dir_path = os.path.join(release_dir_path , file)\n",
    "\n",
    "            participant_files = load_participant_eeg_files_from_dir(participant_dir_path)\n",
    "            results[participant_dir_path] = [list(participant_files.values())]\n",
    "        \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def get_subject_metadata_from_path(data_path: Path):\n",
    "    \"\"\"\n",
    "    Given a path to an EEG .bdf file, locate the participants.tsv file\n",
    "    in the release folder (e.g. R7_L100_bdf) and return the metadata\n",
    "    for that subject ID.\n",
    "\n",
    "    Example:\n",
    "        path = Path(r\"D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARTY010EYP\\eeg\\sub-NDARTY010EYP_task-contrastChangeDetection_run-2_eeg.bdf\")\n",
    "        meta = get_subject_metadata_from_path(path)\n",
    "    \"\"\"\n",
    "    # Extract release folder (e.g. \"R7_L100_bdf\")\n",
    "    release_dir = data_path.parents[2]  # two levels up: /R7_L100_bdf/sub-XXX/eeg/file.bdf\n",
    "    tsv_path = release_dir / \"participants.tsv\"\n",
    "\n",
    "    if not tsv_path.exists():\n",
    "        raise FileNotFoundError(f\"participants.tsv not found in {release_dir}\")\n",
    "\n",
    "    # Extract subject ID\n",
    "    subject_id = data_path.name.split(\"_\")[0]  # e.g. \"sub-NDARTY010EYP\"\n",
    "\n",
    "    # Read and locate row\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "    row = df.loc[df[\"participant_id\"] == subject_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Subject {subject_id} not found in {tsv_path}\")\n",
    "\n",
    "    # Convert row to dictionary\n",
    "    meta = row.iloc[0].to_dict()\n",
    "    meta[\"release_dir\"] = str(release_dir)\n",
    "    return meta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_val_test_split_by_subject_full_data(data : Dict[str , List[Path]], test_size = 0.1 , val_size = 0.1) -> Tuple[List[Path] , List[Path] , List[Path]]:\n",
    "    subjects = list(data.keys())\n",
    "    train , test = train_test_split(subjects , test_size=test_size + val_size  )\n",
    "    test , val = train_test_split(test , test_size = val_size/(test_size + val_size) )\n",
    "    train_data_paths =[]\n",
    "    for subject in train:\n",
    "        for element in data[subject]:\n",
    "            train_data_paths.extend(element)\n",
    "    test_data_paths = []\n",
    "    for subject in test:\n",
    "        for element in data[subject]:\n",
    "            test_data_paths.extend(element)\n",
    "    val_data_paths = []\n",
    "    for subject in val:\n",
    "        for element in data[subject]:\n",
    "            val_data_paths.extend(element)\n",
    "    return train_data_paths , test_data_paths , val_data_paths  \n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_and_shard_eeg_data(data_paths: List[Path], split_type=\"train\", shard_size=1000,\n",
    "                            window_size=200, stride=20, output_dir=\"full_eeg_data_shards\") -> Tuple[Path, int]:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    shard_path = os.path.join(output_dir, split_type)\n",
    "    if not os.path.exists(shard_path):\n",
    "        os.mkdir(shard_path)\n",
    "\n",
    "    eeg_arrays = []\n",
    "    current_size = 0\n",
    "    index = 0\n",
    "    full_number_of_windows = 0\n",
    "    full_dict = {}\n",
    "\n",
    "    print(f\"processing {len(data_paths)} files\")\n",
    "    for i, data_path in tqdm(enumerate(data_paths)):\n",
    "        file_meta_data = get_subject_metadata_from_path(Path(data_path))\n",
    "        participant_id = file_meta_data[\"participant_id\"]\n",
    "\n",
    "        file_name = Path(data_path).name\n",
    "        task_name = file_name.split(\"_task-\")[1].split(\"_\")[0]\n",
    "\n",
    "        if participant_id not in full_dict:\n",
    "            full_dict[participant_id] = {}\n",
    "            full_dict[participant_id][\"meta\"] = file_meta_data\n",
    "            full_dict[participant_id][\"limits\"] = {\"Start\": full_number_of_windows, \"End\": None}\n",
    "            full_dict[participant_id][\"tasks\"] = {}\n",
    "\n",
    "        if task_name not in full_dict[participant_id][\"tasks\"]:\n",
    "            full_dict[participant_id][\"tasks\"][task_name] = {\"Start\": full_number_of_windows, \"End\": None}\n",
    "        raw = mne.io.read_raw_bdf(data_path, preload=False, verbose=False)\n",
    "        n_samples = raw.n_times\n",
    "        chunk_len = window_size * 100\n",
    "        for start in range(0, n_samples, chunk_len):\n",
    "            stop = min(start + chunk_len, n_samples)\n",
    "            data_chunk = raw.get_data(start=start, stop=stop).astype(np.float32)\n",
    "            n_windows = data_chunk.shape[1] // window_size\n",
    "            if n_windows == 0:\n",
    "                continue\n",
    "            data_chunk = data_chunk[:, : n_windows * window_size]\n",
    "            data_chunk = data_chunk.reshape(data_chunk.shape[0], n_windows, window_size).transpose(1, 0, 2)\n",
    "            if data_chunk.shape[1] != 129:\n",
    "                print(f\"Skipping {data_path}, wrong channel count {data_chunk.shape[1]}\")\n",
    "                continue\n",
    "\n",
    "            eeg_arrays.append(data_chunk)\n",
    "            current_size += data_chunk.shape[0]\n",
    "            full_number_of_windows += data_chunk.shape[0]\n",
    "            full_dict[participant_id][\"limits\"][\"End\"] = full_number_of_windows\n",
    "            full_dict[participant_id][\"tasks\"][task_name][\"End\"] = full_number_of_windows\n",
    "\n",
    "            if current_size >= shard_size:\n",
    "                new_eeg_data_windows = np.concatenate(eeg_arrays, axis=0)\n",
    "                to_be_added = new_eeg_data_windows[:shard_size]\n",
    "                X = np.lib.format.open_memmap(\n",
    "                    os.path.join(shard_path, f\"window_shard_{index}.npy\"),\n",
    "                    mode=\"w+\", dtype=np.float32,\n",
    "                    shape=(to_be_added.shape[0], to_be_added.shape[1], to_be_added.shape[2])\n",
    "                )\n",
    "                X[:] = to_be_added\n",
    "                X.flush()\n",
    "                del X, new_eeg_data_windows, to_be_added\n",
    "                eeg_arrays = []\n",
    "                gc.collect()\n",
    "                current_size = 0\n",
    "                index += 1\n",
    "\n",
    "        del raw\n",
    "        gc.collect()\n",
    "\n",
    "    if eeg_arrays:\n",
    "        new_eeg_data_windows = np.concatenate(eeg_arrays, axis=0)\n",
    "        to_be_added = new_eeg_data_windows[:shard_size]\n",
    "        X = np.lib.format.open_memmap(\n",
    "            os.path.join(shard_path, f\"window_shard_{index}.npy\"),\n",
    "            mode=\"w+\", dtype=np.float32,\n",
    "            shape=(to_be_added.shape[0], to_be_added.shape[1], to_be_added.shape[2])\n",
    "        )\n",
    "        X[:] = to_be_added\n",
    "        X.flush()\n",
    "        index += 1\n",
    "        full_number_of_windows += to_be_added.shape[0]\n",
    "        del X, new_eeg_data_windows, to_be_added\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    torch.save(full_dict, os.path.join(shard_path, \"full_dict.pt\"))\n",
    "    return shard_path, full_number_of_windows\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\projects\\pytorch_training\\data_1\"\n",
    "output_dir = r\"D:\\projects\\pytorch_training\\full_eeg_data_shards\"\n",
    "detailed_data = prepare_participants_eeg_data(path)\n",
    "train_data_paths , test_data_paths , val_data_paths = train_val_test_split_by_subject_full_data(detailed_data)\n",
    "\n",
    "train_data_paths = sorted(train_data_paths, key=lambda p: p.name.split(\"_\")[0])\n",
    "test_data_paths = sorted(test_data_paths, key=lambda p: p.name.split(\"_\")[0])\n",
    "val_data_paths = sorted(val_data_paths, key=lambda p: p.name.split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_15092\\361888402.py:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  test_path = Path(\"D:\\projects\\pytorch_training\\data_1\\R1_L100_bdf\\sub-NDARCZ947WU5\\eeg\\sub-NDARCZ947WU5_task-surroundSupp_run-1_eeg.bdf\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'participant_id': 'sub-NDARCZ947WU5',\n",
       " 'release_number': 'R1',\n",
       " 'sex': 'F',\n",
       " 'age': 9.3646,\n",
       " 'ehq_total': 80.04,\n",
       " 'commercial_use': 'Yes',\n",
       " 'full_pheno': 'Yes',\n",
       " 'p_factor': -0.355,\n",
       " 'attention': 0.7170000000000001,\n",
       " 'internalizing': -1.45,\n",
       " 'externalizing': -0.448,\n",
       " 'RestingState': 'available',\n",
       " 'DespicableMe': 'available',\n",
       " 'FunwithFractals': 'available',\n",
       " 'ThePresent': 'available',\n",
       " 'DiaryOfAWimpyKid': 'available',\n",
       " 'contrastChangeDetection_1': 'available',\n",
       " 'contrastChangeDetection_2': 'available',\n",
       " 'contrastChangeDetection_3': 'available',\n",
       " 'surroundSupp_1': 'available',\n",
       " 'surroundSupp_2': 'available',\n",
       " 'seqLearning6target': 'unavailable',\n",
       " 'seqLearning8target': 'available',\n",
       " 'symbolSearch': 'available',\n",
       " 'release_dir': 'D:\\\\projects\\\\pytorch_training\\\\data_1\\\\R1_L100_bdf'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = Path(\"D:\\projects\\pytorch_training\\data_1\\R1_L100_bdf\\sub-NDARCZ947WU5\\eeg\\sub-NDARCZ947WU5_task-surroundSupp_run-1_eeg.bdf\")\n",
    "get_subject_metadata_from_path(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detailed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 7159 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [05:28,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-DespicableMe_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-DiaryOfAWimpyKid_eeg.bdf, wrong channel count 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "332it [05:28,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-FunwithFractals_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-FunwithFractals_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-RestingState_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-RestingState_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-RestingState_eeg.bdf, wrong channel count 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "334it [05:28,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-seqLearning8target_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-seqLearning8target_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-symbolSearch_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARBA381JGH\\eeg\\sub-NDARBA381JGH_task-symbolSearch_eeg.bdf, wrong channel count 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5380it [1:21:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-RestingState_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-RestingState_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-seqLearning8target_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-seqLearning8target_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-seqLearning8target_eeg.bdf, wrong channel count 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5382it [1:21:12,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-surroundSupp_run-1_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-surroundSupp_run-1_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-surroundSupp_run-1_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-symbolSearch_eeg.bdf, wrong channel count 6\n",
      "Skipping D:\\projects\\pytorch_training\\data_1\\R7_L100_bdf\\sub-NDARUJ292JXV\\eeg\\sub-NDARUJ292JXV_task-symbolSearch_eeg.bdf, wrong channel count 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7159it [1:46:21,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data path  D:\\projects\\pytorch_training\\full_eeg_data_shards\\train with 1140130 windows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dir_path  , train_windows_number = load_and_shard_eeg_data(train_data_paths , split_type=\"train\" , shard_size=1000 , window_size=200 , stride=20 , output_dir=output_dir)\n",
    "print(f\"train data path  { train_dir_path} with {train_windows_number} windows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 881 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "881it [12:57,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 846 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "846it [13:25,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dir_path , val_windows_number = load_and_shard_eeg_data(val_data_paths , split_type=\"val\" , shard_size=1000 , window_size=200 , stride=20 , output_dir=output_dir)\n",
    "test_dir_path , test_windows_number = load_and_shard_eeg_data(test_data_paths , split_type=\"test\" , shard_size=1000 , window_size=200 , stride=20 , output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = torch.load(os.path.join(train_dir_path, \"full_dict.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'participant_id': 'sub-NDARAB055BPR',\n",
       "  'release_number': 'R6',\n",
       "  'sex': 'F',\n",
       "  'age': 10.7939,\n",
       "  'ehq_total': 60.03,\n",
       "  'commercial_use': 'Yes',\n",
       "  'full_pheno': 'Yes',\n",
       "  'p_factor': 0.366,\n",
       "  'attention': 1.029,\n",
       "  'internalizing': -0.461,\n",
       "  'externalizing': 0.144,\n",
       "  'RestingState': 'available',\n",
       "  'DespicableMe': 'available',\n",
       "  'FunwithFractals': 'available',\n",
       "  'ThePresent': 'available',\n",
       "  'DiaryOfAWimpyKid': 'available',\n",
       "  'contrastChangeDetection_1': 'available',\n",
       "  'contrastChangeDetection_2': 'available',\n",
       "  'contrastChangeDetection_3': 'available',\n",
       "  'surroundSupp_1': 'available',\n",
       "  'surroundSupp_2': 'available',\n",
       "  'seqLearning6target': 'unavailable',\n",
       "  'seqLearning8target': 'available',\n",
       "  'symbolSearch': 'caution',\n",
       "  'release_dir': 'D:\\\\projects\\\\pytorch_training\\\\data_1\\\\R6_L100_bdf'},\n",
       " 'limits': {'Start': 0, 'End': 1298},\n",
       " 'tasks': {'contrastChangeDetection': {'Start': 0, 'End': 361},\n",
       "  'DespicableMe': {'Start': 361, 'End': 447},\n",
       "  'DiaryOfAWimpyKid': {'Start': 447, 'End': 506},\n",
       "  'FunwithFractals': {'Start': 506, 'End': 589},\n",
       "  'RestingState': {'Start': 589, 'End': 769},\n",
       "  'seqLearning8target': {'Start': 769, 'End': 923},\n",
       "  'surroundSupp': {'Start': 923, 'End': 1196},\n",
       "  'ThePresent': {'Start': 1196, 'End': 1298}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict[\"sub-NDARAB055BPR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_subjects = []\n",
    "ordered_starts = []\n",
    "\n",
    "valid_subjects = [\n",
    "    (subj, meta_dict[subj][\"limits\"][\"Start\"])\n",
    "    for subj in meta_dict\n",
    "    if meta_dict[subj][\"limits\"][\"Start\"] != meta_dict[subj][\"limits\"][\"End\"]\n",
    "]\n",
    "\n",
    "valid_subjects.sort(key=lambda x: x[1])\n",
    "\n",
    "for subj, start in valid_subjects:\n",
    "    ordered_subjects.append(subj)\n",
    "    ordered_starts.append(start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_task = {}\n",
    "\n",
    "for subj, subj_data in meta_dict.items():\n",
    "    tasks = subj_data.get(\"tasks\", {})\n",
    "    if not tasks:\n",
    "        continue\n",
    "\n",
    "    task_items = [\n",
    "        (t_name, t_info[\"Start\"])\n",
    "        for t_name, t_info in tasks.items()\n",
    "        if t_info[\"End\"] is not None and t_info[\"Start\"] != t_info[\"End\"]\n",
    "    ]\n",
    "\n",
    "    task_items.sort(key=lambda x: x[1])\n",
    "\n",
    "    subject_task[subj] = {\n",
    "        \"tasks\": [t for t, _ in task_items],\n",
    "        \"starts\": [s for _, s in task_items]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub-NDARAB055BPR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [0, 361, 447, 506, 589, 769, 923, 1196]},\n",
       " 'sub-NDARAB348EWR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1298, 1752, 1838, 1898, 1981, 2198, 2470, 2656]},\n",
       " 'sub-NDARAB458VK9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [2758, 3231, 3319, 3380, 3465, 3684, 3836, 4289, 4512]},\n",
       " 'sub-NDARAB708LM5': {'tasks': ['RestingState'], 'starts': [4615]},\n",
       " 'sub-NDARAB756JDJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [4837, 5189, 5250, 5433, 5638, 5751, 6044]},\n",
       " 'sub-NDARAB793GL3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [6165, 6955, 7043, 7104, 7188, 7392, 7651, 8263, 8543]},\n",
       " 'sub-NDARAC853DTE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [8647, 8734, 8794, 8876, 9054, 9187, 9603, 9682]},\n",
       " 'sub-NDARAC904DMU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [9784, 10140, 10226, 10286, 10368, 10570, 10802, 11192, 11270]},\n",
       " 'sub-NDARAD224CRB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [11373, 11760, 11846, 11905, 11988, 12164, 12339, 12633, 12779]},\n",
       " 'sub-NDARAD774HAZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [12881, 13542, 13628, 13688, 13771, 13976, 14118, 14508, 14632]},\n",
       " 'sub-NDARAE301XTM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [14734, 15105, 15191, 15251, 15333, 15512, 15672, 15948, 16036]},\n",
       " 'sub-NDARAE828CML': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [16138, 16738, 16824, 16883, 16966, 17141, 17271, 17647, 17727]},\n",
       " 'sub-NDARAE866UVF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [17829, 18105, 18193, 18255, 18338, 18517, 18641, 19161, 19241]},\n",
       " 'sub-NDARAE877NER': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [19345, 19649, 19737, 19798, 19883, 20059, 20240, 20811]},\n",
       " 'sub-NDARAF440XWG': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [20915, 21040, 21112, 21216, 21415, 21583, 21704]},\n",
       " 'sub-NDARAF535XK6': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [21807, 21931, 22007, 22101, 22280, 22548, 22931]},\n",
       " 'sub-NDARAG143ARJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [23034, 23613, 23699, 23758, 23840, 24033, 24160, 24566]},\n",
       " 'sub-NDARAG340ERT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [24668, 25343, 25429, 25489, 25572, 25761, 25938, 26344, 26424]},\n",
       " 'sub-NDARAH948UF0': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [26526, 26625, 26697, 26792, 26964, 27124, 27358]},\n",
       " 'sub-NDARAL828WXM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [27461, 28402, 28490, 28550, 28724, 28851, 29363]},\n",
       " 'sub-NDARAM487XU3': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [29467, 29589, 29650, 29764, 29966]},\n",
       " 'sub-NDARAM675UR8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [30070, 31096, 31182, 31241, 31324, 31504, 31753, 32219, 32305]},\n",
       " 'sub-NDARAM704GKZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [32407, 33062, 33148, 33207, 33290, 33463, 33847, 34103]},\n",
       " 'sub-NDARAM873GAC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [34205, 34879, 34965, 35025, 35108, 35309, 35431, 35812]},\n",
       " 'sub-NDARAN076TPG': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [35914, 36094, 36229]},\n",
       " 'sub-NDARAN262WK6': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [37515, 37756, 38004, 38219]},\n",
       " 'sub-NDARAN385MDH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [38431, 39102, 39188, 39247, 39330, 39504, 39693, 40115]},\n",
       " 'sub-NDARAP176AD1': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [40217, 41187, 41367, 41542, 41677]},\n",
       " 'sub-NDARAP359UM6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [41759, 42427, 42514, 42573, 42656, 42845, 43011, 43405, 43488]},\n",
       " 'sub-NDARAP782TVC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [43590, 43691, 43818, 43994, 44512]},\n",
       " 'sub-NDARAR238RZ8': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [44616, 44860, 45010]},\n",
       " 'sub-NDARAT244VGA': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [45218, 45403, 45564, 47269]},\n",
       " 'sub-NDARAT680GJA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [47368, 47800, 47888, 47949, 48060, 48292, 48420, 48926, 49008]},\n",
       " 'sub-NDARAV031PPJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState'],\n",
       "  'starts': [49111, 49198, 49312, 49395]},\n",
       " 'sub-NDARAV519RND': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [49612, 50268, 50356, 50432, 50516, 50718, 50967, 51194, 51282]},\n",
       " 'sub-NDARAV610EY3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [51386, 52017, 52103, 52162, 52245, 52416, 52533, 52901]},\n",
       " 'sub-NDARAW320CGR': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [53003, 53118, 53194, 53290, 53504, 53633, 53942, 54023]},\n",
       " 'sub-NDARBA381JGH': {'tasks': [], 'starts': []},\n",
       " 'sub-NDARBA680RFY': {'tasks': ['DespicableMe',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch'],\n",
       "  'starts': [54125, 54369, 54565, 54744]},\n",
       " 'sub-NDARBA839HLG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [54897, 55529, 55615, 55674, 55758, 55931, 56107, 56476, 56555]},\n",
       " 'sub-NDARBD879MBX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [56657, 57314, 57402, 57463, 57547, 57761, 57915, 58469, 58566]},\n",
       " 'sub-NDARBE641DGZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [58670, 59346, 59432, 59491, 59574, 59748, 59854, 60260, 60346]},\n",
       " 'sub-NDARBF176LPM': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [60448, 60591, 60667, 60802, 61364]},\n",
       " 'sub-NDARBF183RFB': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [61468, 61643, 61797, 62066]},\n",
       " 'sub-NDARBF293YRB': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [62180, 62307, 62388, 62500, 62705, 62886, 63004]},\n",
       " 'sub-NDARBF906DVB': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch'],\n",
       "  'starts': [63108, 63281, 63530]},\n",
       " 'sub-NDARBG574KF4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [63610, 64447, 64533, 64592, 64675, 64849, 64955, 65328, 65408]},\n",
       " 'sub-NDARBG831VK4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [65511, 66207, 66293, 66353, 66436, 66764, 66904, 67301, 67388]},\n",
       " 'sub-NDARBH024NH2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [67490, 68194, 68280, 68339, 68422, 68629, 68870, 69326, 69411]},\n",
       " 'sub-NDARBJ159HXB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [69513, 70295, 70382, 70442, 70526, 70800, 70931, 71502, 71604]},\n",
       " 'sub-NDARBJ637CFZ': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [71708, 71885, 72055, 73595]},\n",
       " 'sub-NDARBK082PDD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [73682, 74469, 74555, 74614, 74697, 74873, 74976, 75483, 75561]},\n",
       " 'sub-NDARBL042LWN': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [75664, 75854, 76023]},\n",
       " 'sub-NDARBM173BJG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [77668, 78196, 78282, 78342, 78425, 78640, 78887, 79260]},\n",
       " 'sub-NDARBM197EFF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [79362, 79606, 79693, 79755, 79838, 80014, 80179, 80776]},\n",
       " 'sub-NDARBM642JFT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [80880, 81385, 81473, 81535, 81619, 81797, 81980, 82565, 82674]},\n",
       " 'sub-NDARBM839WR5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [82778, 83351, 83438, 83552, 83636, 83893, 84072, 84696, 84816]},\n",
       " 'sub-NDARBN611GRA': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [84920, 85118, 85304, 87070]},\n",
       " 'sub-NDARBN620TT7': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [87175, 87364, 87495]},\n",
       " 'sub-NDARBR740NKV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [88928, 89334, 89420, 89479, 89561, 89738, 89884, 90262]},\n",
       " 'sub-NDARBU607ZZ3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [90364, 91031, 91118, 91177, 91260, 91440, 91630, 92021]},\n",
       " 'sub-NDARBU668NTV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [92123, 92639, 92727, 92789, 92873, 93054, 93173, 93735]},\n",
       " 'sub-NDARBU730PN8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [93838, 94552, 94638, 94697, 94779, 94957, 95091, 95478, 95557]},\n",
       " 'sub-NDARBV680AA8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [95659, 96499, 96585, 96645, 96728, 97091, 97226, 97617, 97703]},\n",
       " 'sub-NDARBW026UGE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [97805, 98265, 98352, 98412, 98495, 98690, 98877, 99147, 99295]},\n",
       " 'sub-NDARBW255JE1': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [99397, 99545, 99606, 99749, 99970, 100525]},\n",
       " 'sub-NDARBX121UM9': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [100629, 100715, 100793, 100875, 101052, 101185, 101557]},\n",
       " 'sub-NDARBX400RTC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [101659, 101768, 101844, 101940, 102129, 102251, 102401]},\n",
       " 'sub-NDARBX830ZD4': {'tasks': ['RestingState'], 'starts': [102503]},\n",
       " 'sub-NDARBX974XDR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [102510, 103020, 103108, 103168, 103252, 103500, 103654, 104234]},\n",
       " 'sub-NDARBZ475NKK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [104338, 104463, 104544, 104650, 104824, 105029, 105307]},\n",
       " 'sub-NDARBZ925PRF': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [105409, 105596, 105713, 107129]},\n",
       " 'sub-NDARCA153NKE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [107214,\n",
       "   107747,\n",
       "   107833,\n",
       "   107892,\n",
       "   107975,\n",
       "   108148,\n",
       "   108335,\n",
       "   108766,\n",
       "   108849]},\n",
       " 'sub-NDARCA578CEB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [108951,\n",
       "   109332,\n",
       "   109418,\n",
       "   109478,\n",
       "   109560,\n",
       "   109737,\n",
       "   109864,\n",
       "   110140,\n",
       "   110219]},\n",
       " 'sub-NDARCA618DGA': {'tasks': ['contrastChangeDetection',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [110321, 110368, 110528, 110737, 110899, 111154]},\n",
       " 'sub-NDARCB142ZPB': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [111247, 111382, 111527, 111609, 111783, 112270]},\n",
       " 'sub-NDARCD296XU9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [112372,\n",
       "   112597,\n",
       "   112684,\n",
       "   112745,\n",
       "   112829,\n",
       "   113004,\n",
       "   113136,\n",
       "   113784,\n",
       "   113890]},\n",
       " 'sub-NDARCD357CZR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [113994, 114846, 114934, 114996, 115080, 115425, 115628, 116236]},\n",
       " 'sub-NDARCE721YB5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [116339,\n",
       "   116903,\n",
       "   116991,\n",
       "   117052,\n",
       "   117136,\n",
       "   117325,\n",
       "   117460,\n",
       "   118104,\n",
       "   118202]},\n",
       " 'sub-NDARCG785NND': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [118306, 118610, 118697, 118757, 118842, 119021, 119466, 119621]},\n",
       " 'sub-NDARCH514JCT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [119725, 119951, 120040, 120102, 120186, 120367, 120542, 120869]},\n",
       " 'sub-NDARCH628LKW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [120973,\n",
       "   121391,\n",
       "   121479,\n",
       "   121540,\n",
       "   121624,\n",
       "   121799,\n",
       "   121918,\n",
       "   122454,\n",
       "   122552]},\n",
       " 'sub-NDARCH881EFG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [122656, 123328, 123414, 123473, 123556, 123735, 123872, 124259]},\n",
       " 'sub-NDARCH889NUF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [124361, 124735, 124821, 124881, 124964, 125138, 125265, 125549]},\n",
       " 'sub-NDARCJ011TTG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [125651, 125901, 125989, 126078, 126162, 126338, 126543, 126944]},\n",
       " 'sub-NDARCJ170CT9': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [127048, 127149, 127242, 127324, 127505, 128110, 128363]},\n",
       " 'sub-NDARCJ594BWQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [128465,\n",
       "   129134,\n",
       "   129220,\n",
       "   129279,\n",
       "   129362,\n",
       "   129546,\n",
       "   129693,\n",
       "   130064,\n",
       "   130146]},\n",
       " 'sub-NDARCK162REX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [130248,\n",
       "   130562,\n",
       "   130649,\n",
       "   130709,\n",
       "   130791,\n",
       "   131102,\n",
       "   131254,\n",
       "   131779,\n",
       "   131948]},\n",
       " 'sub-NDARCK481KRH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [132050,\n",
       "   132435,\n",
       "   132521,\n",
       "   132581,\n",
       "   132663,\n",
       "   132864,\n",
       "   132984,\n",
       "   133260,\n",
       "   133344]},\n",
       " 'sub-NDARCK647MU6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [133446, 134271, 134359, 134419, 134503, 134767, 134911, 135455]},\n",
       " 'sub-NDARCK661RZ6': {'tasks': ['DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState'],\n",
       "  'starts': [135559, 135663, 135870]},\n",
       " 'sub-NDARCL008HLA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [136061,\n",
       "   136323,\n",
       "   136410,\n",
       "   136472,\n",
       "   136556,\n",
       "   136729,\n",
       "   136916,\n",
       "   137404,\n",
       "   137504]},\n",
       " 'sub-NDARCL016NHB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [137608,\n",
       "   137990,\n",
       "   138078,\n",
       "   138140,\n",
       "   138224,\n",
       "   138411,\n",
       "   138536,\n",
       "   139072,\n",
       "   139158]},\n",
       " 'sub-NDARCL105LUE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [139261, 139347, 139406, 139514, 139690, 140082, 140267]},\n",
       " 'sub-NDARCL895UV8': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [140369, 140455, 140514, 140597, 140778]},\n",
       " 'sub-NDARCM135DVC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [140880, 141019, 141142, 141244, 141443, 141607]},\n",
       " 'sub-NDARCN500KJG': {'tasks': ['FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [141709, 141794, 141974, 142117, 142447]},\n",
       " 'sub-NDARCN669XPR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [142534,\n",
       "   142957,\n",
       "   143044,\n",
       "   143105,\n",
       "   143189,\n",
       "   143396,\n",
       "   143563,\n",
       "   144139,\n",
       "   144225]},\n",
       " 'sub-NDARCP292KPA': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [144356, 144476, 144578, 144706, 144916, 145093, 145248]},\n",
       " 'sub-NDARCP642KML': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [145352, 145464, 145567, 145678, 145851, 146005, 146172]},\n",
       " 'sub-NDARCR499NE4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [146276,\n",
       "   146439,\n",
       "   146540,\n",
       "   146633,\n",
       "   146731,\n",
       "   146908,\n",
       "   147088,\n",
       "   147217,\n",
       "   147300]},\n",
       " 'sub-NDARCR743RHQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [147402, 147683, 147770, 147830, 147915, 148092, 148217, 148683]},\n",
       " 'sub-NDARCU621EBN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [148787, 148899, 148991, 149119, 149296, 149472]},\n",
       " 'sub-NDARCU865PBV': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [149574, 149905, 149998, 150275]},\n",
       " 'sub-NDARCV628UUW': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [150377, 150560, 150715]},\n",
       " 'sub-NDARCV944JA6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [152088,\n",
       "   152727,\n",
       "   152813,\n",
       "   152872,\n",
       "   152955,\n",
       "   153145,\n",
       "   153400,\n",
       "   153783,\n",
       "   153864]},\n",
       " 'sub-NDARCW071AU5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [153967, 154619, 154705, 154765, 154848, 155052, 155161, 155558]},\n",
       " 'sub-NDARCW094JCG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [155660,\n",
       "   156314,\n",
       "   156400,\n",
       "   156459,\n",
       "   156542,\n",
       "   156747,\n",
       "   156869,\n",
       "   157242,\n",
       "   157330]},\n",
       " 'sub-NDARCW932EU6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [157432,\n",
       "   157679,\n",
       "   157767,\n",
       "   157828,\n",
       "   157913,\n",
       "   158124,\n",
       "   158300,\n",
       "   158845,\n",
       "   158943]},\n",
       " 'sub-NDARCW933FD5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [159047,\n",
       "   159462,\n",
       "   159550,\n",
       "   159612,\n",
       "   159696,\n",
       "   159886,\n",
       "   160037,\n",
       "   160615,\n",
       "   160722]},\n",
       " 'sub-NDARCX462NVA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [160825,\n",
       "   161213,\n",
       "   161299,\n",
       "   161359,\n",
       "   161441,\n",
       "   161614,\n",
       "   161744,\n",
       "   162034,\n",
       "   162115]},\n",
       " 'sub-NDARCY178KJP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [162217,\n",
       "   162894,\n",
       "   162980,\n",
       "   163039,\n",
       "   163122,\n",
       "   163310,\n",
       "   163450,\n",
       "   163822,\n",
       "   163907]},\n",
       " 'sub-NDARCZ388PF5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [164009,\n",
       "   164400,\n",
       "   164488,\n",
       "   164661,\n",
       "   164745,\n",
       "   164931,\n",
       "   165055,\n",
       "   165514,\n",
       "   165599]},\n",
       " 'sub-NDARCZ770BRG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [165704,\n",
       "   166394,\n",
       "   166480,\n",
       "   166539,\n",
       "   166622,\n",
       "   166799,\n",
       "   167003,\n",
       "   167418,\n",
       "   167501]},\n",
       " 'sub-NDARCZ947WU5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [167603,\n",
       "   168180,\n",
       "   168266,\n",
       "   168326,\n",
       "   168408,\n",
       "   168639,\n",
       "   168836,\n",
       "   169216,\n",
       "   169294]},\n",
       " 'sub-NDARDA656RBN': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [169396, 169585, 169701]},\n",
       " 'sub-NDARDC003YG7': {'tasks': ['DespicableMe',\n",
       "   'RestingState',\n",
       "   'seqLearning8target'],\n",
       "  'starts': [170950, 171039, 171228]},\n",
       " 'sub-NDARDC704GKW': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [171367, 171487, 171561, 171680, 171945]},\n",
       " 'sub-NDARDD073JKZ': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [172047, 172288, 172414]},\n",
       " 'sub-NDARDD193HUP': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [173358, 173534, 173683]},\n",
       " 'sub-NDARDD531EP2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [174707, 175424, 175510, 175569, 175652, 175829, 175959, 176342]},\n",
       " 'sub-NDARDE294HNX': {'tasks': ['contrastChangeDetection',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [176444, 177029, 177112, 177285, 177398]},\n",
       " 'sub-NDARDE319VD1': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [177704, 177831, 177930, 178025, 178216, 178411, 178507]},\n",
       " 'sub-NDARDE827UF0': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [178611, 178737, 178813, 178937, 179190]},\n",
       " 'sub-NDARDG484ZJ1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [179294,\n",
       "   180052,\n",
       "   180139,\n",
       "   180200,\n",
       "   180285,\n",
       "   180688,\n",
       "   180856,\n",
       "   181289,\n",
       "   181371]},\n",
       " 'sub-NDARDH270YR5': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [181475, 181604, 181852, 182024, 182163]},\n",
       " 'sub-NDARDH305AV2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [182315, 182690, 182776, 182835, 182918, 183133, 183269, 183548]},\n",
       " 'sub-NDARDH670PXH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [183650,\n",
       "   184634,\n",
       "   184723,\n",
       "   184784,\n",
       "   184868,\n",
       "   185073,\n",
       "   185451,\n",
       "   185951,\n",
       "   186024]},\n",
       " 'sub-NDARDH793TRR': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [186128, 186189, 186362]},\n",
       " 'sub-NDARDJ947WXC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [186466, 186746, 186820, 186920, 187232, 187951]},\n",
       " 'sub-NDARDJ970ELG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [188055,\n",
       "   188406,\n",
       "   188492,\n",
       "   188552,\n",
       "   188634,\n",
       "   188806,\n",
       "   188933,\n",
       "   189213,\n",
       "   189310]},\n",
       " 'sub-NDARDL511UND': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [189412,\n",
       "   189790,\n",
       "   189877,\n",
       "   189938,\n",
       "   190043,\n",
       "   190240,\n",
       "   190385,\n",
       "   190957,\n",
       "   191051]},\n",
       " 'sub-NDARDM385EK2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [191155,\n",
       "   191878,\n",
       "   191964,\n",
       "   192023,\n",
       "   192106,\n",
       "   192198,\n",
       "   192344,\n",
       "   192719,\n",
       "   192800]},\n",
       " 'sub-NDARDN996CPF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [192902, 193294, 193380, 193440, 193522, 193700, 193875, 194196]},\n",
       " 'sub-NDARDR067YMC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [194298, 194701, 194789, 194850, 194934, 195149, 195274, 195851]},\n",
       " 'sub-NDARDR240WKM': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [195955, 196139, 196318, 196871, 196976]},\n",
       " 'sub-NDARDR296XHN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [197080, 197767, 197853, 197912, 197995, 198169, 198349, 198726]},\n",
       " 'sub-NDARDR591AUC': {'tasks': ['contrastChangeDetection',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target'],\n",
       "  'starts': [198828, 199301, 200314, 200648]},\n",
       " 'sub-NDARDR595ZE5': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [200777, 200968, 201111, 202658]},\n",
       " 'sub-NDARDT528ZZJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [202743, 203413, 203500, 203559, 203642, 203830, 203982, 204366]},\n",
       " 'sub-NDARDU482DGK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [204468,\n",
       "   204717,\n",
       "   204805,\n",
       "   204867,\n",
       "   204951,\n",
       "   205141,\n",
       "   205277,\n",
       "   205776,\n",
       "   205886]},\n",
       " 'sub-NDARDU602LPX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [205990,\n",
       "   206334,\n",
       "   206423,\n",
       "   206483,\n",
       "   206567,\n",
       "   206747,\n",
       "   206917,\n",
       "   207436,\n",
       "   207524]},\n",
       " 'sub-NDARDU617ZW1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [207627,\n",
       "   207990,\n",
       "   208076,\n",
       "   208135,\n",
       "   208218,\n",
       "   208393,\n",
       "   208503,\n",
       "   208786,\n",
       "   208873]},\n",
       " 'sub-NDARDU986RBM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [208975,\n",
       "   209657,\n",
       "   209743,\n",
       "   209802,\n",
       "   209885,\n",
       "   210076,\n",
       "   210224,\n",
       "   210629,\n",
       "   210712]},\n",
       " 'sub-NDARDV088AA3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [210815, 211207, 211295, 211356, 211440, 211621, 211812, 212331]},\n",
       " 'sub-NDARDV204BYM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [212435,\n",
       "   212702,\n",
       "   212789,\n",
       "   212850,\n",
       "   212935,\n",
       "   213108,\n",
       "   213236,\n",
       "   213738,\n",
       "   213820]},\n",
       " 'sub-NDARDV865ENP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [213924,\n",
       "   214150,\n",
       "   214238,\n",
       "   214299,\n",
       "   214383,\n",
       "   214562,\n",
       "   214695,\n",
       "   215142,\n",
       "   215223]},\n",
       " 'sub-NDARDW416KWZ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [215327, 215427, 215516, 215643, 215819, 215969, 216424]},\n",
       " 'sub-NDARDY150ZP9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [216526,\n",
       "   217162,\n",
       "   217248,\n",
       "   217308,\n",
       "   217391,\n",
       "   217563,\n",
       "   217681,\n",
       "   218057,\n",
       "   218140]},\n",
       " 'sub-NDARDZ058NZN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [218242,\n",
       "   218898,\n",
       "   218984,\n",
       "   219043,\n",
       "   219126,\n",
       "   219300,\n",
       "   219521,\n",
       "   219912,\n",
       "   219995]},\n",
       " 'sub-NDARDZ147ETZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [220097,\n",
       "   220551,\n",
       "   220637,\n",
       "   220697,\n",
       "   220779,\n",
       "   220972,\n",
       "   221123,\n",
       "   221426,\n",
       "   221506]},\n",
       " 'sub-NDARDZ266KET': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [221608,\n",
       "   221869,\n",
       "   221957,\n",
       "   222019,\n",
       "   222103,\n",
       "   222340,\n",
       "   222436,\n",
       "   222898,\n",
       "   223000]},\n",
       " 'sub-NDARDZ425JVB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [223103, 223422, 223510, 223571, 223799, 224008, 225151]},\n",
       " 'sub-NDARDZ440NGK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [225255, 225674, 225760, 225820, 225902, 226150, 226334, 226626]},\n",
       " 'sub-NDAREB662RLE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [226728, 226874, 226953, 227059, 227244, 227372, 227796]},\n",
       " 'sub-NDAREC277JCP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [227898, 228013, 228084, 228181, 228444, 228571]},\n",
       " 'sub-NDAREC377AU2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [228673,\n",
       "   229339,\n",
       "   229425,\n",
       "   229485,\n",
       "   229568,\n",
       "   229744,\n",
       "   229923,\n",
       "   230315,\n",
       "   230399]},\n",
       " 'sub-NDAREC542MH3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [230501,\n",
       "   231008,\n",
       "   231095,\n",
       "   231157,\n",
       "   231241,\n",
       "   231429,\n",
       "   231596,\n",
       "   232156,\n",
       "   232274]},\n",
       " 'sub-NDAREF389RY2': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [232378, 232479, 232552, 232728, 232962, 233263]},\n",
       " 'sub-NDAREF893ZM8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [233365, 233949, 234035, 234094, 234177, 234350, 234484, 234865]},\n",
       " 'sub-NDAREH074NG8': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [234967, 235067, 235142, 235256, 235438, 235570, 235764]},\n",
       " 'sub-NDAREJ923AYR': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [235866, 236015, 236118, 236253, 236821]},\n",
       " 'sub-NDAREK549XUQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [236924,\n",
       "   237397,\n",
       "   237485,\n",
       "   237573,\n",
       "   237657,\n",
       "   237838,\n",
       "   237967,\n",
       "   238527,\n",
       "   238641]},\n",
       " 'sub-NDAREL721PGQ': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [238745, 238925, 239117, 240726]},\n",
       " 'sub-NDAREM500WWH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [240811,\n",
       "   241549,\n",
       "   241637,\n",
       "   241698,\n",
       "   241782,\n",
       "   241986,\n",
       "   242131,\n",
       "   242735,\n",
       "   242869]},\n",
       " 'sub-NDAREM609ZXW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [242972,\n",
       "   243347,\n",
       "   243433,\n",
       "   243493,\n",
       "   243576,\n",
       "   243752,\n",
       "   243901,\n",
       "   244173,\n",
       "   244254]},\n",
       " 'sub-NDAREM731BYM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [244357,\n",
       "   244797,\n",
       "   244883,\n",
       "   244943,\n",
       "   245025,\n",
       "   245198,\n",
       "   245341,\n",
       "   245713,\n",
       "   245798]},\n",
       " 'sub-NDAREM887YY8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [245900,\n",
       "   246562,\n",
       "   246648,\n",
       "   246707,\n",
       "   246790,\n",
       "   246964,\n",
       "   247123,\n",
       "   247493,\n",
       "   247580]},\n",
       " 'sub-NDAREN151YXN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [247682, 247943, 248030, 248091, 248176, 248353, 248507, 248988]},\n",
       " 'sub-NDAREN519BLJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [249092,\n",
       "   250021,\n",
       "   250107,\n",
       "   250166,\n",
       "   250362,\n",
       "   250536,\n",
       "   250691,\n",
       "   251149,\n",
       "   251232]},\n",
       " 'sub-NDAREN969HF7': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [251334, 251458, 251540, 251633, 251837, 251983, 252183, 252385]},\n",
       " 'sub-NDAREN999ERM': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [252489, 252668, 252817]},\n",
       " 'sub-NDAREP505XAD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [254209, 254663, 254749, 254808, 254891, 255142, 255277, 255550]},\n",
       " 'sub-NDARER379GTP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [255652, 256367, 256453, 256513, 256595, 256829, 256989, 257331]},\n",
       " 'sub-NDARET332CTB': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [257433, 257533, 257606, 257717, 258014]},\n",
       " 'sub-NDARET550NXC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [258116, 258690, 258778, 258839, 258923, 259120, 259291, 259803]},\n",
       " 'sub-NDAREV527ZRF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [259907,\n",
       "   260309,\n",
       "   260397,\n",
       "   260458,\n",
       "   260591,\n",
       "   260787,\n",
       "   260948,\n",
       "   261504,\n",
       "   261595]},\n",
       " 'sub-NDAREV601CE7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [261699,\n",
       "   262313,\n",
       "   262401,\n",
       "   262463,\n",
       "   262547,\n",
       "   262734,\n",
       "   262902,\n",
       "   263521,\n",
       "   263637]},\n",
       " 'sub-NDAREV848HWX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [263741, 264038, 264126, 264187, 264270, 264461, 264629, 265126]},\n",
       " 'sub-NDAREW661NZJ': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [265229, 265423, 265588]},\n",
       " 'sub-NDAREW937RMY': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [267002, 267176, 267325]},\n",
       " 'sub-NDAREY512KVX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [267547,\n",
       "   268026,\n",
       "   268136,\n",
       "   268212,\n",
       "   268297,\n",
       "   268549,\n",
       "   268677,\n",
       "   268888,\n",
       "   268996]},\n",
       " 'sub-NDAREZ416XDW': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [269100, 269277, 269392]},\n",
       " 'sub-NDARFA089ZZG': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [270883, 270944, 271122, 271312, 272000]},\n",
       " 'sub-NDARFA402LMW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [272103, 272957, 273043, 273102, 273185, 273378, 273579, 273968]},\n",
       " 'sub-NDARFA804ZLW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [274070,\n",
       "   274312,\n",
       "   274399,\n",
       "   274460,\n",
       "   274544,\n",
       "   274722,\n",
       "   274904,\n",
       "   275357,\n",
       "   275466]},\n",
       " 'sub-NDARFA815FXE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [275570,\n",
       "   276419,\n",
       "   276505,\n",
       "   276564,\n",
       "   276712,\n",
       "   276886,\n",
       "   277107,\n",
       "   277479,\n",
       "   277561]},\n",
       " 'sub-NDARFB908HVX': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [277663, 277764, 277824, 277921, 278096, 278314, 278704]},\n",
       " 'sub-NDARFC203HCD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [278806, 279221, 279307, 279367, 279449, 279672]},\n",
       " 'sub-NDARFD316UD2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [279774, 280173, 280261, 280322, 280406, 280604, 280750, 281293]},\n",
       " 'sub-NDARFE555KXB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [281397,\n",
       "   281755,\n",
       "   281841,\n",
       "   281900,\n",
       "   281983,\n",
       "   282159,\n",
       "   282303,\n",
       "   282582,\n",
       "   282682]},\n",
       " 'sub-NDARFF070XHV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [282784,\n",
       "   283259,\n",
       "   283346,\n",
       "   283450,\n",
       "   283534,\n",
       "   283716,\n",
       "   283872,\n",
       "   284465,\n",
       "   284579]},\n",
       " 'sub-NDARFF644ZGD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [284683,\n",
       "   285224,\n",
       "   285310,\n",
       "   285369,\n",
       "   285452,\n",
       "   285628,\n",
       "   285777,\n",
       "   286152,\n",
       "   286232]},\n",
       " 'sub-NDARFG757CEU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [286334,\n",
       "   286496,\n",
       "   286598,\n",
       "   286689,\n",
       "   286786,\n",
       "   286989,\n",
       "   287162,\n",
       "   287405,\n",
       "   287515]},\n",
       " 'sub-NDARFJ000DCY': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [287619, 287722, 287797, 287929, 288123]},\n",
       " 'sub-NDARFK610GY5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [288226,\n",
       "   288583,\n",
       "   288669,\n",
       "   288728,\n",
       "   288811,\n",
       "   288992,\n",
       "   289140,\n",
       "   289536,\n",
       "   289613]},\n",
       " 'sub-NDARFL579BAP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [289715, 289842, 289945, 290069, 290275, 290429]},\n",
       " 'sub-NDARFR108JNB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [290532,\n",
       "   291268,\n",
       "   291355,\n",
       "   291416,\n",
       "   291499,\n",
       "   291717,\n",
       "   291954,\n",
       "   292523,\n",
       "   292635]},\n",
       " 'sub-NDARFR301KKP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [292739,\n",
       "   292964,\n",
       "   293052,\n",
       "   293114,\n",
       "   293198,\n",
       "   293375,\n",
       "   293471,\n",
       "   293933,\n",
       "   294039]},\n",
       " 'sub-NDARFT176NJP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [294143,\n",
       "   294607,\n",
       "   294694,\n",
       "   294754,\n",
       "   294836,\n",
       "   295014,\n",
       "   295171,\n",
       "   295496,\n",
       "   295581]},\n",
       " 'sub-NDARFT305CG1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [295683,\n",
       "   296344,\n",
       "   296432,\n",
       "   296494,\n",
       "   296578,\n",
       "   296783,\n",
       "   296953,\n",
       "   297457,\n",
       "   297543]},\n",
       " 'sub-NDARFT581ZW5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [297647,\n",
       "   298085,\n",
       "   298171,\n",
       "   298230,\n",
       "   298313,\n",
       "   298490,\n",
       "   298623,\n",
       "   299101,\n",
       "   299180]},\n",
       " 'sub-NDARFT615JZ6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [299282,\n",
       "   299532,\n",
       "   299620,\n",
       "   299681,\n",
       "   299765,\n",
       "   299940,\n",
       "   300108,\n",
       "   300607,\n",
       "   300702]},\n",
       " 'sub-NDARFU407YL4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [300806,\n",
       "   301299,\n",
       "   301386,\n",
       "   301447,\n",
       "   301531,\n",
       "   301705,\n",
       "   301885,\n",
       "   302544,\n",
       "   302694]},\n",
       " 'sub-NDARFV557XAA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [302798,\n",
       "   303513,\n",
       "   303599,\n",
       "   303658,\n",
       "   303741,\n",
       "   303925,\n",
       "   304114,\n",
       "   304504,\n",
       "   304599]},\n",
       " 'sub-NDARFV780ABD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [304701,\n",
       "   305157,\n",
       "   305244,\n",
       "   305333,\n",
       "   305417,\n",
       "   305614,\n",
       "   305781,\n",
       "   306294,\n",
       "   306381]},\n",
       " 'sub-NDARFW038ZNE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [306485, 306571, 306631, 306713, 306886, 307027, 307315]},\n",
       " 'sub-NDARFW972KFQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [307417,\n",
       "   307766,\n",
       "   307852,\n",
       "   307911,\n",
       "   307994,\n",
       "   308179,\n",
       "   308360,\n",
       "   308731,\n",
       "   308810]},\n",
       " 'sub-NDARFX455JFY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [308912, 309325, 309411, 309471, 309553, 309727, 309887, 310197]},\n",
       " 'sub-NDARFX626DFH': {'tasks': ['RestingState', 'seqLearning6target'],\n",
       "  'starts': [310299, 310480]},\n",
       " 'sub-NDARFX710UZA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [310641,\n",
       "   311008,\n",
       "   311095,\n",
       "   311154,\n",
       "   311237,\n",
       "   311468,\n",
       "   311695,\n",
       "   311971,\n",
       "   312050]},\n",
       " 'sub-NDARGA056TMW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [312152,\n",
       "   312864,\n",
       "   312950,\n",
       "   313009,\n",
       "   313092,\n",
       "   313267,\n",
       "   313393,\n",
       "   313764,\n",
       "   313844]},\n",
       " 'sub-NDARGA499CKF': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [313946, 314034, 314095, 314180, 314423, 314645, 315203, 315322]},\n",
       " 'sub-NDARGB040MGR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [315426, 315868, 315954, 316013, 316095, 316268, 316391, 316761]},\n",
       " 'sub-NDARGB102NWJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [316863,\n",
       "   317217,\n",
       "   317305,\n",
       "   317366,\n",
       "   317450,\n",
       "   317631,\n",
       "   317751,\n",
       "   318355,\n",
       "   318471]},\n",
       " 'sub-NDARGB441VVD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [318575, 319172, 319260, 319321, 319405, 319596, 319781, 320322]},\n",
       " 'sub-NDARGB866MPN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [320426, 320513, 320574, 320771, 320934]},\n",
       " 'sub-NDARGB998HX3': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [321154, 321277, 321351, 321479, 321675, 321848]},\n",
       " 'sub-NDARGC170UK2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [321951, 322313, 322399, 322459, 322541, 322714, 322852, 323178]},\n",
       " 'sub-NDARGC559LN8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [323280, 323668, 323754, 323813, 323896, 324076, 324240, 324526]},\n",
       " 'sub-NDARGD414HBX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [324628,\n",
       "   324901,\n",
       "   324989,\n",
       "   325050,\n",
       "   325135,\n",
       "   325308,\n",
       "   325510,\n",
       "   326009,\n",
       "   326101]},\n",
       " 'sub-NDARGD507TDZ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [326205, 326308, 326383, 326496, 326802]},\n",
       " 'sub-NDARGE994BMX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [326904,\n",
       "   327299,\n",
       "   327385,\n",
       "   327445,\n",
       "   327527,\n",
       "   327700,\n",
       "   327860,\n",
       "   328191,\n",
       "   328275]},\n",
       " 'sub-NDARGF192VD1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [328377, 328923, 329009, 329069, 329152, 329433, 329552, 329890]},\n",
       " 'sub-NDARGF367KVL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [329992, 330629, 330715, 330775, 330858, 331030, 331147, 331533]},\n",
       " 'sub-NDARGG205WVN': {'tasks': ['contrastChangeDetection',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [331635, 331762, 331968, 332376, 332534, 332664]},\n",
       " 'sub-NDARGH592NZ2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [332748,\n",
       "   333015,\n",
       "   333102,\n",
       "   333163,\n",
       "   333248,\n",
       "   333424,\n",
       "   333572,\n",
       "   334054,\n",
       "   334140]},\n",
       " 'sub-NDARGH775KF5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [334244,\n",
       "   335171,\n",
       "   335257,\n",
       "   335316,\n",
       "   335400,\n",
       "   335656,\n",
       "   335843,\n",
       "   336289,\n",
       "   336398]},\n",
       " 'sub-NDARGH790CEF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [336500, 337270, 337356, 337467, 337753, 338002, 338157]},\n",
       " 'sub-NDARGJ878ZP4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [338270,\n",
       "   338954,\n",
       "   339040,\n",
       "   339099,\n",
       "   339182,\n",
       "   339356,\n",
       "   339474,\n",
       "   339860,\n",
       "   339943]},\n",
       " 'sub-NDARGK442YHH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [340045,\n",
       "   340426,\n",
       "   340512,\n",
       "   340571,\n",
       "   340654,\n",
       "   340830,\n",
       "   340955,\n",
       "   341241,\n",
       "   341327]},\n",
       " 'sub-NDARGK736HF4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [341429,\n",
       "   342350,\n",
       "   342436,\n",
       "   342495,\n",
       "   342578,\n",
       "   342755,\n",
       "   342959,\n",
       "   343631,\n",
       "   343754]},\n",
       " 'sub-NDARGL800LDW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [343856, 344353, 344439, 344521, 344703, 345111, 345193]},\n",
       " 'sub-NDARGN721GKT': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [345295, 345469, 345599, 345806]},\n",
       " 'sub-NDARGP132PYA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [345886, 346316, 346403, 346463, 346546, 346728, 346954, 347398]},\n",
       " 'sub-NDARGP191YHN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [347502,\n",
       "   347864,\n",
       "   347951,\n",
       "   348010,\n",
       "   348093,\n",
       "   348269,\n",
       "   348412,\n",
       "   348685,\n",
       "   348775]},\n",
       " 'sub-NDARGP399TDM': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [348877, 349057, 349235, 349479]},\n",
       " 'sub-NDARGP984YLN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [349562, 349668, 349741, 349836, 350105]},\n",
       " 'sub-NDARGR106TJB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [350209,\n",
       "   350721,\n",
       "   350809,\n",
       "   350871,\n",
       "   350955,\n",
       "   351232,\n",
       "   351434,\n",
       "   351934,\n",
       "   352021]},\n",
       " 'sub-NDARGT551AFK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [352125,\n",
       "   352521,\n",
       "   352609,\n",
       "   352671,\n",
       "   352755,\n",
       "   353027,\n",
       "   353163,\n",
       "   353759,\n",
       "   353847]},\n",
       " 'sub-NDARGT634DUJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [353951,\n",
       "   354303,\n",
       "   354389,\n",
       "   354448,\n",
       "   354531,\n",
       "   354704,\n",
       "   354862,\n",
       "   355138,\n",
       "   355219]},\n",
       " 'sub-NDARGT682ZWN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [355321,\n",
       "   355864,\n",
       "   355950,\n",
       "   356009,\n",
       "   356091,\n",
       "   356264,\n",
       "   356410,\n",
       "   356780,\n",
       "   356867]},\n",
       " 'sub-NDARGU182ELW': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [356969, 357146, 357270, 359079]},\n",
       " 'sub-NDARGU271CPG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [359159,\n",
       "   359454,\n",
       "   359542,\n",
       "   359604,\n",
       "   359688,\n",
       "   359912,\n",
       "   360048,\n",
       "   360520,\n",
       "   360633]},\n",
       " 'sub-NDARGV436PFT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [360737,\n",
       "   361109,\n",
       "   361195,\n",
       "   361255,\n",
       "   361337,\n",
       "   361510,\n",
       "   361625,\n",
       "   361893,\n",
       "   362013]},\n",
       " 'sub-NDARGV781AMW': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [362115, 362635, 362809, 363008, 363295]},\n",
       " 'sub-NDARGW404GJL': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [363416, 363502, 363561, 363644, 363819, 363935, 364389, 364467]},\n",
       " 'sub-NDARGX001CB1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [364569, 365654, 365741, 365802, 365946, 366140, 366333, 366873]},\n",
       " 'sub-NDARGY959RN8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [366977, 367877, 367965, 368026, 368110, 368287, 368448, 369008]},\n",
       " 'sub-NDARGZ038DRL': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [369112, 369463, 369679, 370310]},\n",
       " 'sub-NDARGZ553TMQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [370469,\n",
       "   370701,\n",
       "   370788,\n",
       "   370849,\n",
       "   370934,\n",
       "   371112,\n",
       "   371242,\n",
       "   371801,\n",
       "   371918]},\n",
       " 'sub-NDARHA387FPM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [372022,\n",
       "   372739,\n",
       "   372825,\n",
       "   372885,\n",
       "   372968,\n",
       "   373147,\n",
       "   373317,\n",
       "   373764,\n",
       "   373855]},\n",
       " 'sub-NDARHB764VZ2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [373958, 374739, 374825, 374884, 374967, 375144, 375256, 375642]},\n",
       " 'sub-NDARHC462NGR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [375744,\n",
       "   375890,\n",
       "   375996,\n",
       "   376071,\n",
       "   376171,\n",
       "   376362,\n",
       "   376535,\n",
       "   376765,\n",
       "   376918]},\n",
       " 'sub-NDARHC661KGK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [377022, 377110, 377171, 377255, 377695]},\n",
       " 'sub-NDARHC814ZZL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [377799, 378025, 378112, 378173, 378257, 378432, 378552, 379002]},\n",
       " 'sub-NDARHE283KZN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [379105,\n",
       "   379497,\n",
       "   379584,\n",
       "   379644,\n",
       "   379726,\n",
       "   379899,\n",
       "   380061,\n",
       "   380337,\n",
       "   380437]},\n",
       " 'sub-NDARHF150FDY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [380539, 380950, 381038, 381099, 381183, 381434, 381623, 382169]},\n",
       " 'sub-NDARHF351AA6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [382273, 382823, 382991, 383079, 383162, 383338, 383525, 383658]},\n",
       " 'sub-NDARHF545HFW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [383760,\n",
       "   384317,\n",
       "   384403,\n",
       "   384463,\n",
       "   384545,\n",
       "   384718,\n",
       "   384836,\n",
       "   385144,\n",
       "   385227]},\n",
       " 'sub-NDARHG260BM9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [385329,\n",
       "   385714,\n",
       "   385800,\n",
       "   385859,\n",
       "   385942,\n",
       "   386146,\n",
       "   386285,\n",
       "   386566,\n",
       "   386651]},\n",
       " 'sub-NDARHH654ZF9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [386753,\n",
       "   387152,\n",
       "   387240,\n",
       "   387341,\n",
       "   387425,\n",
       "   387636,\n",
       "   387839,\n",
       "   388392,\n",
       "   388538]},\n",
       " 'sub-NDARHH819LH3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [388642, 388984, 389071, 389132, 389339, 389536, 389681, 390258]},\n",
       " 'sub-NDARHJ318KZA': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [390362, 390477, 390574, 390669, 390883, 391059, 391172]},\n",
       " 'sub-NDARHL684WYU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [391276, 391758, 391844, 391903, 391986, 392158, 392436, 392635]},\n",
       " 'sub-NDARHL822EJK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [392737, 393092, 393178, 393238, 393320, 393493, 393620, 393905]},\n",
       " 'sub-NDARHM413HVA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [394007,\n",
       "   394405,\n",
       "   394492,\n",
       "   394554,\n",
       "   394637,\n",
       "   394833,\n",
       "   394966,\n",
       "   395602,\n",
       "   395686]},\n",
       " 'sub-NDARHM421JUP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [395790, 396221, 396307, 396367, 396449, 396626, 396771, 397169]},\n",
       " 'sub-NDARHM611GDR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [397271, 397639, 397725, 397785, 397867, 398040, 398193, 398475]},\n",
       " 'sub-NDARHN078CDT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [398578, 399083, 399169, 399229, 399312, 399527, 399689, 400033]},\n",
       " 'sub-NDARHN158DC6': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [400135, 400235, 400310, 400406, 400590, 400764]},\n",
       " 'sub-NDARHN224TPA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [400866,\n",
       "   401297,\n",
       "   401383,\n",
       "   401443,\n",
       "   401525,\n",
       "   401702,\n",
       "   401929,\n",
       "   402235,\n",
       "   402337]},\n",
       " 'sub-NDARHN482HPM': {'tasks': ['RestingState'], 'starts': [402439]},\n",
       " 'sub-NDARHP039DBU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [402679,\n",
       "   403088,\n",
       "   403174,\n",
       "   403234,\n",
       "   403316,\n",
       "   403489,\n",
       "   403673,\n",
       "   403946,\n",
       "   404024]},\n",
       " 'sub-NDARHP705RFA': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [404126, 404214, 404275, 404360, 404538, 404689, 405140, 405225]},\n",
       " 'sub-NDARHP841RMR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [405329,\n",
       "   405778,\n",
       "   405864,\n",
       "   405924,\n",
       "   406006,\n",
       "   406179,\n",
       "   406301,\n",
       "   406659,\n",
       "   406743]},\n",
       " 'sub-NDARHP924ZHW': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [406845, 406975, 407049, 407131, 407303, 407920, 408245]},\n",
       " 'sub-NDARHR372GJ7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [408347,\n",
       "   408745,\n",
       "   408831,\n",
       "   408890,\n",
       "   408972,\n",
       "   409144,\n",
       "   409272,\n",
       "   409650,\n",
       "   409728]},\n",
       " 'sub-NDARHR443EHF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [409830, 410481, 410567, 410626, 410709, 410883, 411015]},\n",
       " 'sub-NDARHR753ZKU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [411273,\n",
       "   412044,\n",
       "   412130,\n",
       "   412189,\n",
       "   412272,\n",
       "   412466,\n",
       "   412651,\n",
       "   413037,\n",
       "   413117]},\n",
       " 'sub-NDARHR763RB4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [413219, 413712, 414179, 414366, 414528]},\n",
       " 'sub-NDARHT774ZK1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [414745,\n",
       "   415132,\n",
       "   415218,\n",
       "   415278,\n",
       "   415360,\n",
       "   415531,\n",
       "   415643,\n",
       "   415934,\n",
       "   416016]},\n",
       " 'sub-NDARHU211PYD': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [416118, 416218, 416325, 416421, 416883]},\n",
       " 'sub-NDARHU423YXP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [416985, 417086, 417159, 417256, 417436, 417564]},\n",
       " 'sub-NDARHU831NXU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [417666, 417926, 418012, 418132, 418245, 418463, 418982]},\n",
       " 'sub-NDARHX145ZKV': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [419084, 419175, 419238, 419324, 419523]},\n",
       " 'sub-NDARHY177LY9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [419627, 420309, 420395, 420454, 420537, 420713, 420859, 421229]},\n",
       " 'sub-NDARHY255FVU': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [421331, 421433, 421511, 421615, 421842, 422349]},\n",
       " 'sub-NDARHZ255RA6': {'tasks': ['DespicableMe', 'RestingState'],\n",
       "  'starts': [422453, 422539]},\n",
       " 'sub-NDARHZ413DZL': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [422789, 422888, 422982, 423075, 423447, 423641, 423744]},\n",
       " 'sub-NDARJA157YB3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [423848,\n",
       "   424386,\n",
       "   424472,\n",
       "   424531,\n",
       "   424614,\n",
       "   424773,\n",
       "   424896,\n",
       "   425233,\n",
       "   425301]},\n",
       " 'sub-NDARJA737FAJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [425403, 425825, 425911, 425970, 426053, 426227, 426380, 426662]},\n",
       " 'sub-NDARJB027GMR': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [426764, 426851, 426930, 427036, 427210, 427724]},\n",
       " 'sub-NDARJD235KZA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [427826, 428219, 428306, 428365, 428448, 428720, 428800]},\n",
       " 'sub-NDARJD387HWG': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [428902, 429006, 429082, 429189, 429489, 429859]},\n",
       " 'sub-NDARJE070PLA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [429961,\n",
       "   430458,\n",
       "   430544,\n",
       "   430604,\n",
       "   430686,\n",
       "   430860,\n",
       "   431033,\n",
       "   431334,\n",
       "   431434]},\n",
       " 'sub-NDARJF027UGB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [431536, 431959, 432047, 432109, 432192, 432380, 432526, 433063]},\n",
       " 'sub-NDARJG300HWH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [433167,\n",
       "   433787,\n",
       "   433873,\n",
       "   433932,\n",
       "   434015,\n",
       "   434293,\n",
       "   434448,\n",
       "   434876,\n",
       "   434959]},\n",
       " 'sub-NDARJG477THE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [435061,\n",
       "   435709,\n",
       "   435795,\n",
       "   435854,\n",
       "   435936,\n",
       "   436178,\n",
       "   436409,\n",
       "   436802,\n",
       "   436922]},\n",
       " 'sub-NDARJG488XHT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [437024,\n",
       "   437559,\n",
       "   437645,\n",
       "   437704,\n",
       "   437787,\n",
       "   437962,\n",
       "   438163,\n",
       "   438495,\n",
       "   438567]},\n",
       " 'sub-NDARJG494YDY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [438669, 439032, 439118, 439178, 439260, 439435, 439574, 439855]},\n",
       " 'sub-NDARJG687YYX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [439957, 440317, 440404, 440463, 440546, 440765, 440916, 441184]},\n",
       " 'sub-NDARJG821GH3': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [441286, 441372, 441431, 441514, 441688]},\n",
       " 'sub-NDARJH513HZX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [441790,\n",
       "   442191,\n",
       "   442279,\n",
       "   442341,\n",
       "   442425,\n",
       "   442680,\n",
       "   442869,\n",
       "   443426,\n",
       "   443512]},\n",
       " 'sub-NDARJJ343TR0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [443616,\n",
       "   444168,\n",
       "   444255,\n",
       "   444317,\n",
       "   444401,\n",
       "   444605,\n",
       "   444771,\n",
       "   445325,\n",
       "   445461]},\n",
       " 'sub-NDARJJ512AYJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [445565,\n",
       "   446156,\n",
       "   446242,\n",
       "   446302,\n",
       "   446384,\n",
       "   446566,\n",
       "   446678,\n",
       "   447092,\n",
       "   447262]},\n",
       " 'sub-NDARJJ638NAC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [447364,\n",
       "   447767,\n",
       "   447853,\n",
       "   447912,\n",
       "   447995,\n",
       "   448169,\n",
       "   448332,\n",
       "   448606,\n",
       "   448766]},\n",
       " 'sub-NDARJJ817UP1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [448868,\n",
       "   449138,\n",
       "   449226,\n",
       "   449286,\n",
       "   449371,\n",
       "   449584,\n",
       "   449716,\n",
       "   450214,\n",
       "   450315]},\n",
       " 'sub-NDARJM618NZH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [450419, 450508, 450570, 450655, 451301]},\n",
       " 'sub-NDARJM828PAL': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [451405, 451510, 451591, 451695, 452199]},\n",
       " 'sub-NDARJN928UPY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [452303, 452742, 452830, 452891, 452975, 453175, 453325, 453774]},\n",
       " 'sub-NDARJP444DZM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [453877, 454621, 454707, 454766, 454849, 455094, 455300, 455712]},\n",
       " 'sub-NDARJR473HXT': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [455809, 455983, 456177, 457124]},\n",
       " 'sub-NDARJT064LRE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [457216,\n",
       "   457575,\n",
       "   457661,\n",
       "   457720,\n",
       "   457803,\n",
       "   457975,\n",
       "   458134,\n",
       "   458426,\n",
       "   458506]},\n",
       " 'sub-NDARJT172UE0': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [458608, 458782, 458929, 459413]},\n",
       " 'sub-NDARJT517YHZ': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [459504, 460461, 460645, 460811, 461043]},\n",
       " 'sub-NDARJT556KAM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [461127,\n",
       "   461791,\n",
       "   461877,\n",
       "   461936,\n",
       "   462019,\n",
       "   462234,\n",
       "   462427,\n",
       "   462801,\n",
       "   462884]},\n",
       " 'sub-NDARJU805JG5': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [462986, 463086, 463159, 463275, 463582]},\n",
       " 'sub-NDARJV225EYT': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [463684, 463872, 463985, 465600]},\n",
       " 'sub-NDARJV268PNW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [465704,\n",
       "   466266,\n",
       "   466352,\n",
       "   466411,\n",
       "   466494,\n",
       "   466666,\n",
       "   466848,\n",
       "   467219,\n",
       "   467300]},\n",
       " 'sub-NDARJV377HG4': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [467402, 467488, 467562, 467660, 467926, 468133, 468560, 468672]},\n",
       " 'sub-NDARJV411EH6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [468774, 469469, 469557, 469618, 469794, 469938, 470400, 470481]},\n",
       " 'sub-NDARJW373UE3': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [470585, 470694, 470923, 471049, 471194]},\n",
       " 'sub-NDARJY033DKZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [471272,\n",
       "   471925,\n",
       "   472011,\n",
       "   472070,\n",
       "   472153,\n",
       "   472421,\n",
       "   472632,\n",
       "   473023,\n",
       "   473103]},\n",
       " 'sub-NDARJY141RFE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [473205, 473839, 473925, 473985, 474067, 474247, 474466, 474880]},\n",
       " 'sub-NDARJY747PRJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [474982, 475421, 475508, 475570, 475654, 475906, 476065, 476586]},\n",
       " 'sub-NDARJZ089HVP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [476689,\n",
       "   477430,\n",
       "   477516,\n",
       "   477576,\n",
       "   477659,\n",
       "   477836,\n",
       "   477991,\n",
       "   478395,\n",
       "   478483]},\n",
       " 'sub-NDARJZ274PRQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [478585,\n",
       "   479219,\n",
       "   479305,\n",
       "   479364,\n",
       "   479447,\n",
       "   479620,\n",
       "   479782,\n",
       "   480151,\n",
       "   480236]},\n",
       " 'sub-NDARJZ526HN3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [480338, 481056, 481142, 481202, 481285, 481465, 481594, 482068]},\n",
       " 'sub-NDARKA085YRG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [482170, 482626, 482712, 482771, 482853, 483246, 483400, 483898]},\n",
       " 'sub-NDARKA627LF8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [483977,\n",
       "   484690,\n",
       "   484776,\n",
       "   484835,\n",
       "   484918,\n",
       "   485091,\n",
       "   485271,\n",
       "   485678,\n",
       "   485764]},\n",
       " 'sub-NDARKB614KGY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [485866,\n",
       "   486227,\n",
       "   486313,\n",
       "   486373,\n",
       "   486456,\n",
       "   486627,\n",
       "   486762,\n",
       "   487036,\n",
       "   487116]},\n",
       " 'sub-NDARKC143BEE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [487218,\n",
       "   487905,\n",
       "   487991,\n",
       "   488050,\n",
       "   488133,\n",
       "   488307,\n",
       "   488455,\n",
       "   488876,\n",
       "   488955]},\n",
       " 'sub-NDARKC220LZG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [489057, 489484, 489572, 489632, 489809, 489907, 491147, 491267]},\n",
       " 'sub-NDARKC880ZHY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [491370, 491960, 492046, 492105, 492188, 492371, 492503, 492940]},\n",
       " 'sub-NDARKC978MR4': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [493042, 493255, 493437, 494577]},\n",
       " 'sub-NDARKD134TCX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [494664,\n",
       "   495481,\n",
       "   495568,\n",
       "   495627,\n",
       "   495788,\n",
       "   495989,\n",
       "   496119,\n",
       "   496491,\n",
       "   496579]},\n",
       " 'sub-NDARKE358TFU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [496681,\n",
       "   496972,\n",
       "   497059,\n",
       "   497120,\n",
       "   497204,\n",
       "   497378,\n",
       "   497509,\n",
       "   497956,\n",
       "   498047]},\n",
       " 'sub-NDARKE456BVQ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [498151, 498248, 498321, 498415, 498671, 498883, 499079]},\n",
       " 'sub-NDARKE650TFQ': {'tasks': ['DespicableMe', 'RestingState'],\n",
       "  'starts': [499183, 499269]},\n",
       " 'sub-NDARKF704MKL': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [499431, 499605, 499789]},\n",
       " 'sub-NDARKF779EK2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [500809,\n",
       "   501201,\n",
       "   501287,\n",
       "   501347,\n",
       "   501429,\n",
       "   501601,\n",
       "   501735,\n",
       "   502003,\n",
       "   502087]},\n",
       " 'sub-NDARKG697CEW': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [502189, 502276, 502336, 502421, 502650, 502826, 503447, 503531]},\n",
       " 'sub-NDARKG718VF7': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [503635, 504365, 504544, 504692, 504823]},\n",
       " 'sub-NDARKH291KRE': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [504906, 505146, 505316, 506617]},\n",
       " 'sub-NDARKH837TB2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [506711,\n",
       "   507356,\n",
       "   507442,\n",
       "   507502,\n",
       "   507585,\n",
       "   507763,\n",
       "   507863,\n",
       "   508237,\n",
       "   508317]},\n",
       " 'sub-NDARKJ871YD6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [508419, 508838, 508924, 508983, 509066, 509247, 509470, 509756]},\n",
       " 'sub-NDARKK597VDH': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [509859, 510766, 510967, 511283, 511681]},\n",
       " 'sub-NDARKK774VP5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [511879, 512274, 512360, 512420, 512502, 512676, 512847, 513119]},\n",
       " 'sub-NDARKM250ET5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [513221,\n",
       "   513659,\n",
       "   513745,\n",
       "   513805,\n",
       "   513887,\n",
       "   514058,\n",
       "   514240,\n",
       "   514510,\n",
       "   514594]},\n",
       " 'sub-NDARKM635UY0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [514696, 515275, 515363, 515424, 515508, 515721, 515912, 516504]},\n",
       " 'sub-NDARKM941XLL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [516608, 517056, 517144, 517205, 517289, 517488, 517649, 518234]},\n",
       " 'sub-NDARKN175HWB': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [518338, 518572, 518711, 519331]},\n",
       " 'sub-NDARKN346XZH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [519492, 519597, 519699, 519800, 519979, 520148, 520257]},\n",
       " 'sub-NDARKN477EBH': {'tasks': ['contrastChangeDetection',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [520359, 521069, 521130, 521380, 521579, 522157, 522339]},\n",
       " 'sub-NDARKP414AZ5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [522443, 522685, 522773, 522834, 522919, 523143, 523275, 523760]},\n",
       " 'sub-NDARKP815KPZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [523863,\n",
       "   524640,\n",
       "   524726,\n",
       "   524785,\n",
       "   524868,\n",
       "   525153,\n",
       "   525361,\n",
       "   525809,\n",
       "   525908]},\n",
       " 'sub-NDARKT032RYR': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [526010, 526109, 526182, 526311, 526487, 526737, 526833]},\n",
       " 'sub-NDARKT811ATJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [526935,\n",
       "   527540,\n",
       "   527628,\n",
       "   527690,\n",
       "   527775,\n",
       "   527971,\n",
       "   528135,\n",
       "   528775,\n",
       "   528866]},\n",
       " 'sub-NDARKT952ML4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [528970,\n",
       "   529189,\n",
       "   529277,\n",
       "   529339,\n",
       "   529423,\n",
       "   529612,\n",
       "   529813,\n",
       "   530307,\n",
       "   530415]},\n",
       " 'sub-NDARKU198KFL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [530519,\n",
       "   531138,\n",
       "   531226,\n",
       "   531287,\n",
       "   531370,\n",
       "   531571,\n",
       "   531716,\n",
       "   532231,\n",
       "   532316]},\n",
       " 'sub-NDARKU278YRR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [532419,\n",
       "   533090,\n",
       "   533178,\n",
       "   533239,\n",
       "   533323,\n",
       "   533515,\n",
       "   533642,\n",
       "   534134,\n",
       "   534220]},\n",
       " 'sub-NDARKU616UBL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [534324, 535028, 535114, 535173, 535256, 535430, 535564, 536028]},\n",
       " 'sub-NDARKV800DTW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [536130, 536396, 536484, 536545, 536630, 536806, 536933, 537402]},\n",
       " 'sub-NDARKV965WLU': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [537506, 537628, 537704, 537787, 537961, 538612]},\n",
       " 'sub-NDARKW565ZT9': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [538714, 538889, 539003, 540331]},\n",
       " 'sub-NDARKX701BJ4': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [540428, 540876, 541036, 541177]},\n",
       " 'sub-NDARKY667THK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [541408,\n",
       "   541927,\n",
       "   542015,\n",
       "   542077,\n",
       "   542161,\n",
       "   542354,\n",
       "   542493,\n",
       "   543044,\n",
       "   543132]},\n",
       " 'sub-NDARKZ031NJZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [543236, 543447, 543535, 543620, 543703, 543881, 544002, 544399]},\n",
       " 'sub-NDARKZ198TAA': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [544502, 544849, 545067, 546393]},\n",
       " 'sub-NDARLA998XAC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [546588,\n",
       "   546892,\n",
       "   546980,\n",
       "   547058,\n",
       "   547141,\n",
       "   547324,\n",
       "   547451,\n",
       "   547943,\n",
       "   548104]},\n",
       " 'sub-NDARLD287MLP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [548207,\n",
       "   548662,\n",
       "   548748,\n",
       "   548808,\n",
       "   548890,\n",
       "   549065,\n",
       "   549228,\n",
       "   549533,\n",
       "   549616]},\n",
       " 'sub-NDARLF142AF5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [549718,\n",
       "   550372,\n",
       "   550458,\n",
       "   550517,\n",
       "   550600,\n",
       "   550920,\n",
       "   551073,\n",
       "   551511,\n",
       "   551590]},\n",
       " 'sub-NDARLF484WJL': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [551692, 551878, 552023, 553383]},\n",
       " 'sub-NDARLF616PBU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [553486, 553967, 554055, 554116, 554200, 554557, 554709, 555381]},\n",
       " 'sub-NDARLG918MG8': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [555485, 555708, 555871]},\n",
       " 'sub-NDARLH263KCL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [556078,\n",
       "   556396,\n",
       "   556484,\n",
       "   556570,\n",
       "   556654,\n",
       "   556849,\n",
       "   557023,\n",
       "   557505,\n",
       "   557585]},\n",
       " 'sub-NDARLH979WFX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [557689,\n",
       "   558615,\n",
       "   558704,\n",
       "   558765,\n",
       "   558849,\n",
       "   559054,\n",
       "   559200,\n",
       "   559773,\n",
       "   559871]},\n",
       " 'sub-NDARLJ168LXY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [559975,\n",
       "   560258,\n",
       "   560345,\n",
       "   560406,\n",
       "   560490,\n",
       "   560669,\n",
       "   560793,\n",
       "   561218,\n",
       "   561312]},\n",
       " 'sub-NDARLJ280UCN': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch'],\n",
       "  'starts': [561415, 562645, 562833, 562963]},\n",
       " 'sub-NDARLJ356HAQ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'symbolSearch'],\n",
       "  'starts': [563172, 563275, 563334, 563436, 563617]},\n",
       " 'sub-NDARLJ654TZ0': {'tasks': ['DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [563823, 563909, 563992, 564193, 564354, 564491]},\n",
       " 'sub-NDARLJ886BFK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [564578, 564664, 564724, 564807, 564979, 565089, 565494]},\n",
       " 'sub-NDARLK181GL4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [565596,\n",
       "   566130,\n",
       "   566217,\n",
       "   566276,\n",
       "   566359,\n",
       "   566687,\n",
       "   566810,\n",
       "   567141,\n",
       "   567228]},\n",
       " 'sub-NDARLK963HXP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [567330, 568021, 568107, 568167, 568250, 568424, 568551, 568938]},\n",
       " 'sub-NDARLL720BGU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [569041,\n",
       "   569772,\n",
       "   569858,\n",
       "   569917,\n",
       "   570000,\n",
       "   570180,\n",
       "   570389,\n",
       "   570815,\n",
       "   570929]},\n",
       " 'sub-NDARLL914UW4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [571031,\n",
       "   571672,\n",
       "   571758,\n",
       "   571817,\n",
       "   571900,\n",
       "   572077,\n",
       "   572253,\n",
       "   572631,\n",
       "   572712]},\n",
       " 'sub-NDARLN070VX7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [572814, 573768, 573854, 573913, 574076, 574271, 574461, 574923]},\n",
       " 'sub-NDARLN550RM6': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [575025, 575113, 575216, 575300, 575475, 575875, 576027]},\n",
       " 'sub-NDARLN778RYN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [576131, 576217, 576277, 576360, 576533, 576688]},\n",
       " 'sub-NDARLR030EW6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [576790, 577505, 577592, 577651, 577734, 577954, 578082, 578482]},\n",
       " 'sub-NDARLR039GJL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [578584, 578986, 579072, 579131, 579214, 579394, 579518, 579895]},\n",
       " 'sub-NDARLR083PKF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [579997,\n",
       "   580355,\n",
       "   580441,\n",
       "   580501,\n",
       "   580583,\n",
       "   580762,\n",
       "   580899,\n",
       "   581168,\n",
       "   581247]},\n",
       " 'sub-NDARLT314TWP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [581349, 581687, 581775, 581837, 581921, 582101, 582289, 582755]},\n",
       " 'sub-NDARLT365LHK': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [582859, 583030, 583144]},\n",
       " 'sub-NDARLU606ZDD': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [583957, 584044, 584104, 584186, 584437, 585192]},\n",
       " 'sub-NDARLV155RW0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [585294,\n",
       "   585595,\n",
       "   585683,\n",
       "   585744,\n",
       "   585857,\n",
       "   586045,\n",
       "   586194,\n",
       "   586625,\n",
       "   586775]},\n",
       " 'sub-NDARLV252DY2': {'tasks': ['RestingState', 'seqLearning8target'],\n",
       "  'starts': [586879, 587098]},\n",
       " 'sub-NDARLV387GP4': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [587279, 587379, 587452, 587550, 587915]},\n",
       " 'sub-NDARLV812JXX': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [588018, 588106, 588167, 588250, 588422, 588592, 589109]},\n",
       " 'sub-NDARLY585ZVN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [589212, 589763, 589849, 589908, 589991, 590164, 590279, 590655]},\n",
       " 'sub-NDARLY872ZXA': {'tasks': ['surroundSupp'], 'starts': [590757]},\n",
       " 'sub-NDARLZ007NHG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [590887, 591242, 591330, 591391, 591474, 591793, 591975, 592464]},\n",
       " 'sub-NDARMA154FAK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [592567,\n",
       "   592934,\n",
       "   593022,\n",
       "   593369,\n",
       "   593453,\n",
       "   593728,\n",
       "   593956,\n",
       "   594431,\n",
       "   594564]},\n",
       " 'sub-NDARMA390CHB': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [594668, 594853, 595019, 596392]},\n",
       " 'sub-NDARMA598JTX': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [596475, 596596, 596871, 597112, 597319, 597434, 597747]},\n",
       " 'sub-NDARMC481AWW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [597851, 598165, 598253, 598313, 598488, 598809, 598919, 599405]},\n",
       " 'sub-NDARMC760NEC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [599509, 600080, 600166, 600226, 600309, 600484, 600615, 600988]},\n",
       " 'sub-NDARMD134JRU': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [601090, 601191, 601296, 601422, 601642]},\n",
       " 'sub-NDARMD575AXD': {'tasks': ['DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch'],\n",
       "  'starts': [601746, 601847, 602002, 602331, 602453]},\n",
       " 'sub-NDARMD638CM6': {'tasks': ['RestingState'], 'starts': [602533]},\n",
       " 'sub-NDARME573TRB': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'surroundSupp'],\n",
       "  'starts': [602873, 602934, 603110]},\n",
       " 'sub-NDARME689TAF': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [603390, 603490, 603594, 603677, 603855]},\n",
       " 'sub-NDARME789TD2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [603959, 604397, 604483, 604543, 604626, 604627, 604899, 605172]},\n",
       " 'sub-NDARMF116AFR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [605274,\n",
       "   605747,\n",
       "   605834,\n",
       "   605895,\n",
       "   605979,\n",
       "   606202,\n",
       "   606357,\n",
       "   606910,\n",
       "   607003]},\n",
       " 'sub-NDARMG239GUV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [607107,\n",
       "   607435,\n",
       "   607522,\n",
       "   607584,\n",
       "   607668,\n",
       "   607844,\n",
       "   607936,\n",
       "   608461,\n",
       "   608553]},\n",
       " 'sub-NDARMG675JB4': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [608657, 608760, 608834, 608942, 609120, 609263]},\n",
       " 'sub-NDARMG741FDL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [609366, 609915, 610001, 610060, 610144, 610325, 610507, 610934]},\n",
       " 'sub-NDARMJ849UKD': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [611036, 611138, 611212, 611309, 611597, 611713]},\n",
       " 'sub-NDARML024FF2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [611815, 612551, 612637, 612696, 612779, 612957, 613111, 613489]},\n",
       " 'sub-NDARML406ZB8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [613591,\n",
       "   614154,\n",
       "   614241,\n",
       "   614301,\n",
       "   614383,\n",
       "   614593,\n",
       "   614723,\n",
       "   615200,\n",
       "   615313]},\n",
       " 'sub-NDARMM005DLK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [615415, 616721, 616807, 616867, 616950, 617161, 617288, 617673]},\n",
       " 'sub-NDARMM137PT6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [617768,\n",
       "   618429,\n",
       "   618515,\n",
       "   618574,\n",
       "   618657,\n",
       "   618832,\n",
       "   618977,\n",
       "   619353,\n",
       "   619435]},\n",
       " 'sub-NDARMM326JCB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [619537,\n",
       "   619972,\n",
       "   620060,\n",
       "   620121,\n",
       "   620205,\n",
       "   620390,\n",
       "   620545,\n",
       "   621094,\n",
       "   621183]},\n",
       " 'sub-NDARMM951YEH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [621287, 621926, 622012, 622072, 622155, 622326, 622697, 622780]},\n",
       " 'sub-NDARMN376BMF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [622882, 623006, 623110, 623207, 623306, 623486, 623632, 623854]},\n",
       " 'sub-NDARMN415AXR': {'tasks': ['FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [623958, 624043, 624249, 624388, 624650]},\n",
       " 'sub-NDARMN695VFH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [624753, 625028, 625114, 625175, 625257, 625437, 625613, 625845]},\n",
       " 'sub-NDARMR039JV5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [625947, 626598, 626685, 626744, 626827, 627001, 627133, 627520]},\n",
       " 'sub-NDARMR491LM5': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [627622, 627762, 627854, 628062, 628251, 628378, 628619]},\n",
       " 'sub-NDARMR570GJ5': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [628721, 628808, 628870, 629032, 629232, 629474, 630014]},\n",
       " 'sub-NDARMT064WBB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [630117,\n",
       "   631074,\n",
       "   631160,\n",
       "   631219,\n",
       "   631302,\n",
       "   631492,\n",
       "   631605,\n",
       "   632024,\n",
       "   632108]},\n",
       " 'sub-NDARMT784NED': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [632210, 632642, 632728, 632788, 632870, 633051, 633229, 633651]},\n",
       " 'sub-NDARMU161LHN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [633753,\n",
       "   634248,\n",
       "   634336,\n",
       "   634397,\n",
       "   634481,\n",
       "   634660,\n",
       "   634771,\n",
       "   635293,\n",
       "   635413]},\n",
       " 'sub-NDARMV189NXG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [635517,\n",
       "   635951,\n",
       "   636037,\n",
       "   636097,\n",
       "   636179,\n",
       "   636357,\n",
       "   636546,\n",
       "   636860,\n",
       "   636950]},\n",
       " 'sub-NDARMV718DYL': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [637052, 637112, 637342]},\n",
       " 'sub-NDARMW088NNR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [637444, 637795, 637881, 637941, 638023, 638194, 638330, 638603]},\n",
       " 'sub-NDARMW445NBC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [638705, 638808, 638882, 638980, 639156, 639397]},\n",
       " 'sub-NDARMW473LL1': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [639500, 639699, 639940, 640125]},\n",
       " 'sub-NDARMW551YA0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [640233, 640889, 640977, 641038, 641228, 641376, 641821]},\n",
       " 'sub-NDARMW863HW5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [641925,\n",
       "   642155,\n",
       "   642242,\n",
       "   642303,\n",
       "   642387,\n",
       "   642559,\n",
       "   642714,\n",
       "   643176,\n",
       "   643282]},\n",
       " 'sub-NDARMX277VHC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [643385,\n",
       "   643757,\n",
       "   643843,\n",
       "   643903,\n",
       "   643985,\n",
       "   644158,\n",
       "   644314,\n",
       "   644602,\n",
       "   644685]},\n",
       " 'sub-NDARMX328VWC': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [644787, 644909, 644985, 645086, 645261, 645429, 645605]},\n",
       " 'sub-NDARMZ200GVD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [645707, 646650, 646738, 646799, 646882, 647160, 647380, 648002]},\n",
       " 'sub-NDARMZ518UH1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [648105,\n",
       "   648256,\n",
       "   648343,\n",
       "   648453,\n",
       "   648537,\n",
       "   648757,\n",
       "   648902,\n",
       "   649584,\n",
       "   649730]},\n",
       " 'sub-NDARMZ687XRR': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [649834,\n",
       "   650235,\n",
       "   650323,\n",
       "   650384,\n",
       "   650468,\n",
       "   650658,\n",
       "   650819,\n",
       "   651368,\n",
       "   651461]},\n",
       " 'sub-NDARMZ790VHM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [651565,\n",
       "   651997,\n",
       "   652084,\n",
       "   652144,\n",
       "   652226,\n",
       "   652403,\n",
       "   652507,\n",
       "   652780,\n",
       "   652864]},\n",
       " 'sub-NDARNA004AGP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [652966, 653095, 653182, 653314, 653596]},\n",
       " 'sub-NDARNB427ARE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [653700,\n",
       "   654079,\n",
       "   654165,\n",
       "   654225,\n",
       "   654307,\n",
       "   654483,\n",
       "   654640,\n",
       "   654909,\n",
       "   654987]},\n",
       " 'sub-NDARNB674VMY': {'tasks': ['RestingState'], 'starts': [655089]},\n",
       " 'sub-NDARND085WZK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [655300, 655388, 655449, 655534, 655886, 656084]},\n",
       " 'sub-NDARND348HB3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [656188,\n",
       "   656928,\n",
       "   657016,\n",
       "   657078,\n",
       "   657162,\n",
       "   657345,\n",
       "   657521,\n",
       "   658223,\n",
       "   658324]},\n",
       " 'sub-NDARNE729PN2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [658428,\n",
       "   658729,\n",
       "   658817,\n",
       "   658878,\n",
       "   658961,\n",
       "   659141,\n",
       "   659300,\n",
       "   659750,\n",
       "   659875]},\n",
       " 'sub-NDARNE758RD6': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [659978, 660100, 660190, 660272, 660448, 660620, 661152, 661232]},\n",
       " 'sub-NDARNE800DCT': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [661334, 661522, 661659, 662629]},\n",
       " 'sub-NDARNG968RB9': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [662708, 662810, 662881, 662974, 663180, 663326]},\n",
       " 'sub-NDARNJ899HW7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [663429,\n",
       "   663790,\n",
       "   663876,\n",
       "   663936,\n",
       "   664018,\n",
       "   664192,\n",
       "   664343,\n",
       "   664609,\n",
       "   664692]},\n",
       " 'sub-NDARNK768THB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [664794,\n",
       "   665429,\n",
       "   665515,\n",
       "   665574,\n",
       "   665656,\n",
       "   665867,\n",
       "   666012,\n",
       "   666394,\n",
       "   666472]},\n",
       " 'sub-NDARNM783ZVV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [666575,\n",
       "   667032,\n",
       "   667120,\n",
       "   667182,\n",
       "   667266,\n",
       "   667455,\n",
       "   667583,\n",
       "   668225,\n",
       "   668346]},\n",
       " 'sub-NDARNN142WYB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [668450,\n",
       "   668835,\n",
       "   668921,\n",
       "   668981,\n",
       "   669063,\n",
       "   669235,\n",
       "   669398,\n",
       "   669666,\n",
       "   669757]},\n",
       " 'sub-NDARNN624AMU': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [669859, 670051, 670207]},\n",
       " 'sub-NDARNP370WGP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [671496, 671642, 671719, 671816, 672318]},\n",
       " 'sub-NDARNP643KZH': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [672422, 672599, 672815, 674126]},\n",
       " 'sub-NDARNP812WAQ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [674206, 674292, 674352, 674434, 674612, 674776, 675082, 675176]},\n",
       " 'sub-NDARNR773DL4': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [675278, 676036, 676211, 676375]},\n",
       " 'sub-NDARNT541VT2': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [677026, 677160, 677232, 677327, 677500, 677623, 677877]},\n",
       " 'sub-NDARNU137XR4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [677979,\n",
       "   678692,\n",
       "   678778,\n",
       "   678837,\n",
       "   678920,\n",
       "   679096,\n",
       "   679330,\n",
       "   679711,\n",
       "   679803]},\n",
       " 'sub-NDARNU497GTX': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState'],\n",
       "  'starts': [679905, 679991, 680107, 680189]},\n",
       " 'sub-NDARNV399BV4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [680597,\n",
       "   681317,\n",
       "   681405,\n",
       "   681467,\n",
       "   681551,\n",
       "   681744,\n",
       "   681864,\n",
       "   682416,\n",
       "   682502]},\n",
       " 'sub-NDARNV694EMG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [682606,\n",
       "   682970,\n",
       "   683056,\n",
       "   683116,\n",
       "   683198,\n",
       "   683372,\n",
       "   683511,\n",
       "   683782,\n",
       "   683864]},\n",
       " 'sub-NDARNW687HMZ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [683967, 684068, 684190, 684285, 684460, 684569]},\n",
       " 'sub-NDARNX218UY5': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [684671, 684786, 685134, 685348, 685652]},\n",
       " 'sub-NDARNY205RK8': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [685756, 685929, 686095, 686221]},\n",
       " 'sub-NDARNY217PLD': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [686323, 686424, 686498, 686595, 686769]},\n",
       " 'sub-NDARNY894TH7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [686872, 687144, 687232, 687293, 687376, 687570, 688260, 688452]},\n",
       " 'sub-NDARNZ043VH0': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [688556, 688763, 689009]},\n",
       " 'sub-NDARNZ141GNH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [690232, 690360, 690421, 690522, 690707, 690830, 691256, 691344]},\n",
       " 'sub-NDARNZ615UEU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [691448,\n",
       "   692193,\n",
       "   692279,\n",
       "   692338,\n",
       "   692421,\n",
       "   692631,\n",
       "   692779,\n",
       "   693150,\n",
       "   693257]},\n",
       " 'sub-NDARPA827UBJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [693359, 693866, 693972, 694110, 694192, 694389, 694645, 694913]},\n",
       " 'sub-NDARPB984UTU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [695015, 695820, 695906, 695965, 696048, 696227, 696374, 696770]},\n",
       " 'sub-NDARPC817XZ5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [696872, 697344, 697430, 697489, 697571, 697745, 697862, 698251]},\n",
       " 'sub-NDARPC931KR1': {'tasks': ['RestingState', 'seqLearning6target'],\n",
       "  'starts': [698353, 698530]},\n",
       " 'sub-NDARPD517MDH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [698745,\n",
       "   699522,\n",
       "   699609,\n",
       "   699671,\n",
       "   699755,\n",
       "   699971,\n",
       "   700206,\n",
       "   700874,\n",
       "   700986]},\n",
       " 'sub-NDARPE056ACA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [701090,\n",
       "   701775,\n",
       "   701861,\n",
       "   701921,\n",
       "   702004,\n",
       "   702176,\n",
       "   702298,\n",
       "   702685,\n",
       "   702776]},\n",
       " 'sub-NDARPF395NV5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [702878,\n",
       "   703227,\n",
       "   703313,\n",
       "   703373,\n",
       "   703455,\n",
       "   703639,\n",
       "   703767,\n",
       "   704053,\n",
       "   704135]},\n",
       " 'sub-NDARPF988BBV': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [704237, 704430, 704560, 706277]},\n",
       " 'sub-NDARPG082MTJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [706362, 706461, 706521, 706603, 706778, 706892, 707020, 707102]},\n",
       " 'sub-NDARPG847LB8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [707204, 707563, 707649, 707708, 707791, 707966, 708124, 708396]},\n",
       " 'sub-NDARPG874CMG': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [708498, 708679, 708806, 710249]},\n",
       " 'sub-NDARPH022TRY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [710336,\n",
       "   710774,\n",
       "   710860,\n",
       "   710920,\n",
       "   711002,\n",
       "   711186,\n",
       "   711335,\n",
       "   711652,\n",
       "   711742]},\n",
       " 'sub-NDARPH474YF3': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [711844, 712012, 712102, 712405, 712629, 712853]},\n",
       " 'sub-NDARPH844KP2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [712957, 713545, 713631, 713714, 713891, 714041, 714549]},\n",
       " 'sub-NDARPK223WVZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [714706, 714973, 715061, 715122, 715207, 715415, 715558, 715976]},\n",
       " 'sub-NDARPL479LHN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [716080, 716168, 716229, 716405, 716521, 717478, 717560]},\n",
       " 'sub-NDARPL501ZUU': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [717664, 717836, 717976, 719708]},\n",
       " 'sub-NDARPM572ZZV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [719788, 720519, 720607, 720667, 720852, 720999, 721530, 721616]},\n",
       " 'sub-NDARPN418ZKT': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [721719, 721852, 721968, 722118, 722337, 722486, 722799]},\n",
       " 'sub-NDARPN886HH9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [722901,\n",
       "   723268,\n",
       "   723354,\n",
       "   723413,\n",
       "   723496,\n",
       "   723754,\n",
       "   723892,\n",
       "   724178,\n",
       "   724270]},\n",
       " 'sub-NDARPP150TNR': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [724372, 724475, 724558, 724725, 724901, 725072]},\n",
       " 'sub-NDARPT777FDA': {'tasks': ['RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [725175, 725355, 725481]},\n",
       " 'sub-NDARPU897CM5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [725737, 726446, 726532, 726592, 726675, 726849, 726997, 727465]},\n",
       " 'sub-NDARPV145KGL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [727567,\n",
       "   728146,\n",
       "   728234,\n",
       "   728295,\n",
       "   728379,\n",
       "   728571,\n",
       "   728762,\n",
       "   729379,\n",
       "   729471]},\n",
       " 'sub-NDARPW313LAJ': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [729575, 729806, 729986, 730109]},\n",
       " 'sub-NDARPW577WJ7': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [730240, 730326, 730385, 730549, 730775, 731048, 731387]},\n",
       " 'sub-NDARPW635LN4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [731489, 732171, 732257, 732316, 732399, 732677, 732888, 733311]},\n",
       " 'sub-NDARPX219TW0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [733413,\n",
       "   733708,\n",
       "   733796,\n",
       "   733857,\n",
       "   733941,\n",
       "   734180,\n",
       "   734328,\n",
       "   734832,\n",
       "   734906]},\n",
       " 'sub-NDARPY297MD8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [735010, 735318, 735406, 735467, 735578, 735762, 735896, 736392]},\n",
       " 'sub-NDARPY478YM0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [736496, 736871, 736958, 737017, 737100, 737274, 737468, 737750]},\n",
       " 'sub-NDARPZ020AH8': {'tasks': ['DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [737852, 737938, 738021, 738196, 738534, 738795]},\n",
       " 'sub-NDARPZ720WKW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [738897,\n",
       "   739238,\n",
       "   739325,\n",
       "   739387,\n",
       "   739471,\n",
       "   739697,\n",
       "   739874,\n",
       "   740369,\n",
       "   740487]},\n",
       " 'sub-NDARPZ973RK0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [740591, 740823, 740909, 740969, 741051, 741190]},\n",
       " 'sub-NDARRA383KVQ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [741292, 741423, 741529, 741653, 741881, 742024, 742132]},\n",
       " 'sub-NDARRA717GYV': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [742236, 742418, 742729, 743054]},\n",
       " 'sub-NDARRB942UWU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [743175, 743667, 743754, 743815, 743899, 744102, 744645, 744731]},\n",
       " 'sub-NDARRD943ZWU': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [744835, 744946, 745063, 745162, 745358, 745484]},\n",
       " 'sub-NDARRF415CBE': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [745588, 745778, 745926]},\n",
       " 'sub-NDARRF897HB5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [746151,\n",
       "   747063,\n",
       "   747149,\n",
       "   747209,\n",
       "   747292,\n",
       "   747467,\n",
       "   747658,\n",
       "   748095,\n",
       "   748210]},\n",
       " 'sub-NDARRG499ZWN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [748319,\n",
       "   748919,\n",
       "   749006,\n",
       "   749065,\n",
       "   749147,\n",
       "   749323,\n",
       "   749433,\n",
       "   749808,\n",
       "   749886]},\n",
       " 'sub-NDARRG536CVP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [749988, 750106, 750184, 750307, 750566, 750807, 751002]},\n",
       " 'sub-NDARRH407MEY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [751105, 751270, 751392, 751467, 751581, 751770, 751993, 752209]},\n",
       " 'sub-NDARRJ130PV7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [752313,\n",
       "   752597,\n",
       "   752685,\n",
       "   752746,\n",
       "   752830,\n",
       "   753008,\n",
       "   753116,\n",
       "   753568,\n",
       "   753678]},\n",
       " 'sub-NDARRK135YAZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [753782,\n",
       "   754042,\n",
       "   754129,\n",
       "   754191,\n",
       "   754274,\n",
       "   754502,\n",
       "   754661,\n",
       "   755067,\n",
       "   755152]},\n",
       " 'sub-NDARRK163VY8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [755256,\n",
       "   755579,\n",
       "   755667,\n",
       "   755751,\n",
       "   755835,\n",
       "   756037,\n",
       "   756303,\n",
       "   756882,\n",
       "   756968]},\n",
       " 'sub-NDARRK882CLT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [757072,\n",
       "   757700,\n",
       "   757786,\n",
       "   757846,\n",
       "   757929,\n",
       "   758102,\n",
       "   758236,\n",
       "   758605,\n",
       "   758684]},\n",
       " 'sub-NDARRL660CME': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [758786, 759519, 759696, 759847]},\n",
       " 'sub-NDARRM467NP2': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [760145, 760244, 760317, 760413, 760699]},\n",
       " 'sub-NDARRM586XYL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [760801,\n",
       "   761586,\n",
       "   761674,\n",
       "   761735,\n",
       "   761820,\n",
       "   762010,\n",
       "   762142,\n",
       "   762731,\n",
       "   762812]},\n",
       " 'sub-NDARRP008LJ6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [762916,\n",
       "   763278,\n",
       "   763364,\n",
       "   763424,\n",
       "   763506,\n",
       "   763678,\n",
       "   763807,\n",
       "   764119,\n",
       "   764203]},\n",
       " 'sub-NDARRP163YRC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [764305,\n",
       "   764932,\n",
       "   765020,\n",
       "   765082,\n",
       "   765166,\n",
       "   765347,\n",
       "   765469,\n",
       "   766020,\n",
       "   766109]},\n",
       " 'sub-NDARRP614EBU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [766213,\n",
       "   766424,\n",
       "   766511,\n",
       "   766597,\n",
       "   766681,\n",
       "   766859,\n",
       "   767017,\n",
       "   767486,\n",
       "   767570]},\n",
       " 'sub-NDARRP801AX1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [767673,\n",
       "   768076,\n",
       "   768162,\n",
       "   768221,\n",
       "   768304,\n",
       "   768495,\n",
       "   768642,\n",
       "   768952,\n",
       "   769040]},\n",
       " 'sub-NDARRP818DWL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [769142, 769788, 769874, 769933, 770016, 770199, 770327, 770699]},\n",
       " 'sub-NDARRT038VG1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [770801,\n",
       "   771422,\n",
       "   771508,\n",
       "   771568,\n",
       "   771651,\n",
       "   771846,\n",
       "   772053,\n",
       "   772434,\n",
       "   772515]},\n",
       " 'sub-NDARRW141HUT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [772617, 772974, 773060, 773119, 773202, 773379, 773526, 773794]},\n",
       " 'sub-NDARRW643DHJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [773896, 774000, 774107, 774261, 774495, 774708, 774828]},\n",
       " 'sub-NDARRX819BZ0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [774932,\n",
       "   775789,\n",
       "   775923,\n",
       "   775983,\n",
       "   776065,\n",
       "   776236,\n",
       "   776336,\n",
       "   776720,\n",
       "   776802]},\n",
       " 'sub-NDARRY126FA5': {'tasks': ['RestingState'], 'starts': [776904]},\n",
       " 'sub-NDARRY715CY4': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [777218, 777354, 777435, 777571, 778287]},\n",
       " 'sub-NDARRY807MXC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [778391, 778664, 778752, 778814, 778897, 779077, 779197, 779723]},\n",
       " 'sub-NDARRZ356HET': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [779827,\n",
       "   780047,\n",
       "   780134,\n",
       "   780195,\n",
       "   780279,\n",
       "   780470,\n",
       "   780648,\n",
       "   781048,\n",
       "   781144]},\n",
       " 'sub-NDARTB219AJV': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [781248, 781347, 781420, 781544, 781782, 781950]},\n",
       " 'sub-NDARTB883GUN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [782052,\n",
       "   782720,\n",
       "   782806,\n",
       "   782866,\n",
       "   782949,\n",
       "   783149,\n",
       "   783278,\n",
       "   783674,\n",
       "   783757]},\n",
       " 'sub-NDARTC710UM1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [783859, 784204, 784290, 784350, 784432, 784604, 784738, 785004]},\n",
       " 'sub-NDARTC802PGL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [785106, 785663, 785749, 785809, 785892, 786065, 786208, 786578]},\n",
       " 'sub-NDARTD468TUH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [786680, 786816, 786918, 787055, 787480]},\n",
       " 'sub-NDARTD624AF9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [787584,\n",
       "   787930,\n",
       "   788016,\n",
       "   788076,\n",
       "   788159,\n",
       "   788334,\n",
       "   788458,\n",
       "   788760,\n",
       "   788841]},\n",
       " 'sub-NDARTE159EC4': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [788943, 789197, 789365, 790760]},\n",
       " 'sub-NDARTE424BUF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [790882,\n",
       "   791559,\n",
       "   791645,\n",
       "   791704,\n",
       "   791787,\n",
       "   791964,\n",
       "   792124,\n",
       "   792504,\n",
       "   792586]},\n",
       " 'sub-NDARTE432ZRH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [792688, 793242, 793328, 793387, 793470, 793644, 793775, 794084]},\n",
       " 'sub-NDARTE785ZMJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [794186, 794912, 794998, 795057, 795140, 795321, 795456, 795859]},\n",
       " 'sub-NDARTF133LLM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [795961,\n",
       "   796532,\n",
       "   796618,\n",
       "   796677,\n",
       "   796760,\n",
       "   796933,\n",
       "   797135,\n",
       "   797548,\n",
       "   797642]},\n",
       " 'sub-NDARTG035JK8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [797744, 798393, 798479, 798573, 798656, 798871, 799007, 799137]},\n",
       " 'sub-NDARTG681ZDV': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [799220, 799308, 799370, 799453, 799652, 799814, 800249, 800333]},\n",
       " 'sub-NDARTH433TE0': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [800437, 800539, 800652, 800771, 800948, 801188, 801353, 801485]},\n",
       " 'sub-NDARTH473LF8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [801587,\n",
       "   802229,\n",
       "   802315,\n",
       "   802374,\n",
       "   802456,\n",
       "   802628,\n",
       "   802764,\n",
       "   803156,\n",
       "   803245]},\n",
       " 'sub-NDARTJ032FJ2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [803347, 804101, 804187, 804246, 804329, 804505, 804628, 805006]},\n",
       " 'sub-NDARTK056HL3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [805108, 805525, 805611, 805670, 805753, 805935]},\n",
       " 'sub-NDARTK507RJW': {'tasks': ['RestingState'], 'starts': [806037]},\n",
       " 'sub-NDARTK691PJJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [806308,\n",
       "   807514,\n",
       "   807601,\n",
       "   807660,\n",
       "   807743,\n",
       "   807985,\n",
       "   808188,\n",
       "   808606,\n",
       "   808709]},\n",
       " 'sub-NDARTK878GZK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [808811, 808911, 808986, 809080, 809255, 809438, 809537]},\n",
       " 'sub-NDARTK950DY5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [809639, 809768, 809869, 809929, 810031, 810204, 810453]},\n",
       " 'sub-NDARTL667CCG': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [810842, 810928, 810988, 811070, 811247, 811413, 811686, 811772]},\n",
       " 'sub-NDARTN158LRF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [811875,\n",
       "   812576,\n",
       "   812662,\n",
       "   812721,\n",
       "   812804,\n",
       "   813027,\n",
       "   813196,\n",
       "   813623,\n",
       "   813712]},\n",
       " 'sub-NDARTN365HEV': {'tasks': ['RestingState'], 'starts': [813814]},\n",
       " 'sub-NDARTN483XE4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [814014,\n",
       "   814288,\n",
       "   814376,\n",
       "   814437,\n",
       "   814522,\n",
       "   814702,\n",
       "   814952,\n",
       "   815408,\n",
       "   815492]},\n",
       " 'sub-NDARTN487KDK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [815596, 816057, 816160, 816220, 816405, 816562, 816793, 816885]},\n",
       " 'sub-NDARTN712UJK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [816987,\n",
       "   817480,\n",
       "   817566,\n",
       "   817626,\n",
       "   817709,\n",
       "   817888,\n",
       "   818065,\n",
       "   818349,\n",
       "   818439]},\n",
       " 'sub-NDARTP313AGH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [818541, 818666, 818747, 818848, 819070, 819218]},\n",
       " 'sub-NDARTP795WU2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [819320, 820022, 820109, 820168, 820251, 820425, 820533, 821001]},\n",
       " 'sub-NDARTR175UDC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [821103,\n",
       "   821486,\n",
       "   821574,\n",
       "   821636,\n",
       "   821719,\n",
       "   821922,\n",
       "   822084,\n",
       "   822638,\n",
       "   822758]},\n",
       " 'sub-NDARTT383DPY': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [822862, 823046, 823224]},\n",
       " 'sub-NDARTT667RKQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [824897,\n",
       "   825282,\n",
       "   825368,\n",
       "   825428,\n",
       "   825510,\n",
       "   825684,\n",
       "   825830,\n",
       "   826097,\n",
       "   826176]},\n",
       " 'sub-NDARTT979FDD': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [826278, 826489, 826728]},\n",
       " 'sub-NDARTU644WHZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [827417, 827769, 827855, 827915, 827997, 828196, 828355, 828637]},\n",
       " 'sub-NDARTU777GVV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [828739, 829403, 829489, 829548, 829631, 829831, 829989, 830371]},\n",
       " 'sub-NDARTV107XD4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [830473,\n",
       "   830900,\n",
       "   830986,\n",
       "   831045,\n",
       "   831128,\n",
       "   831302,\n",
       "   831457,\n",
       "   831734,\n",
       "   831820]},\n",
       " 'sub-NDARTV222FA9': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [831922, 832210, 832451]},\n",
       " 'sub-NDARTV776ZJ8': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [833454, 833738, 833862]},\n",
       " 'sub-NDARTW456RAG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [834090, 834566, 834653, 834712, 834795, 834982, 835134, 835440]},\n",
       " 'sub-NDARTW501ZKN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [835542, 835693, 835753, 835927, 836095, 836325, 836417]},\n",
       " 'sub-NDARTW850GHU': {'tasks': ['DespicableMe', 'RestingState'],\n",
       "  'starts': [836521, 836528]},\n",
       " 'sub-NDARTW855BPJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [836700, 836904, 836998, 837121, 837297, 837454]},\n",
       " 'sub-NDARTX575YBU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [837556,\n",
       "   838024,\n",
       "   838111,\n",
       "   838171,\n",
       "   838253,\n",
       "   838547,\n",
       "   838670,\n",
       "   838957,\n",
       "   839042]},\n",
       " 'sub-NDARTX659HAF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [839144,\n",
       "   839847,\n",
       "   839934,\n",
       "   840025,\n",
       "   840109,\n",
       "   840296,\n",
       "   840428,\n",
       "   840982,\n",
       "   841072]},\n",
       " 'sub-NDARTY010EYP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [841176,\n",
       "   841502,\n",
       "   841590,\n",
       "   841651,\n",
       "   841734,\n",
       "   841934,\n",
       "   842078,\n",
       "   842564,\n",
       "   842666]},\n",
       " 'sub-NDARTY533VXQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [842770,\n",
       "   843140,\n",
       "   843226,\n",
       "   843285,\n",
       "   843368,\n",
       "   843632,\n",
       "   843795,\n",
       "   844066,\n",
       "   844159]},\n",
       " 'sub-NDARTY549YDG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [844261,\n",
       "   844616,\n",
       "   844702,\n",
       "   844762,\n",
       "   844844,\n",
       "   845020,\n",
       "   845217,\n",
       "   845486,\n",
       "   845600]},\n",
       " 'sub-NDARUB495UD1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [845702,\n",
       "   846057,\n",
       "   846144,\n",
       "   846204,\n",
       "   846286,\n",
       "   846460,\n",
       "   846721,\n",
       "   846990,\n",
       "   847158]},\n",
       " 'sub-NDARUC022JWT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [847260,\n",
       "   847611,\n",
       "   847697,\n",
       "   847757,\n",
       "   847839,\n",
       "   848010,\n",
       "   848114,\n",
       "   848379,\n",
       "   848458]},\n",
       " 'sub-NDARUC144FN6': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [848560, 848745, 848877, 850277]},\n",
       " 'sub-NDARUC356FCD': {'tasks': ['contrastChangeDetection',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [850363, 850508, 850569, 850742, 850905, 851383]},\n",
       " 'sub-NDARUC591FWN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [851487,\n",
       "   851861,\n",
       "   851947,\n",
       "   852007,\n",
       "   852089,\n",
       "   852262,\n",
       "   852396,\n",
       "   852664,\n",
       "   852744]},\n",
       " 'sub-NDARUF236HM7': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [852846, 853024, 853155, 854549]},\n",
       " 'sub-NDARUF540ZJ1': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [854643, 854740, 854811, 854904, 855089, 855264]},\n",
       " 'sub-NDARUG323DM3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [855368,\n",
       "   856076,\n",
       "   856163,\n",
       "   856222,\n",
       "   856304,\n",
       "   856477,\n",
       "   856710,\n",
       "   857102,\n",
       "   857214]},\n",
       " 'sub-NDARUJ182TDW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [857316, 857646, 857734, 857795, 858001, 858130, 858997]},\n",
       " 'sub-NDARUJ292JXV': {'tasks': [], 'starts': []},\n",
       " 'sub-NDARUK101YT3': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [859101, 859282, 859426, 859699]},\n",
       " 'sub-NDARUK658FMU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [859782, 860114, 860201, 860262, 860347, 860558, 860714, 861218]},\n",
       " 'sub-NDARUL250RA6': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [861322, 861409, 861469, 861552, 861812]},\n",
       " 'sub-NDARUL454XRE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [861914,\n",
       "   862236,\n",
       "   862324,\n",
       "   862384,\n",
       "   862467,\n",
       "   862644,\n",
       "   862801,\n",
       "   863261,\n",
       "   863348]},\n",
       " 'sub-NDARUL456EER': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [863452,\n",
       "   864123,\n",
       "   864209,\n",
       "   864268,\n",
       "   864351,\n",
       "   864544,\n",
       "   864656,\n",
       "   865032,\n",
       "   865116]},\n",
       " 'sub-NDARUL945XUU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [865218,\n",
       "   865678,\n",
       "   865765,\n",
       "   865826,\n",
       "   865909,\n",
       "   866098,\n",
       "   866284,\n",
       "   866831,\n",
       "   866925]},\n",
       " 'sub-NDARUN221VCJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [867029, 867148, 867249, 867370, 867591, 867812]},\n",
       " 'sub-NDARUN300FG1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [867916,\n",
       "   868372,\n",
       "   868459,\n",
       "   868519,\n",
       "   868603,\n",
       "   868782,\n",
       "   868951,\n",
       "   869459,\n",
       "   869548]},\n",
       " 'sub-NDARUP067TM8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [869651,\n",
       "   869956,\n",
       "   870044,\n",
       "   870180,\n",
       "   870263,\n",
       "   870469,\n",
       "   870655,\n",
       "   871353,\n",
       "   871534]},\n",
       " 'sub-NDARUR126CFH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [871638, 871783, 871945, 872078, 872323, 872542, 872680]},\n",
       " 'sub-NDARUR987CDM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [872784, 873129, 873217, 873278, 873362, 873566, 873712, 874176]},\n",
       " 'sub-NDARUT018UYN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [874280, 874918, 875004, 875064, 875147, 875325, 875445, 875822]},\n",
       " 'sub-NDARUT607MWK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [875924, 876598, 876684, 876743, 876826, 877000, 877140]},\n",
       " 'sub-NDARUU801VFA': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [877429, 877531, 877604, 877702, 877875, 878052, 878148]},\n",
       " 'sub-NDARUU991VRE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [878250, 878349, 878423, 878518, 878690, 878808, 878902]},\n",
       " 'sub-NDARUW904FMQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [879004, 879485, 879571, 879630, 879713, 879888, 880026]},\n",
       " 'sub-NDARUX216VHK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [880301,\n",
       "   880912,\n",
       "   880998,\n",
       "   881057,\n",
       "   881140,\n",
       "   881312,\n",
       "   881459,\n",
       "   881832,\n",
       "   881911]},\n",
       " 'sub-NDARUX284GGB': {'tasks': ['RestingState'], 'starts': [882013]},\n",
       " 'sub-NDARUX886JLM': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [882671, 883319, 883405, 883464, 883547, 883721, 883964, 884354]},\n",
       " 'sub-NDARUY379PT5': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [884456,\n",
       "   884981,\n",
       "   885067,\n",
       "   885127,\n",
       "   885209,\n",
       "   885387,\n",
       "   885655,\n",
       "   886002,\n",
       "   886119]},\n",
       " 'sub-NDARUY818JG8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [886221,\n",
       "   886604,\n",
       "   886690,\n",
       "   886750,\n",
       "   886832,\n",
       "   887006,\n",
       "   887143,\n",
       "   887425,\n",
       "   887506]},\n",
       " 'sub-NDARUZ206DRV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [887608, 888305, 888391, 888450, 888533, 888705, 888877, 889265]},\n",
       " 'sub-NDARUZ345THD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [889367, 889658, 889746, 889808, 889892, 890190, 890350, 890816]},\n",
       " 'sub-NDARVB819ENX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [890920,\n",
       "   891202,\n",
       "   891290,\n",
       "   891352,\n",
       "   891435,\n",
       "   891711,\n",
       "   891875,\n",
       "   892272,\n",
       "   892379]},\n",
       " 'sub-NDARVB897AH1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [892483,\n",
       "   892837,\n",
       "   892923,\n",
       "   892983,\n",
       "   893065,\n",
       "   893237,\n",
       "   893376,\n",
       "   893665,\n",
       "   893746]},\n",
       " 'sub-NDARVC305AC6': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [893849, 894029, 894187, 894674]},\n",
       " 'sub-NDARVC821LP9': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [894772,\n",
       "   895172,\n",
       "   895258,\n",
       "   895318,\n",
       "   895400,\n",
       "   895573,\n",
       "   895741,\n",
       "   896019,\n",
       "   896105]},\n",
       " 'sub-NDARVC956HHP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [896207, 896355, 896443, 896549, 896727, 897136, 897245]},\n",
       " 'sub-NDARVD194JX2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [897349,\n",
       "   897788,\n",
       "   897874,\n",
       "   897933,\n",
       "   898016,\n",
       "   898190,\n",
       "   898343,\n",
       "   898656,\n",
       "   898768]},\n",
       " 'sub-NDARVD635FX8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [898870, 899090, 899213, 899298, 899401, 899775, 899896, 900108]},\n",
       " 'sub-NDARVD685RRJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [900211, 900580, 900666, 900725, 900808, 900982, 901119, 901493]},\n",
       " 'sub-NDARVF039ZLX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [901595, 902023, 902111, 902173, 902257, 902483, 902606, 903147]},\n",
       " 'sub-NDARVG127VW5': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [903250, 903429, 903615, 903780]},\n",
       " 'sub-NDARVG597HNL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [903862,\n",
       "   904661,\n",
       "   904749,\n",
       "   904809,\n",
       "   904894,\n",
       "   905074,\n",
       "   905326,\n",
       "   905980,\n",
       "   906065]},\n",
       " 'sub-NDARVJ072EHU': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [906169, 906360, 906504, 908098]},\n",
       " 'sub-NDARVK847ZRT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [908181,\n",
       "   908590,\n",
       "   908676,\n",
       "   908735,\n",
       "   908818,\n",
       "   909014,\n",
       "   909177,\n",
       "   909454,\n",
       "   909550]},\n",
       " 'sub-NDARVP109DF0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [909652, 909965, 910052, 910112, 910197, 910378, 910539, 910995]},\n",
       " 'sub-NDARVP135ZGE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [911098, 911221, 911281, 911401, 911577, 912576]},\n",
       " 'sub-NDARVT488RXJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [912678, 912784, 912844, 912927, 913101, 913218, 913606, 913686]},\n",
       " 'sub-NDARVU683CTN': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [913788, 914949, 915201, 915403, 915553]},\n",
       " 'sub-NDARVV199XK2': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [915678, 915854, 915963]},\n",
       " 'sub-NDARVX111CNA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [916241,\n",
       "   916459,\n",
       "   916547,\n",
       "   916608,\n",
       "   916691,\n",
       "   916866,\n",
       "   917022,\n",
       "   917508,\n",
       "   917591]},\n",
       " 'sub-NDARVX185WX2': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [917694, 917878, 918090, 920138]},\n",
       " 'sub-NDARVX337AC0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [920222,\n",
       "   920944,\n",
       "   921030,\n",
       "   921089,\n",
       "   921172,\n",
       "   921364,\n",
       "   921506,\n",
       "   921885,\n",
       "   921964]},\n",
       " 'sub-NDARVY054WEA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [922066,\n",
       "   922781,\n",
       "   922868,\n",
       "   922928,\n",
       "   923011,\n",
       "   923184,\n",
       "   923355,\n",
       "   923792,\n",
       "   923879]},\n",
       " 'sub-NDARVY366HWJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [923981,\n",
       "   924823,\n",
       "   924909,\n",
       "   924968,\n",
       "   925051,\n",
       "   925246,\n",
       "   925366,\n",
       "   925795,\n",
       "   925882]},\n",
       " 'sub-NDARVY564LFB': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [925984, 926342, 926429, 926489, 926571, 926713]},\n",
       " 'sub-NDARVZ078KTW': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState'],\n",
       "  'starts': [926815, 926877, 926970]},\n",
       " 'sub-NDARWA102TY7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [927165,\n",
       "   927554,\n",
       "   927640,\n",
       "   927699,\n",
       "   927782,\n",
       "   927961,\n",
       "   928095,\n",
       "   928374,\n",
       "   928483]},\n",
       " 'sub-NDARWA513WM2': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [928585, 929684, 929856, 929995]},\n",
       " 'sub-NDARWA544RDT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [930123,\n",
       "   930797,\n",
       "   930884,\n",
       "   930943,\n",
       "   931026,\n",
       "   931199,\n",
       "   931370,\n",
       "   931844,\n",
       "   931935]},\n",
       " 'sub-NDARWA622HHZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [932037,\n",
       "   933049,\n",
       "   933138,\n",
       "   933199,\n",
       "   933283,\n",
       "   933473,\n",
       "   933613,\n",
       "   934227,\n",
       "   934381]},\n",
       " 'sub-NDARWC426HEA': {'tasks': ['DespicableMe',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [934485, 934526, 934727, 934900, 935026]},\n",
       " 'sub-NDARWC427JB2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [935108,\n",
       "   935458,\n",
       "   935544,\n",
       "   935604,\n",
       "   935686,\n",
       "   935858,\n",
       "   935989,\n",
       "   936270,\n",
       "   936348]},\n",
       " 'sub-NDARWD655AAA': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [936450, 936552, 936624, 936772, 937069]},\n",
       " 'sub-NDARWD911WBU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [937171, 937584, 937670, 937729, 937811, 937983, 938278]},\n",
       " 'sub-NDARWE717DEU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [938380, 938767, 938853, 938913, 938995, 939139]},\n",
       " 'sub-NDARWE908KF8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [939241, 939651, 939737, 939797, 939879, 940053, 940178, 940447]},\n",
       " 'sub-NDARWF086XF8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [940549, 940953, 941041, 941102, 941185, 941370, 941543, 942047]},\n",
       " 'sub-NDARWF259RB2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [942151,\n",
       "   942541,\n",
       "   942627,\n",
       "   942687,\n",
       "   942769,\n",
       "   942948,\n",
       "   943100,\n",
       "   943378,\n",
       "   943461]},\n",
       " 'sub-NDARWG200CUE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [943564, 944204, 944290, 944350, 944432, 944616, 944771, 945098]},\n",
       " 'sub-NDARWH778CPD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [945200,\n",
       "   945667,\n",
       "   945755,\n",
       "   945817,\n",
       "   945901,\n",
       "   946095,\n",
       "   946224,\n",
       "   946776,\n",
       "   946855]},\n",
       " 'sub-NDARWJ321WKG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [946958,\n",
       "   947403,\n",
       "   947489,\n",
       "   947549,\n",
       "   947631,\n",
       "   947867,\n",
       "   948040,\n",
       "   948417,\n",
       "   948498]},\n",
       " 'sub-NDARWK051GG4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [948600, 948969, 949055, 949115, 949197, 949373, 949512, 949800]},\n",
       " 'sub-NDARWM683XYZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [949902, 950311, 950397, 950456, 950538, 950739, 950916, 951187]},\n",
       " 'sub-NDARWN169DHY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [951289, 951620, 951708, 951769, 951981, 952146, 952633]},\n",
       " 'sub-NDARWN424BPK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [952736,\n",
       "   953237,\n",
       "   953323,\n",
       "   953383,\n",
       "   953465,\n",
       "   953642,\n",
       "   953806,\n",
       "   954089,\n",
       "   954169]},\n",
       " 'sub-NDARWN709HA2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [954271,\n",
       "   954688,\n",
       "   954776,\n",
       "   954924,\n",
       "   955007,\n",
       "   955189,\n",
       "   955340,\n",
       "   955828,\n",
       "   955919]},\n",
       " 'sub-NDARWP595TE6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [956023,\n",
       "   956853,\n",
       "   956939,\n",
       "   956999,\n",
       "   957082,\n",
       "   957255,\n",
       "   957369,\n",
       "   957814,\n",
       "   957957]},\n",
       " 'sub-NDARWP634RZL': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [958059, 958159, 958232, 958326, 958573, 958810]},\n",
       " 'sub-NDARWP954GPJ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [958912, 959050, 959123, 959249, 959530, 959904]},\n",
       " 'sub-NDARWR509KGZ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [960006, 960105, 960179, 960275, 960449, 960684, 960824]},\n",
       " 'sub-NDARWR628EFJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [960926, 961292, 961378, 961438, 961521, 961732, 961846, 962135]},\n",
       " 'sub-NDARWR732NZE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [962237, 962998, 963086, 963147, 963230, 963429, 963587, 964206]},\n",
       " 'sub-NDARWT389BYY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [964310, 965117, 965135, 965136, 965310, 965483, 965747]},\n",
       " 'sub-NDARWT449PUN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [965827,\n",
       "   966235,\n",
       "   966321,\n",
       "   966381,\n",
       "   966463,\n",
       "   966639,\n",
       "   966799,\n",
       "   967224,\n",
       "   967310]},\n",
       " 'sub-NDARWT694TXM': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [967412, 967518, 967599, 967700, 967936, 968119, 968263]},\n",
       " 'sub-NDARWU381EF7': {'tasks': ['DespicableMe',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [968367, 968368, 968541, 968707, 968997]},\n",
       " 'sub-NDARWU876CWH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [969080, 969354, 969441, 969502, 969587, 969834, 970019, 970462]},\n",
       " 'sub-NDARWV405ZW0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [970566,\n",
       "   971227,\n",
       "   971313,\n",
       "   971372,\n",
       "   971455,\n",
       "   971631,\n",
       "   971773,\n",
       "   972163,\n",
       "   972243]},\n",
       " 'sub-NDARWV449GEM': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [972345, 972480, 972624, 972751, 973568]},\n",
       " 'sub-NDARWV452YZN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [973672,\n",
       "   973982,\n",
       "   974070,\n",
       "   974132,\n",
       "   974216,\n",
       "   974395,\n",
       "   974533,\n",
       "   974997,\n",
       "   975078]},\n",
       " 'sub-NDARWV625CRP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [975182, 975865, 975951, 976010, 976093, 976279, 976406, 976805]},\n",
       " 'sub-NDARWV938NKD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [976907, 977299, 977386, 977447, 977634, 977888, 979041]},\n",
       " 'sub-NDARWW005GCU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [979145, 979676, 979764, 979825, 979909, 980142, 980322, 981036]},\n",
       " 'sub-NDARWX173EFW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [981153,\n",
       "   981499,\n",
       "   981587,\n",
       "   981648,\n",
       "   981733,\n",
       "   981926,\n",
       "   982078,\n",
       "   982635,\n",
       "   982718]},\n",
       " 'sub-NDARWX605LAD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [982822, 983186, 983274, 983355, 983439, 983616, 984130, 984333]},\n",
       " 'sub-NDARWY235XNE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [984437,\n",
       "   985087,\n",
       "   985173,\n",
       "   985232,\n",
       "   985315,\n",
       "   985488,\n",
       "   985643,\n",
       "   986012,\n",
       "   986091]},\n",
       " 'sub-NDARWZ024UYK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [986193, 986280, 986340, 986423, 986597, 986788, 987107]},\n",
       " 'sub-NDARWZ495PG4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [987209,\n",
       "   987857,\n",
       "   987943,\n",
       "   988003,\n",
       "   988086,\n",
       "   988267,\n",
       "   988441,\n",
       "   988832,\n",
       "   988913]},\n",
       " 'sub-NDARXB889WUB': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'ThePresent'],\n",
       "  'starts': [989015, 989114, 989186, 989282, 989456, 989601]},\n",
       " 'sub-NDARXC828NXQ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [989703, 990630, 990716, 990775, 990857, 991184, 991330]},\n",
       " 'sub-NDARXD388TTE': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [991433, 991523, 991585, 991671, 991889, 992244]},\n",
       " 'sub-NDARXD944ZXV': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [992348,\n",
       "   993015,\n",
       "   993101,\n",
       "   993161,\n",
       "   993244,\n",
       "   993421,\n",
       "   993547,\n",
       "   993927,\n",
       "   994005]},\n",
       " 'sub-NDARXE132BTC': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [994107, 994326, 994532, 994864]},\n",
       " 'sub-NDARXE854EDK': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [994993, 995169, 995303, 996694]},\n",
       " 'sub-NDARXF203DCD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [996774, 997199, 997286, 997347, 997430, 997635, 997767, 998311]},\n",
       " 'sub-NDARXF890FLA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [998414, 998651, 998738, 998799, 999002, 999158, 999581, 999694]},\n",
       " 'sub-NDARXG469XGP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [999798,\n",
       "   1000356,\n",
       "   1000444,\n",
       "   1000505,\n",
       "   1000589,\n",
       "   1000847,\n",
       "   1001048,\n",
       "   1001611,\n",
       "   1001697]},\n",
       " 'sub-NDARXG571WPL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1001801,\n",
       "   1002484,\n",
       "   1002570,\n",
       "   1002630,\n",
       "   1002713,\n",
       "   1002886,\n",
       "   1003046,\n",
       "   1003421,\n",
       "   1003501]},\n",
       " 'sub-NDARXG710BVQ': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1003603, 1003778, 1003928, 1005469]},\n",
       " 'sub-NDARXG736VP9': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [1005563, 1005679, 1005753, 1005892, 1006089]},\n",
       " 'sub-NDARXG995EV6': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1006191, 1006369, 1006507, 1006672]},\n",
       " 'sub-NDARXH529MTX': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1006756, 1006860, 1006921, 1007020, 1007209, 1007337]},\n",
       " 'sub-NDARXJ624ZF2': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1007435,\n",
       "   1007811,\n",
       "   1007897,\n",
       "   1007957,\n",
       "   1008039,\n",
       "   1008220,\n",
       "   1008353,\n",
       "   1008634]},\n",
       " 'sub-NDARXK076XU8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1008736, 1009299, 1009387, 1009493, 1009577, 1009808, 1010422]},\n",
       " 'sub-NDARXM200CFE': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1010525,\n",
       "   1010906,\n",
       "   1010994,\n",
       "   1011055,\n",
       "   1011140,\n",
       "   1011348,\n",
       "   1011498,\n",
       "   1012063,\n",
       "   1012192]},\n",
       " 'sub-NDARXN893UEZ': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1012295, 1012414, 1012495, 1012590, 1012805, 1012929, 1013019]},\n",
       " 'sub-NDARXR250LW3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1013123, 1013550, 1013675, 1013757, 1013929, 1014154, 1014249]},\n",
       " 'sub-NDARXR637JER': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1014351,\n",
       "   1015357,\n",
       "   1015444,\n",
       "   1015504,\n",
       "   1015589,\n",
       "   1015774,\n",
       "   1015943,\n",
       "   1016806]},\n",
       " 'sub-NDARXR965TFK': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1016910,\n",
       "   1017488,\n",
       "   1017576,\n",
       "   1017637,\n",
       "   1017721,\n",
       "   1017971,\n",
       "   1018155,\n",
       "   1018729,\n",
       "   1018814]},\n",
       " 'sub-NDARXT325FV6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1018918,\n",
       "   1019643,\n",
       "   1019729,\n",
       "   1019788,\n",
       "   1019871,\n",
       "   1020044,\n",
       "   1020207,\n",
       "   1020638]},\n",
       " 'sub-NDARXT792GY8': {'tasks': ['contrastChangeDetection',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1020740, 1022114, 1022477, 1022657, 1022801, 1022981]},\n",
       " 'sub-NDARXT822TAA': {'tasks': ['DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1023071, 1023130, 1023212, 1023386, 1023550, 1023643]},\n",
       " 'sub-NDARXU018RGY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1023746,\n",
       "   1024593,\n",
       "   1024679,\n",
       "   1024738,\n",
       "   1024821,\n",
       "   1024998,\n",
       "   1025158,\n",
       "   1025546,\n",
       "   1025626]},\n",
       " 'sub-NDARXU253JHC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1025728,\n",
       "   1026365,\n",
       "   1026452,\n",
       "   1026511,\n",
       "   1026594,\n",
       "   1026767,\n",
       "   1026901,\n",
       "   1027272,\n",
       "   1027354]},\n",
       " 'sub-NDARXU376FDN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1027456, 1027563, 1027637, 1027750, 1027927, 1028092, 1028255]},\n",
       " 'sub-NDARXU883NMY': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1028359,\n",
       "   1028934,\n",
       "   1029020,\n",
       "   1029079,\n",
       "   1029162,\n",
       "   1029336,\n",
       "   1029518,\n",
       "   1029856]},\n",
       " 'sub-NDARXU902BRT': {'tasks': ['contrastChangeDetection',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1029958, 1030160, 1030243, 1030416, 1030550, 1030677]},\n",
       " 'sub-NDARXU995GAJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1030755,\n",
       "   1031033,\n",
       "   1031121,\n",
       "   1031182,\n",
       "   1031265,\n",
       "   1031458,\n",
       "   1031582,\n",
       "   1032073]},\n",
       " 'sub-NDARXV445NYZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1032177,\n",
       "   1032841,\n",
       "   1032927,\n",
       "   1032986,\n",
       "   1033069,\n",
       "   1033249,\n",
       "   1033500,\n",
       "   1033877]},\n",
       " 'sub-NDARXW330ZJT': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1033979, 1034348, 1034503, 1035497]},\n",
       " 'sub-NDARXY162ERA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1035687,\n",
       "   1036297,\n",
       "   1036383,\n",
       "   1036442,\n",
       "   1036525,\n",
       "   1036700,\n",
       "   1036814,\n",
       "   1037235,\n",
       "   1037321]},\n",
       " 'sub-NDARXY240WJC': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1037423,\n",
       "   1038111,\n",
       "   1038197,\n",
       "   1038257,\n",
       "   1038340,\n",
       "   1038601,\n",
       "   1038724,\n",
       "   1039120,\n",
       "   1039203]},\n",
       " 'sub-NDARXY745NXJ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1039305,\n",
       "   1040069,\n",
       "   1040155,\n",
       "   1040214,\n",
       "   1040297,\n",
       "   1040473,\n",
       "   1040595,\n",
       "   1040991]},\n",
       " 'sub-NDARXY938XY7': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1041093, 1041193, 1041265, 1041369, 1041541, 1041725, 1041816]},\n",
       " 'sub-NDARXZ692ULW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1041918,\n",
       "   1042273,\n",
       "   1042359,\n",
       "   1042419,\n",
       "   1042501,\n",
       "   1042674,\n",
       "   1042843,\n",
       "   1043115,\n",
       "   1043198]},\n",
       " 'sub-NDARYA776KBK': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [1043300, 1043386, 1043463, 1043562, 1043735, 1044272]},\n",
       " 'sub-NDARYA807PVN': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [1044374, 1044461, 1044523, 1044606, 1044827]},\n",
       " 'sub-NDARYA851GL6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1044930, 1045994, 1046082, 1046143, 1046329, 1046465, 1046925]},\n",
       " 'sub-NDARYA952GA1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1047029,\n",
       "   1047535,\n",
       "   1047621,\n",
       "   1047681,\n",
       "   1047764,\n",
       "   1047962,\n",
       "   1048084,\n",
       "   1048406,\n",
       "   1048491]},\n",
       " 'sub-NDARYA955CY1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1048593,\n",
       "   1048855,\n",
       "   1048942,\n",
       "   1049003,\n",
       "   1049088,\n",
       "   1049267,\n",
       "   1049398,\n",
       "   1049865,\n",
       "   1049950]},\n",
       " 'sub-NDARYB432MB5': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1050054, 1050153, 1050227, 1050322, 1050497, 1050695, 1050903]},\n",
       " 'sub-NDARYD071TYL': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1051005,\n",
       "   1051472,\n",
       "   1051560,\n",
       "   1051622,\n",
       "   1051705,\n",
       "   1051913,\n",
       "   1052040,\n",
       "   1052619,\n",
       "   1052733]},\n",
       " 'sub-NDARYD195BDH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1052837,\n",
       "   1053638,\n",
       "   1053726,\n",
       "   1053787,\n",
       "   1053870,\n",
       "   1053993,\n",
       "   1054144,\n",
       "   1054738,\n",
       "   1054860]},\n",
       " 'sub-NDARYD546HCB': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'ThePresent'],\n",
       "  'starts': [1054964, 1055067, 1055162, 1055261, 1055472, 1055813]},\n",
       " 'sub-NDARYD666FL0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1055917,\n",
       "   1056302,\n",
       "   1056388,\n",
       "   1056448,\n",
       "   1056530,\n",
       "   1056704,\n",
       "   1056847,\n",
       "   1057150,\n",
       "   1057227]},\n",
       " 'sub-NDARYF135ZHU': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1057329,\n",
       "   1057769,\n",
       "   1057856,\n",
       "   1057915,\n",
       "   1057998,\n",
       "   1058195,\n",
       "   1058324,\n",
       "   1058766,\n",
       "   1058860]},\n",
       " 'sub-NDARYF368WEA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1058962,\n",
       "   1059570,\n",
       "   1059656,\n",
       "   1059716,\n",
       "   1059799,\n",
       "   1059973,\n",
       "   1060106,\n",
       "   1060493,\n",
       "   1060574]},\n",
       " 'sub-NDARYF728BEP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1060677,\n",
       "   1061059,\n",
       "   1061147,\n",
       "   1061208,\n",
       "   1061292,\n",
       "   1061483,\n",
       "   1061604,\n",
       "   1062261,\n",
       "   1062376]},\n",
       " 'sub-NDARYG546ZFF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1062480,\n",
       "   1062891,\n",
       "   1062977,\n",
       "   1063037,\n",
       "   1063119,\n",
       "   1063347,\n",
       "   1063491,\n",
       "   1063883]},\n",
       " 'sub-NDARYJ301DYN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1063985,\n",
       "   1064363,\n",
       "   1064449,\n",
       "   1064509,\n",
       "   1064591,\n",
       "   1064763,\n",
       "   1064911,\n",
       "   1065191,\n",
       "   1065271]},\n",
       " 'sub-NDARYL090TGH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1065373,\n",
       "   1065682,\n",
       "   1065770,\n",
       "   1065832,\n",
       "   1065916,\n",
       "   1066097,\n",
       "   1066285,\n",
       "   1066889,\n",
       "   1066991]},\n",
       " 'sub-NDARYL758JGG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1067095,\n",
       "   1067841,\n",
       "   1067929,\n",
       "   1067991,\n",
       "   1068075,\n",
       "   1068353,\n",
       "   1068544,\n",
       "   1069133]},\n",
       " 'sub-NDARYN474PEK': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1069237, 1069524, 1069642, 1071215]},\n",
       " 'sub-NDARYN857XMX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1071297,\n",
       "   1071667,\n",
       "   1071753,\n",
       "   1071813,\n",
       "   1071895,\n",
       "   1072067,\n",
       "   1072269,\n",
       "   1072563,\n",
       "   1072649]},\n",
       " 'sub-NDARYR150UDP': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1072751,\n",
       "   1072837,\n",
       "   1072922,\n",
       "   1073004,\n",
       "   1073196,\n",
       "   1073359,\n",
       "   1073777,\n",
       "   1073862]},\n",
       " 'sub-NDARYT155NHX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1073965,\n",
       "   1074509,\n",
       "   1074595,\n",
       "   1074654,\n",
       "   1074737,\n",
       "   1074908,\n",
       "   1075062,\n",
       "   1075433]},\n",
       " 'sub-NDARYT405NET': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1075535,\n",
       "   1076020,\n",
       "   1076108,\n",
       "   1076169,\n",
       "   1076253,\n",
       "   1076501,\n",
       "   1076657,\n",
       "   1077237,\n",
       "   1077335]},\n",
       " 'sub-NDARYT885XH3': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1077439,\n",
       "   1077719,\n",
       "   1077805,\n",
       "   1077865,\n",
       "   1077948,\n",
       "   1078144,\n",
       "   1078336,\n",
       "   1078669]},\n",
       " 'sub-NDARYV120ZLG': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [1078771, 1078857, 1078916, 1078999, 1079179]},\n",
       " 'sub-NDARYW984FLT': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1079281,\n",
       "   1079569,\n",
       "   1079657,\n",
       "   1079719,\n",
       "   1079803,\n",
       "   1079980,\n",
       "   1080112,\n",
       "   1080611,\n",
       "   1080694]},\n",
       " 'sub-NDARYY218LU2': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [1080798, 1080903, 1080980, 1081114, 1081617]},\n",
       " 'sub-NDARYZ363HP4': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'ThePresent'],\n",
       "  'starts': [1081721, 1081810, 1081873, 1081959, 1082444]},\n",
       " 'sub-NDARYZ489EHH': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1082548, 1082653, 1082712, 1082811, 1083029, 1083156, 1083496]},\n",
       " 'sub-NDARYZ637LK4': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1083598,\n",
       "   1084001,\n",
       "   1084087,\n",
       "   1084146,\n",
       "   1084229,\n",
       "   1084410,\n",
       "   1084608,\n",
       "   1084932,\n",
       "   1085022]},\n",
       " 'sub-NDARYZ770NA1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1085124,\n",
       "   1085823,\n",
       "   1085910,\n",
       "   1085969,\n",
       "   1086052,\n",
       "   1086228,\n",
       "   1086388,\n",
       "   1086777,\n",
       "   1086860]},\n",
       " 'sub-NDARYZ909VND': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1086962,\n",
       "   1087633,\n",
       "   1087719,\n",
       "   1087778,\n",
       "   1087861,\n",
       "   1088035,\n",
       "   1088195,\n",
       "   1088585,\n",
       "   1088673]},\n",
       " 'sub-NDARYZ986HEW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1088775,\n",
       "   1089039,\n",
       "   1089127,\n",
       "   1089189,\n",
       "   1089272,\n",
       "   1089474,\n",
       "   1089673,\n",
       "   1090057,\n",
       "   1090157]},\n",
       " 'sub-NDARZB128KCF': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1090261, 1090549, 1090637, 1090748, 1090832, 1091514, 1091977]},\n",
       " 'sub-NDARZB323LHZ': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1092080,\n",
       "   1092740,\n",
       "   1092827,\n",
       "   1092886,\n",
       "   1092969,\n",
       "   1093155,\n",
       "   1093272,\n",
       "   1093643]},\n",
       " 'sub-NDARZB517KTU': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [1093745, 1093923, 1094087]},\n",
       " 'sub-NDARZD099KWW': {'tasks': ['contrastChangeDetection',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp'],\n",
       "  'starts': [1095396, 1096208, 1096307, 1096389, 1096593, 1096723]},\n",
       " 'sub-NDARZD415ZZ1': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1096855,\n",
       "   1097414,\n",
       "   1097501,\n",
       "   1097562,\n",
       "   1097646,\n",
       "   1097935,\n",
       "   1098068,\n",
       "   1098719,\n",
       "   1098832]},\n",
       " 'sub-NDARZD985LU8': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1098936, 1099113, 1099273, 1100737]},\n",
       " 'sub-NDARZE685UJ5': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1100820,\n",
       "   1100908,\n",
       "   1100969,\n",
       "   1101053,\n",
       "   1101229,\n",
       "   1101357,\n",
       "   1101882,\n",
       "   1102045]},\n",
       " 'sub-NDARZE831DKC': {'tasks': ['contrastChangeDetection',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1102149, 1102521, 1102729, 1102886, 1103123]},\n",
       " 'sub-NDARZE850WXD': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1103208,\n",
       "   1103908,\n",
       "   1103994,\n",
       "   1104053,\n",
       "   1104136,\n",
       "   1104393,\n",
       "   1104523,\n",
       "   1104923]},\n",
       " 'sub-NDARZF615DC0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1105025,\n",
       "   1105383,\n",
       "   1105471,\n",
       "   1105532,\n",
       "   1105643,\n",
       "   1105826,\n",
       "   1105941,\n",
       "   1106387]},\n",
       " 'sub-NDARZG081LYG': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1106491,\n",
       "   1106753,\n",
       "   1106841,\n",
       "   1106902,\n",
       "   1106986,\n",
       "   1107168,\n",
       "   1107282,\n",
       "   1107746,\n",
       "   1107833]},\n",
       " 'sub-NDARZH761YA7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1107937,\n",
       "   1108638,\n",
       "   1108726,\n",
       "   1108787,\n",
       "   1108871,\n",
       "   1109117,\n",
       "   1109311,\n",
       "   1109891,\n",
       "   1110013]},\n",
       " 'sub-NDARZJ016GL0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1110116,\n",
       "   1110749,\n",
       "   1110835,\n",
       "   1110894,\n",
       "   1110977,\n",
       "   1111149,\n",
       "   1111302,\n",
       "   1111677,\n",
       "   1111757]},\n",
       " 'sub-NDARZJ603JM0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1111859,\n",
       "   1112590,\n",
       "   1112676,\n",
       "   1112736,\n",
       "   1112819,\n",
       "   1112992,\n",
       "   1113131,\n",
       "   1113524,\n",
       "   1113610]},\n",
       " 'sub-NDARZK732FZ0': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1113712,\n",
       "   1114382,\n",
       "   1114468,\n",
       "   1114527,\n",
       "   1114610,\n",
       "   1114785,\n",
       "   1114916,\n",
       "   1115309]},\n",
       " 'sub-NDARZK745JGG': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1115411, 1115510, 1115571, 1115782, 1115971, 1116410, 1116499]},\n",
       " 'sub-NDARZL113CU0': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1116603, 1116803, 1116935, 1117147]},\n",
       " 'sub-NDARZL683VNX': {'tasks': ['RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1117233, 1117503, 1117640, 1118562]},\n",
       " 'sub-NDARZL799MFW': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1118642,\n",
       "   1118914,\n",
       "   1119002,\n",
       "   1119064,\n",
       "   1119148,\n",
       "   1119355,\n",
       "   1119512,\n",
       "   1120024,\n",
       "   1120110]},\n",
       " 'sub-NDARZL855WVA': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1120214,\n",
       "   1120906,\n",
       "   1120992,\n",
       "   1121052,\n",
       "   1121135,\n",
       "   1121309,\n",
       "   1121501,\n",
       "   1121862]},\n",
       " 'sub-NDARZN148PMN': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1121965,\n",
       "   1122256,\n",
       "   1122344,\n",
       "   1122466,\n",
       "   1122550,\n",
       "   1122756,\n",
       "   1122906,\n",
       "   1123453,\n",
       "   1123572]},\n",
       " 'sub-NDARZN277NR6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1123676,\n",
       "   1124264,\n",
       "   1124350,\n",
       "   1124409,\n",
       "   1124492,\n",
       "   1124687,\n",
       "   1124838,\n",
       "   1125210]},\n",
       " 'sub-NDARZN578YDP': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1125313,\n",
       "   1125646,\n",
       "   1125734,\n",
       "   1125797,\n",
       "   1125881,\n",
       "   1126168,\n",
       "   1126378,\n",
       "   1126841,\n",
       "   1126931]},\n",
       " 'sub-NDARZN914GPB': {'tasks': ['RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch'],\n",
       "  'starts': [1127034, 1127212, 1127320, 1127647]},\n",
       " 'sub-NDARZR538EZH': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1127734,\n",
       "   1128019,\n",
       "   1128106,\n",
       "   1128168,\n",
       "   1128251,\n",
       "   1128448,\n",
       "   1128602,\n",
       "   1129029]},\n",
       " 'sub-NDARZV608BL8': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1129133,\n",
       "   1129770,\n",
       "   1129858,\n",
       "   1129918,\n",
       "   1130001,\n",
       "   1130185,\n",
       "   1130316,\n",
       "   1130873]},\n",
       " 'sub-NDARZW363UGM': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1130977, 1131080, 1131170, 1131303, 1131486, 1131687, 1131789]},\n",
       " 'sub-NDARZW662UK6': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1131893,\n",
       "   1132293,\n",
       "   1132379,\n",
       "   1132439,\n",
       "   1132521,\n",
       "   1132715,\n",
       "   1132857,\n",
       "   1133165,\n",
       "   1133257]},\n",
       " 'sub-NDARZW930MF2': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1133359,\n",
       "   1133447,\n",
       "   1133564,\n",
       "   1133695,\n",
       "   1133880,\n",
       "   1134063,\n",
       "   1134635,\n",
       "   1134717]},\n",
       " 'sub-NDARZX561DR9': {'tasks': ['DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'ThePresent'],\n",
       "  'starts': [1134820, 1134920, 1134980, 1135079, 1135253, 1135391, 1136006]},\n",
       " 'sub-NDARZY224TFX': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning6target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1136108,\n",
       "   1136354,\n",
       "   1136441,\n",
       "   1136502,\n",
       "   1136586,\n",
       "   1136802,\n",
       "   1136999,\n",
       "   1137445,\n",
       "   1137528]},\n",
       " 'sub-NDARZZ830JM7': {'tasks': ['contrastChangeDetection',\n",
       "   'DespicableMe',\n",
       "   'DiaryOfAWimpyKid',\n",
       "   'FunwithFractals',\n",
       "   'RestingState',\n",
       "   'seqLearning8target',\n",
       "   'surroundSupp',\n",
       "   'symbolSearch',\n",
       "   'ThePresent'],\n",
       "  'starts': [1137632,\n",
       "   1137931,\n",
       "   1138019,\n",
       "   1138080,\n",
       "   1138164,\n",
       "   1138341,\n",
       "   1138479,\n",
       "   1138959,\n",
       "   1139043]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Encoder_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, window_size=200, stride=20,\n",
    "                 split=\"train\", shard_size=1000):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.window_size = int(window_size)\n",
    "        self.stride = int(stride)\n",
    "        self.split = split\n",
    "        self.shard_size = int(shard_size)\n",
    "\n",
    "        split_dir = self.data_dir / split\n",
    "        if not split_dir.exists():\n",
    "            raise FileNotFoundError(split_dir)\n",
    "\n",
    "        self.shard_paths = sorted(\n",
    "            (p for p in split_dir.glob(\"window_shard_*.npy\")),\n",
    "            key=lambda p: int(p.stem.split(\"_\")[-1])\n",
    "        )\n",
    "        if not self.shard_paths:\n",
    "            raise RuntimeError(\"No shard files matching window_shard_*.npy\")\n",
    "\n",
    "        self.shard_meta = []\n",
    "        self.prefix_virtual = [0]\n",
    "        total_virtual = 0\n",
    "        for path in self.shard_paths:\n",
    "            arr = np.load(path, mmap_mode=\"r\")\n",
    "            n_base, C, W = arr.shape\n",
    "            if W != self.window_size:\n",
    "                raise ValueError(f\"{path.name}: W={W} != window_size={self.window_size}\")\n",
    "            total_samples = n_base * self.window_size\n",
    "            n_virtual = ((total_samples - self.window_size) // self.stride) + 1 if total_samples >= self.window_size else 0\n",
    "            self.shard_meta.append({\n",
    "                \"path\": path,\n",
    "                \"n_base\": int(n_base),\n",
    "                \"C\": int(C),\n",
    "                \"W\": int(W),\n",
    "                \"n_virtual\": int(n_virtual),\n",
    "            })\n",
    "            total_virtual += n_virtual\n",
    "            self.prefix_virtual.append(total_virtual)\n",
    "        self.total_virtual = total_virtual\n",
    "        if self.total_virtual == 0:\n",
    "            raise RuntimeError(\"No virtual windows possible.\")\n",
    "\n",
    "        # Load meta\n",
    "        self.full_dict = torch.load(os.path.join(split_dir, \"full_dict.pt\"))\n",
    "        self.ordered_subjects = []\n",
    "        self.ordered_starts = []\n",
    "\n",
    "        valid_subjects = [\n",
    "            (subj, self.full_dict[subj][\"limits\"][\"Start\"])\n",
    "            for subj in self.full_dict\n",
    "            if self.full_dict[subj][\"limits\"][\"Start\"] is not None\n",
    "            and self.full_dict[subj][\"limits\"][\"End\"] is not None\n",
    "            and self.full_dict[subj][\"limits\"][\"Start\"] != self.full_dict[subj][\"limits\"][\"End\"]\n",
    "        ]\n",
    "        valid_subjects.sort(key=lambda x: x[1])\n",
    "\n",
    "        for subj, start in valid_subjects:\n",
    "            self.ordered_subjects.append(subj)\n",
    "            self.ordered_starts.append(start)\n",
    "\n",
    "        self.subject_task = {}\n",
    "        for subj, subj_data in self.full_dict.items():\n",
    "            tasks = subj_data.get(\"tasks\", {})\n",
    "            if not tasks:\n",
    "                continue\n",
    "            task_items = [\n",
    "                (t_name, t_info[\"Start\"])\n",
    "                for t_name, t_info in tasks.items()\n",
    "                if t_info[\"End\"] is not None and t_info[\"Start\"] != t_info[\"End\"]\n",
    "            ]\n",
    "            task_items.sort(key=lambda x: x[1])\n",
    "            self.subject_task[subj] = {\n",
    "                \"tasks\": [t for t, _ in task_items],\n",
    "                \"starts\": [s for _, s in task_items]\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_virtual\n",
    "\n",
    "    def _load_shard(self, shard_idx: int):\n",
    "        path = self.shard_meta[shard_idx][\"path\"]\n",
    "        return np.load(path, mmap_mode=\"r\")\n",
    "\n",
    "    def _locate(self, global_index: int):\n",
    "        s = bisect.bisect_right(self.prefix_virtual, global_index) - 1\n",
    "        local_idx = global_index - self.prefix_virtual[s]\n",
    "        return s, local_idx\n",
    "\n",
    "    def _get_metadata(self, window_index: int):\n",
    "        subj_idx = bisect.bisect_right(self.ordered_starts, window_index) - 1\n",
    "        subject = self.ordered_subjects[subj_idx]\n",
    "        task_idx = bisect.bisect_right(self.subject_task[subject][\"starts\"], window_index) - 1\n",
    "        task = self.subject_task[subject][\"tasks\"][task_idx]\n",
    "        meta_data = self.full_dict[subject]\n",
    "        return subject, task, meta_data\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index >= self.total_virtual:\n",
    "            raise IndexError\n",
    "\n",
    "        shard_idx, local_idx = self._locate(index)\n",
    "        meta = self.shard_meta[shard_idx]\n",
    "        shard = self._load_shard(shard_idx)\n",
    "\n",
    "        W = self.window_size\n",
    "        start = local_idx * self.stride\n",
    "        base_idx = start // W\n",
    "        offset = start % W\n",
    "\n",
    "        if offset == 0:\n",
    "            x = shard[base_idx]\n",
    "        else:\n",
    "            first = shard[base_idx, :, offset:]\n",
    "            need = W - first.shape[1]\n",
    "            second = shard[base_idx + 1, :, :need]\n",
    "            x = np.concatenate([first, second], axis=1)\n",
    "\n",
    "        global_window_index = self.prefix_virtual[shard_idx] + local_idx\n",
    "        subject_id, task, meta_data = self._get_metadata(global_window_index)\n",
    "        return torch.from_numpy(np.asarray(x, dtype=np.float32)), subject_id, task, meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"D:\\projects\\pytorch_training\\full_eeg_data_shards\"\n",
    "train_dataset = Encoder_Dataset(output_dir ,stride=10, split=\"train\")\n",
    "test_dataset = Encoder_Dataset(output_dir ,stride=10, split=\"test\")\n",
    "val_dataset = Encoder_Dataset(output_dir ,stride=10, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21798931"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 129, 200])\n",
      "('sub-NDARZZ830JM7',)\n",
      "('ThePresent',)\n",
      "{'meta': {'participant_id': ['sub-NDARZZ830JM7'], 'release_number': ['R7'], 'sex': ['M'], 'age': tensor([13.1215], dtype=torch.float64), 'ehq_total': tensor([-13.3400], dtype=torch.float64), 'commercial_use': ['Yes'], 'full_pheno': ['Yes'], 'p_factor': tensor([1.3690], dtype=torch.float64), 'attention': tensor([-1.1980], dtype=torch.float64), 'internalizing': tensor([-0.1100], dtype=torch.float64), 'externalizing': tensor([0.6360], dtype=torch.float64), 'RestingState': ['available'], 'DespicableMe': ['available'], 'FunwithFractals': ['available'], 'ThePresent': ['available'], 'DiaryOfAWimpyKid': ['available'], 'contrastChangeDetection_1': ['unavailable'], 'contrastChangeDetection_2': ['available'], 'contrastChangeDetection_3': ['available'], 'surroundSupp_1': ['available'], 'surroundSupp_2': ['available'], 'seqLearning6target': ['unavailable'], 'seqLearning8target': ['available'], 'symbolSearch': ['available'], 'release_dir': ['D:\\\\projects\\\\pytorch_training\\\\data_1\\\\R7_L100_bdf']}, 'limits': {'Start': tensor([1137632]), 'End': tensor([1139147])}, 'tasks': {'contrastChangeDetection': {'Start': tensor([1137632]), 'End': tensor([1137931])}, 'DespicableMe': {'Start': tensor([1137931]), 'End': tensor([1138019])}, 'DiaryOfAWimpyKid': {'Start': tensor([1138019]), 'End': tensor([1138080])}, 'FunwithFractals': {'Start': tensor([1138080]), 'End': tensor([1138164])}, 'RestingState': {'Start': tensor([1138164]), 'End': tensor([1138341])}, 'seqLearning8target': {'Start': tensor([1138341]), 'End': tensor([1138479])}, 'surroundSupp': {'Start': tensor([1138479]), 'End': tensor([1138959])}, 'symbolSearch': {'Start': tensor([1138959]), 'End': tensor([1139043])}, 'ThePresent': {'Start': tensor([1139043]), 'End': tensor([1139147])}}}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x , subject_id, task, meta_data = batch\n",
    "    print(x.shape)\n",
    "    print(subject_id)\n",
    "    print(task)\n",
    "    print(meta_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self,nb_tokens , c_dim = 129   ,  t_dim = 200 , slice_size = 10 ,target_c_dim = 64 , target_t_dim = 6  , emb_dim = 512     ):\n",
    "        super().__init__()\n",
    "        self.time_projection = nn.Linear(slice_size  ,target_t_dim)\n",
    "        self.channel_projection = nn.Linear(c_dim , target_c_dim)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1 , 1 , 1 , target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1 , 1 , target_c_dim , 1))\n",
    "        self.token_projection = nn.Linear(target_c_dim*target_t_dim , emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(  nb_tokens , emb_dim))\n",
    "        self.mask_paramater = nn.Parameter(torch.zeros( c_dim , slice_size))\n",
    "        nn.init.xavier_uniform_(self.time_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.channel_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mask_paramater)\n",
    "\n",
    "        self.slice_size = slice_size\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        # x (B , N , C , slice)  N = T // slice\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_paramater\n",
    "\n",
    "\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size , target_c_dim = self.c_dim , target_t_dim = self.slice_size , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        masked_index = torch.randint(low=0 , high=self.nb_tokens , size=(x.shape[0],int(self.nb_tokens * mask_ratio),))\n",
    "        return   masked_index\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/306584 [00:00<?, ?it/s]C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_16572\\2161087430.py:97: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  return torch.from_numpy(np.asarray(x, dtype=np.float32))\n",
      "  0%|          | 1001/306584 [03:40<13:11:24,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , loss 1.005246112704277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2000/306584 [06:31<17:14:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , loss 1.0073635778427124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2304/306584 [07:23<16:15:26,  5.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m curr_loss = \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m curr_nb_batches = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_indexes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mEncoder_Dataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     93\u001b[39m     need = W - first.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     94\u001b[39m     second = shard[base_idx + \u001b[32m1\u001b[39m, :, :need]\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(np.asarray(x, dtype=np.float32))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "# optimizer\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "# epochs\n",
    "epochs = 20\n",
    "print_after = 1000\n",
    "save_after = 10000\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "\n",
    "# scheduler (epoch-based warmup + cosine)\n",
    "warmup_epochs = 1\n",
    "cosine_epochs = epochs - warmup_epochs\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=1e-8, end_factor=1.0, total_iters=warmup_epochs\n",
    ")\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=cosine_epochs\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs]\n",
    ")\n",
    "\n",
    "# loss tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# resume\n",
    "start_epoch = 0\n",
    "start_it = 0\n",
    "global_steps = 0\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "    print(f\"Resumed: epoch={start_epoch}, it_in_epoch={start_it}, steps={global_steps}\")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # ===================== train =====================\n",
    "    model.train()\n",
    "    curr_loss = 0\n",
    "    curr_nb_batches = 0\n",
    "\n",
    "    for it, x in enumerate(tqdm(train_loader)):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask_indexes = model._make_mask(x).to(device)\n",
    "        batch_index = torch.arange(mask_indexes.shape[0], device=device).unsqueeze(1).expand(mask_indexes.shape[0], mask_indexes.shape[1])\n",
    "\n",
    "        out, target = model(x, mask_indexes)\n",
    "        out = out.reshape(out.shape[0], out.shape[1], model.c_dim, model.slice_size)\n",
    "        out = out[batch_index, mask_indexes]\n",
    "        target = target[batch_index, mask_indexes]\n",
    "\n",
    "        loss = loss_f(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val = loss.item()\n",
    "        curr_loss += val\n",
    "        curr_nb_batches += 1\n",
    "        global_steps += 1\n",
    "\n",
    "        if curr_nb_batches >= print_after:\n",
    "            avg_chunk = curr_loss / curr_nb_batches\n",
    "            print(f\"Epoch {epoch} , loss {avg_chunk}\")\n",
    "            train_losses.append(avg_chunk)\n",
    "            curr_loss = 0\n",
    "            curr_nb_batches = 0\n",
    "\n",
    "        if global_steps % save_after == 0:\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch,\n",
    "                 \"it_in_epoch\": it + 1,\n",
    "                 \"steps\": global_steps,\n",
    "                 \"model\": model.state_dict(),\n",
    "                 \"train_losses\": train_losses,\n",
    "                 \"val_losses\": val_losses},\n",
    "                save_path\n",
    "            )\n",
    "            print(f\"Saved checkpoint at step {global_steps} -> {save_path}\")\n",
    "\n",
    "    # ===================== test =====================\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    val_curr_loss = 0\n",
    "    val_curr_nb_batches = 0\n",
    "    with torch.inference_mode():\n",
    "        for x in tqdm(val_loader):\n",
    "            x = x.to(device)\n",
    "            mask_indexes = model._make_mask(x).to(device)\n",
    "            batch_index = torch.arange(mask_indexes.shape[0], device=device).unsqueeze(1).expand(mask_indexes.shape[0], mask_indexes.shape[1])\n",
    "\n",
    "            out, target = model(x, mask_indexes)\n",
    "            out = out.reshape(out.shape[0], out.shape[1], model.c_dim, model.slice_size)\n",
    "            out = out[batch_index, mask_indexes]\n",
    "            target = target[batch_index, mask_indexes]\n",
    "\n",
    "            loss = loss_f(out, target)\n",
    "            v = loss.item()\n",
    "            val_loss += v\n",
    "            val_batches += 1\n",
    "\n",
    "            val_curr_loss += v\n",
    "            val_curr_nb_batches += 1\n",
    "            if val_curr_nb_batches >= print_after:\n",
    "                print(f\"Epoch {epoch} , val_loss {val_curr_loss / val_curr_nb_batches}\")\n",
    "                val_curr_loss = 0\n",
    "                val_curr_nb_batches = 0\n",
    "\n",
    "    if val_batches > 0:\n",
    "        avg_val = val_loss / val_batches\n",
    "        print(f\"Epoch {epoch} , val_loss {avg_val}\")\n",
    "        val_losses.append(avg_val)\n",
    "\n",
    "    # step scheduler ONCE per epoch (not per batch)\n",
    "    scheduler.step()\n",
    "\n",
    "    # save checkpoint at end of epoch\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch + 1,\n",
    "         \"it_in_epoch\": 0,\n",
    "         \"steps\": global_steps,\n",
    "         \"model\": model.state_dict(),\n",
    "         \"train_losses\": train_losses,\n",
    "         \"val_losses\": val_losses},\n",
    "        save_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x211ba716fc0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(train_losses[:] , label=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x211baace450>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_window_size = 10\n",
    "smoother_train_losses = [np.mean(train_losses[max(0, i - loss_window_size):i]) for i in range(len(train_losses)-2500 , len(train_losses ) , loss_window_size )]\n",
    "plt.plot(smoother_train_losses , label=\"train smoothed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x211baba0f50>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_losses[-5000:] , label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3605722486972809\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "\n",
    "    x = next(iter(train_loader)).to(device)\n",
    "    mask_indexes = model._make_mask(x)\n",
    "    out , target = model(x , mask_indexes)\n",
    "\n",
    "    out = out.reshape(out.shape[0] , out.shape[1] , model.c_dim , model.slice_size)\n",
    "    batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , mask_indexes.shape[1])\n",
    "    out = out[batch_index , mask_indexes]\n",
    "    target = target[batch_index , mask_indexes]\n",
    "    loss =  loss_f(out , target).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# -------------------- EMBEDDING --------------------\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                 target_c_dim=129, target_t_dim=10, emb_dim=512):\n",
    "        super().__init__()\n",
    "        self.slice_size = slice_size\n",
    "        self.c_dim = c_dim\n",
    "        self.nb_tokens = nb_tokens\n",
    "\n",
    "        # axis-wise projections\n",
    "        self.time_projection    = nn.Linear(slice_size,  target_t_dim)  # (..., slice) -> (..., Dt)\n",
    "        self.channel_projection = nn.Linear(c_dim,       target_c_dim)  # (..., C)     -> (..., Dc)\n",
    "\n",
    "        # axis embeddings (added before flatten)\n",
    "        self.time_positional_emb    = nn.Parameter(torch.zeros(1, 1, 1, target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1, 1, target_c_dim, 1))\n",
    "\n",
    "        # token projection (Dc*Dt -> D)\n",
    "        self.token_projection = nn.Linear(target_c_dim * target_t_dim, emb_dim)\n",
    "\n",
    "        # token positional embedding (added in D space)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(1, nb_tokens, emb_dim))\n",
    "\n",
    "    def _targets_from_raw(self, x: Tensor) -> Tensor:\n",
    "        # x: (B, C, T) -> targets per token: (B, N, C*slice)\n",
    "        B, C, T = x.shape\n",
    "        N = T // self.slice_size\n",
    "        return x.view(B, C, N, self.slice_size).permute(0, 2, 1, 3).reshape(B, N, C * self.slice_size)\n",
    "\n",
    "    def forward(self, x: Tensor, add_token_pos: bool = True, return_targets: bool = False):\n",
    "        \"\"\"\n",
    "        Returns token features before/after adding token positional embeddings.\n",
    "        x: (B, C, T) with T % slice_size == 0\n",
    "        \"\"\"\n",
    "        B, C, T = x.shape\n",
    "        assert T % self.slice_size == 0, \"T must be divisible by slice_size\"\n",
    "        N = T // self.slice_size\n",
    "\n",
    "        # (B, C, T) -> (B, N, C, slice)\n",
    "        x_slices = x.view(B, C, N, self.slice_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # project time then channels -> (B, N, Dc, Dt)\n",
    "        xt = self.time_projection(x_slices)                 # (B, N, C, Dt)\n",
    "        xt = xt.permute(0, 1, 3, 2)                         # (B, N, Dt, C)\n",
    "        xt = self.channel_projection(xt)                    # (B, N, Dt, Dc)\n",
    "        xt = xt.permute(0, 1, 3, 2)                         # (B, N, Dc, Dt)\n",
    "\n",
    "        # add axis embeddings before flatten\n",
    "        xt = xt + self.time_positional_emb + self.channel_positional_emb\n",
    "\n",
    "        # flatten and map to D\n",
    "        tokens = self.token_projection(xt.reshape(B, N, -1))  # (B, N, D)\n",
    "\n",
    "        # token positional (in D space)\n",
    "        if add_token_pos:\n",
    "            tokens = tokens + self.token_positional_emb[:, :N, :]\n",
    "\n",
    "        if return_targets:\n",
    "            targets = self._targets_from_raw(x)  # (B, N, C*slice)\n",
    "            return tokens, targets\n",
    "        return tokens\n",
    "\n",
    "\n",
    "# -------------------- ENCODER + MAE --------------------\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self, c_dim=129, t_dim=200, slice_size=10, emb_dim=512, nhead=8,\n",
    "                 num_layers=6, ff_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert t_dim % slice_size == 0\n",
    "        assert emb_dim % nhead == 0, \"emb_dim must be divisible by nhead\"\n",
    "\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.nb_tokens = t_dim // slice_size\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = Vit_EEG_Embedding(\n",
    "            nb_tokens=self.nb_tokens, c_dim=c_dim, t_dim=t_dim,\n",
    "            slice_size=slice_size, target_c_dim=c_dim, target_t_dim=slice_size,\n",
    "            emb_dim=emb_dim\n",
    "        )\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead,\n",
    "            dim_feedforward=ff_mult * emb_dim,\n",
    "            dropout=dropout, activation=\"gelu\",\n",
    "            batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers, enable_nested_tensor=False)\n",
    "\n",
    "        # MAE bits\n",
    "        self.mask_token = nn.Parameter(torch.zeros(emb_dim))\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        # decoder: predict raw slice (C*slice) per token\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * emb_dim, c_dim * slice_size)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _make_mask(self, B: int, ratio: float, device=None) -> torch.BoolTensor:\n",
    "        N = self.nb_tokens\n",
    "        k = max(1, int(ratio * N))\n",
    "        mask = torch.zeros(B, N, dtype=torch.bool, device=device)\n",
    "        for b in range(B):\n",
    "            idx = torch.randperm(N, device=device)[:k]\n",
    "            mask[b, idx] = True\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Tensor, mask: torch.BoolTensor | None = None, return_pred: bool = False):\n",
    "        \"\"\"\n",
    "        x: (B, C, T)\n",
    "        mask: (B, N) True -> masked; if None, no masking (e.g., finetune/inference)\n",
    "        returns:\n",
    "          - if return_pred: (enc_out, pred, targets, mask)\n",
    "          - else: enc_out\n",
    "        \"\"\"\n",
    "        # tokens BEFORE adding token positional, plus raw targets for MAE\n",
    "        tokens_prepos, targets = self.embedding(x, add_token_pos=False, return_targets=True)  # (B, N, D), (B, N, C*slice)\n",
    "        pos = self.embedding.token_positional_emb[:, :tokens_prepos.size(1), :]               # (1, N, D)\n",
    "\n",
    "        z = tokens_prepos\n",
    "        if mask is not None:\n",
    "            z = z.clone()\n",
    "            z[mask] = self.mask_token  # replace content with learnable mask vector\n",
    "\n",
    "        z = z + pos                          # add token positions (masked + unmasked)\n",
    "        enc_out = self.encoder(z)            # (B, N, D)\n",
    "\n",
    "        if not return_pred:\n",
    "            return enc_out\n",
    "\n",
    "        pred = self.decoder(enc_out)         # (B, N, C*slice)\n",
    "        return enc_out, pred, targets, mask\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, step, path=\"checkpoint.pt\"):\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": step,\n",
    "    }, path)\n",
    "    print(f\"💾 Saved checkpoint at step {step:,} (epoch {epoch}) -> {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path=\"checkpoint.pt\", device=\"cpu\"):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"⚠️ No checkpoint found, starting fresh\")\n",
    "        return 0, 0\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    print(f\"✅ Resumed from {path} at step {ckpt['step']:,} (epoch {ckpt['epoch']})\")\n",
    "    return ckpt[\"epoch\"], ckpt[\"step\"]\n",
    "\n",
    "def _masked_nrmse_loss(pred, targets, mask, eps=1e-6):\n",
    "    pred_m = pred[mask]\n",
    "    targ_m = targets[mask]\n",
    "    var = targ_m.var(dim=1, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "    return ((pred_m - targ_m) ** 2 / var).mean()\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "def pretrain_epoch(model, train_loader, optimizer, device,\n",
    "                   epoch, start_step=0, mask_ratio=0.7,\n",
    "                   grad_clip=None, log_interval=1000, save_interval=10_000,\n",
    "                   ckpt_path=\"checkpoint.pt\"):\n",
    "    model.train()\n",
    "    loss_sum, n_batches = 0.0, 0\n",
    "    curr_loss , curr_n_batches = 0.0, 0\n",
    "    global_step = start_step\n",
    "\n",
    "    for batch_idx, x in enumerate(tqdm(train_loader, desc=f\"train e{epoch}\")):\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            x = x[0]\n",
    "            \n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "\n",
    "        B = x.size(0)\n",
    "        mask = model._make_mask(B, mask_ratio, device=x.device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        _, pred, targets, m = model(x, mask=mask, return_pred=True)\n",
    "\n",
    "        loss = _masked_nrmse_loss(pred, targets, m)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update meters\n",
    "        loss_val = float(loss.detach().cpu().item())\n",
    "        loss_sum += loss_val\n",
    "        n_batches += 1\n",
    "        curr_loss += loss_val\n",
    "        curr_n_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            avg = curr_loss / curr_n_batches\n",
    "            curr_loss = 0.0\n",
    "            curr_n_batches = 0\n",
    "            raw_mse = float(F.mse_loss(pred[m], targets[m]).detach().cpu().item())\n",
    "            print(f\"[train] step {global_step:,} | loss {loss_val:.6f} | avg {avg:.6f} | raw_mse {raw_mse:.2e}\")\n",
    "\n",
    "        if global_step % save_interval == 0:\n",
    "            save_checkpoint(model, optimizer, epoch, global_step, path=ckpt_path)\n",
    "\n",
    "    return loss_sum / max(1, n_batches), global_step\n",
    "\n",
    "# -------------------- EVAL LOOP --------------------\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, mask_ratio=0.7, log_interval=1000, tag=\"val\", fixed_mask=True):\n",
    "    model.eval()\n",
    "    loss_sum, n_batches = 0.0, 0\n",
    "    curr_loss , curr_n_batches = 0.0, 0\n",
    "    for batch_idx, x in enumerate(tqdm(loader, desc=tag)):\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            x = x[0]\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "\n",
    "        B = x.size(0)\n",
    "        if fixed_mask:\n",
    "            N = model.nb_tokens\n",
    "            k = max(1, int(mask_ratio * N))\n",
    "            mask = torch.zeros(B, N, dtype=torch.bool, device=x.device)\n",
    "            mask[:, :k] = True\n",
    "        else:\n",
    "            mask = model._make_mask(B, mask_ratio, device=x.device)\n",
    "\n",
    "        _, pred, targets, m = model(x, mask=mask, return_pred=True)\n",
    "        loss = _masked_nrmse_loss(pred, targets, m)\n",
    "\n",
    "        loss_val = float(loss.detach().cpu().item())\n",
    "        loss_sum += loss_val\n",
    "        n_batches += 1\n",
    "        curr_loss += loss_val\n",
    "        curr_n_batches += 1\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            avg = curr_loss / curr_n_batches\n",
    "            curr_loss = 0.0\n",
    "            curr_n_batches = 0\n",
    "            raw_mse = float(F.mse_loss(pred[m], targets[m]).detach().cpu().item())\n",
    "            print(f\"[{tag}] batch {batch_idx+1} | loss {loss_val:.6f} | avg {avg:.6f} | raw_mse {raw_mse:.2e}\")\n",
    "\n",
    "    return loss_sum / max(1, n_batches)\n",
    "\n",
    "# -------------------- FULL TRAIN DRIVER --------------------\n",
    "def pretrain(model, train_loader, val_loader, test_loader, optimizer, device,\n",
    "             epochs=10, mask_ratio=0.7, grad_clip=1.0, log_interval=100,\n",
    "             save_interval=10_000, ckpt_path=\"checkpoint.pt\"):\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    # resume if checkpoint exists\n",
    "    start_epoch, global_step = load_checkpoint(model, optimizer, ckpt_path, device=device)\n",
    "\n",
    "    for epoch in range(start_epoch+1, epochs + 1):\n",
    "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "        tr, global_step = pretrain_epoch(model, train_loader, optimizer, device,\n",
    "                                         epoch, start_step=global_step,\n",
    "                                         mask_ratio=mask_ratio,\n",
    "                                         grad_clip=grad_clip,\n",
    "                                         log_interval=log_interval,\n",
    "                                         save_interval=save_interval,\n",
    "                                         ckpt_path=ckpt_path)\n",
    "        va = eval_epoch(model, val_loader, device, mask_ratio=mask_ratio,\n",
    "                        log_interval=log_interval, tag=\"val\")\n",
    "        print(f\"epoch {epoch:02d} | train {tr:.4f} | val {va:.4f}\")\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            print(f\"  ↳ new best val: {best_val:.4f}\")\n",
    "            # also save best model\n",
    "            save_checkpoint(model, optimizer, epoch, global_step, path=\"best_\"+ckpt_path)\n",
    "\n",
    "    te = eval_epoch(model, test_loader, device, mask_ratio=mask_ratio,\n",
    "                    log_interval=log_interval, tag=\"test\")\n",
    "    print(f\"\\n==> test loss: {te:.4f}\")\n",
    "    return best_val, te\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resumed from checkpoint.pt at step 70,000 (epoch 2)\n",
      "\n",
      "=== Epoch 3/20 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   1%|▏         | 1001/68061 [02:26<2:40:23,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 71,000 | loss 0.177026 | avg 0.145068 | raw_mse 1.83e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   3%|▎         | 2001/68061 [04:57<2:55:02,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 72,000 | loss 0.104785 | avg 0.147324 | raw_mse 2.22e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   4%|▍         | 3001/68061 [07:34<2:48:56,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 73,000 | loss 0.125967 | avg 0.145808 | raw_mse 8.11e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   6%|▌         | 4001/68061 [10:11<2:57:46,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 74,000 | loss 0.152168 | avg 0.147062 | raw_mse 8.19e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   7%|▋         | 5001/68061 [12:48<2:44:45,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 75,000 | loss 0.110713 | avg 0.143594 | raw_mse 7.97e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:   9%|▉         | 6001/68061 [15:26<2:39:10,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 76,000 | loss 0.119568 | avg 0.144321 | raw_mse 3.22e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  10%|█         | 7000/68061 [18:10<3:18:26,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 77,000 | loss 0.149486 | avg 0.145632 | raw_mse 9.58e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  12%|█▏        | 8000/68061 [21:26<3:02:58,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 78,000 | loss 0.104593 | avg 0.145571 | raw_mse 1.52e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  13%|█▎        | 9000/68061 [24:45<3:12:52,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 79,000 | loss 0.127671 | avg 0.143587 | raw_mse 4.85e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  15%|█▍        | 9999/68061 [27:59<3:10:02,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 80,000 | loss 0.162697 | avg 0.146029 | raw_mse 5.58e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  15%|█▍        | 10000/68061 [28:00<5:03:01,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint at step 80,000 (epoch 3) -> checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  16%|█▌        | 11000/68061 [31:13<3:37:03,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 81,000 | loss 0.145672 | avg 0.143305 | raw_mse 5.92e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  18%|█▊        | 12000/68061 [34:29<2:54:12,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 82,000 | loss 0.211408 | avg 0.143768 | raw_mse 6.39e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  19%|█▉        | 13001/68061 [37:30<2:39:54,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 83,000 | loss 0.211233 | avg 0.145149 | raw_mse 7.06e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  21%|██        | 14001/68061 [40:26<2:30:25,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 84,000 | loss 0.134917 | avg 0.143524 | raw_mse 5.58e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  22%|██▏       | 15001/68061 [43:19<2:31:43,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 85,000 | loss 0.198306 | avg 0.146326 | raw_mse 1.20e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  24%|██▎       | 16000/68061 [46:15<2:30:14,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 86,000 | loss 0.149110 | avg 0.144572 | raw_mse 7.94e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  25%|██▍       | 17001/68061 [49:07<2:10:55,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step 87,000 | loss 0.174316 | avg 0.145796 | raw_mse 3.64e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train e3:  25%|██▍       | 17012/68061 [49:09<2:27:31,  5.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     12\u001b[39m optimizer = torch.optim.AdamW(\n\u001b[32m     13\u001b[39m     model.parameters(),\n\u001b[32m     14\u001b[39m     lr=\u001b[32m1e-3\u001b[39m,\n\u001b[32m     15\u001b[39m     weight_decay=\u001b[32m0.05\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# run training with checkpointing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m best_val, test_loss = \u001b[43mpretrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# save every 10k steps\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoint.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# checkpoint file\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mpretrain\u001b[39m\u001b[34m(model, train_loader, val_loader, test_loader, optimizer, device, epochs, mask_ratio, grad_clip, log_interval, save_interval, ckpt_path)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch+\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     tr, global_step = \u001b[43mpretrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     va = eval_epoch(model, val_loader, device, mask_ratio=mask_ratio,\n\u001b[32m    278\u001b[39m                     log_interval=log_interval, tag=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mpretrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, device, epoch, start_step, mask_ratio, grad_clip, log_interval, save_interval, ckpt_path)\u001b[39m\n\u001b[32m    193\u001b[39m _, pred, targets, m = model(x, mask=mask, return_pred=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    195\u001b[39m loss = _masked_nrmse_loss(pred, targets, m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    198\u001b[39m     nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Vit_EEG_Encoder(\n",
    "    c_dim=129,\n",
    "    t_dim=200,\n",
    "    slice_size=10,\n",
    "    emb_dim=512,\n",
    "    nhead=8,\n",
    "    num_layers=8\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "\n",
    "# run training with checkpointing\n",
    "best_val, test_loss = pretrain(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=20,\n",
    "    mask_ratio=0.3,\n",
    "    grad_clip=1.0,\n",
    "    log_interval=1000,\n",
    "    save_interval=10_000,          # save every 10k steps\n",
    "    ckpt_path=\"checkpoint.pt\"      # checkpoint file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vit_eeg_encoder.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Encoder and Decoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder  ,c_dim=129, t_dim=200  ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder =encoder\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.extract_features(x)\n",
    "        x= x.mean(dim=1)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_regressor_model = Vit_EEG_RT_Decoder(model).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 1e-5},   # small lr\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 1e-4}  # normal lr\n",
    "], weight_decay=0.05)\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "loss_f = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "train nRMSE : 4.317117803763316\n",
      "test nRMSE : 4.545596083566749\n",
      "val nRMSE : 4.436107194537155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:10<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr : 1e-05\n",
      "train epoch : 1 , loss : 0.3477277384895199 , nRMSE : 0.9827244811715338\n",
      "val epoch : 1 , nRMSE : 1.0387631450425168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:01<00:00, 33.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m     loss = loss_f(y_pred.squeeze(-\u001b[32m1\u001b[39m) , y)\n\u001b[32m     39\u001b[39m     cumulative_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m nrmse_over_test = \u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrt_regressor_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ccd_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest epoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m , loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcumulative_loss/\u001b[38;5;28mlen\u001b[39m(test_ccd_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m , nRMSE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrmse_over_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m      6\u001b[39m n = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     19\u001b[39m shard_pos = index // \u001b[38;5;28mself\u001b[39m.shard_size \n\u001b[32m     20\u001b[39m window_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwindow_shard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_memmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_shard_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m rt_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrt_shard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m Y = np.lib.format.open_memmap(rt_shard_path , mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\lib\\format.py:944\u001b[39m, in \u001b[36mopen_memmap\u001b[39m\u001b[34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001b[39m\n\u001b[32m    941\u001b[39m         offset = fp.tell()\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    943\u001b[39m     \u001b[38;5;66;03m# Read the header of the file first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    945\u001b[39m         version = read_magic(fp)\n\u001b[32m    946\u001b[39m         _check_version(version)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"before training\")\n",
    "print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    scheduler1.step(nrmse_val)\n",
    "    if lr!= optimizer.param_groups[0]['lr']:\n",
    "        print(f\"new lr : {optimizer.param_groups[0]['lr']}\")\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 test nRMSE : 1.1583635492097804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_r5_test =pairs_to_fast_loading_shards([] ,\"final_r5_test\" , shard_size)\n",
    "r5_test_ccd_data = EEGDataset(final_r5_test ,  shard_size)\n",
    "\n",
    "r5_test_ccd_dataloader = DataLoader(r5_test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim))  # learnable query\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        x = self.pool(x)                      # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 1e-4},    \n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 1e-4},\n",
    "], weight_decay=0.05)\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loss_f = nn.SmoothL1Loss(beta=0.4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 7e-5},    \n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 7e-5},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 7e-5},\n",
    "], weight_decay=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "best model loaded with rnmse : 0.9513838748892413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 938/2101 [00:50<01:02, 18.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m rt_regressor_model.train()\n\u001b[32m     17\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ccd_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     19\u001b[39m shard_pos = index // \u001b[38;5;28mself\u001b[39m.shard_size \n\u001b[32m     20\u001b[39m window_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwindow_shard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_memmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_shard_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m rt_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrt_shard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m Y = np.lib.format.open_memmap(rt_shard_path , mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\lib\\format.py:944\u001b[39m, in \u001b[36mopen_memmap\u001b[39m\u001b[34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001b[39m\n\u001b[32m    941\u001b[39m         offset = fp.tell()\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    943\u001b[39m     \u001b[38;5;66;03m# Read the header of the file first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_check_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"before training\")\n",
    "#print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "#print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "#print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n",
      "r5 test nRMSE : 0.9530603590279609\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "final_r5_test =pairs_to_fast_loading_shards([] ,\"final_r5_test\" , shard_size)\n",
    "r5_test_ccd_data = EEGDataset(final_r5_test ,  shard_size)\n",
    "\n",
    "r5_test_ccd_dataloader = DataLoader(r5_test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over fitting or r5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:19<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 1 , loss : 0.15343870099964022 , nRMSE : 0.9352530453511047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:19<00:00, 24.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 2 , loss : 0.14974657910628408 , nRMSE : 0.9239035630424651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:18<00:00, 25.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 3 , loss : 0.14491455839310266 , nRMSE : 0.9153584355400755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:18<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 4 , loss : 0.14319177399456248 , nRMSE : 0.9058914321551959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:18<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 5 , loss : 0.13980630712135683 , nRMSE : 0.8916456415217986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:18<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 6 , loss : 0.13682088624481661 , nRMSE : 0.8811182185052867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:18<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 7 , loss : 0.13176102394764685 , nRMSE : 0.8736697282149481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:19<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 8 , loss : 0.12840041065124896 , nRMSE : 0.8598803175685144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:20<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 9 , loss : 0.12531180846414486 , nRMSE : 0.8356834321556025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [00:21<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 epoch : 10 , loss : 0.1220434103711245 , nRMSE : 0.8287744737426552\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 1e-5},    \n",
    "        {\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 1e-4},\n",
    "    ], weight_decay=0.05)\n",
    "    scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "    loss_f = nn.MSELoss()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(r5_test_ccd_dataloader):\n",
    "        \n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    nrmse_r5 = nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader ,device)\n",
    "    scheduler1.step(nrmse_r5)\n",
    "    print(f\"r5 epoch : {epoch +1} , loss : {cumulative_loss/len(r5_test_ccd_dataloader)} , nRMSE : {nrmse_r5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"history/last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.alpha = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.beta = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def merge(self, x, y):\n",
    "        return self.alpha * x + self.beta * y\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        a = self.pool(x)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(x)             # (B, 512)\n",
    "        x = self.merge(a, b)                  # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "best model loaded with rnmse : 0.9269812395697398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:37<00:00,  6.68it/s]\n",
      "100%|██████████| 134/134 [00:16<00:00,  8.18it/s]\n",
      "100%|██████████| 134/134 [00:12<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.15313250376884616 , nRMSE : 0.9821708177380102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:35<00:00,  6.78it/s]\n",
      "100%|██████████| 134/134 [00:16<00:00,  8.35it/s]\n",
      "100%|██████████| 134/134 [00:14<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , loss : 0.15457140604284272 , nRMSE : 0.9865407311703274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:42<00:00,  6.47it/s]\n",
      "100%|██████████| 134/134 [00:18<00:00,  7.31it/s]\n",
      "100%|██████████| 134/134 [00:14<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 3 , loss : 0.15258370673478538 , nRMSE : 0.9803930496263682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:32<00:00,  6.90it/s]\n",
      "100%|██████████| 134/134 [00:15<00:00,  8.87it/s]\n",
      "100%|██████████| 134/134 [00:11<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 4 , loss : 0.14942933218692667 , nRMSE : 0.9701093624206569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:14<00:00,  7.84it/s]\n",
      "100%|██████████| 134/134 [00:15<00:00,  8.69it/s]\n",
      "100%|██████████| 134/134 [00:11<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 5 , loss : 0.14915233010898776 , nRMSE : 0.969208045473612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:26<00:00,  7.16it/s]\n",
      "100%|██████████| 134/134 [00:18<00:00,  7.10it/s]\n",
      "100%|██████████| 134/134 [00:14<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 6 , loss : 0.1497997663247941 , nRMSE : 0.9713476410004903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:43<00:00,  6.42it/s]\n",
      "100%|██████████| 134/134 [00:18<00:00,  7.12it/s]\n",
      "100%|██████████| 134/134 [00:14<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 7 , loss : 0.14754700118592426 , nRMSE : 0.9638346275957183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [02:42<00:00,  6.46it/s]\n",
      "100%|██████████| 134/134 [00:18<00:00,  7.21it/s]\n",
      "100%|██████████| 134/134 [00:14<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 8 , loss : 0.14819167740643024 , nRMSE : 0.9659677463988544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    #{\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 8e-4},    \n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.alpha, \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.beta, \"lr\": 2e-4},\n",
    "], weight_decay=2e-2)\n",
    "\n",
    "epochs = 8\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 300\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()  \n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [01:28<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5 test nRMSE : 0.8898448906710629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "r5_test_ccd_data = EEGDataset(shards_path ,  shard_size , \"final_r5_test\")\n",
    "\n",
    "r5_test_ccd_dataloader = DataLoader(r5_test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, dim=512, hidden=256, bias_to_x=0.5):\n",
    "        super().__init__()\n",
    "        self.ln_x = nn.LayerNorm(dim)\n",
    "        self.ln_y = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, dim)\n",
    "        )\n",
    "        # Optional: bias gate toward x at init (sigmoid(bias_to_x))\n",
    "        nn.init.constant_(self.mlp[-1].bias, bias_to_x)\n",
    "\n",
    "    def forward(self, x, y):           # x,y: (B, dim)\n",
    "        x_n = self.ln_x(x)\n",
    "        y_n = self.ln_y(y)\n",
    "        h = torch.cat([x_n, y_n], dim=-1)\n",
    "        g = torch.sigmoid(self.mlp(h)) # (B, dim)\n",
    "        fused = g * x + (1.0 - g) * y  # convex blend per feature\n",
    "        return fused, g                # returning g helps you log/inspect\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.fusion = GatedFusion(dim=512, hidden=256)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        a = self.pool(x)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(x)             # (B, 512)\n",
    "        x , _= self.fusion(a, b)                  # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "best model loaded with rnmse : 1.0002088929500983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 332/364 [03:30<00:19,  1.67it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\history\\last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 1e-4},    \n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": rt_regressor_model.fusion.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 5e-4},\n",
    "], weight_decay=0.02)\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 300\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()  \n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        var_loss = ((y_pred.var(unbiased=False) - y.var(unbiased=False)) ** 2)\n",
    "        loss = loss_f(y_pred.squeeze(-1), y) + 2e-2 * var_loss\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:12<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 0.9469827677936646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results=nrmse_over_data(rt_regressor_model , test_ccd_dataloader,device)\n",
    "print(f\"test nRMSE : {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9464991508025221\n",
      "best model loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:12<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true -> mean: 1.606110 | std: 0.413031 | min: 0.000000 | max: 2.410000\n",
      "y_pred -> mean: 1.588255 | std: 0.199178 | min: 0.975843 | max: 2.361883\n"
     ]
    }
   ],
   "source": [
    "ys = [ ]\n",
    "y_preds = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        ys.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "ys = torch.cat(ys, dim=0)\n",
    "y_preds = torch.cat(y_preds, dim=0)\n",
    "# stats for y_true (ys) and y_pred (y_preds)\n",
    "def print_stats(t: torch.Tensor, name: str):\n",
    "    t = t.detach().float().cpu().flatten()\n",
    "    mean = t.mean().item()\n",
    "    std  = t.std(unbiased=False).item()  # population std to match their scoring\n",
    "    tmin = t.min().item()\n",
    "    tmax = t.max().item()\n",
    "    print(f\"{name} -> mean: {mean:.6f} | std: {std:.6f} | min: {tmin:.6f} | max: {tmax:.6f}\")\n",
    "\n",
    "print_stats(ys, \"y_true\")\n",
    "print_stats(y_preds, \"y_pred\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true -> mean: 1.606110 | std: 0.413031 | min: 0.000000 | max: 2.410000\n",
      "y_pred -> mean: 1.624534 | std: 0.186095 | min: 1.040085 | max: 2.277258\n"
     ]
    }
   ],
   "source": [
    "print_stats(ys, \"y_true\")\n",
    "print_stats(y_preds, \"y_pred\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 4 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens+1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "        self.CLS = nn.Parameter(torch.zeros(1, 1, self.emb_dim))\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.CLS)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x= torch.concat([self.CLS.expand(x.shape[0] , 1 , x.shape[2]) , x] , dim=1)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x \n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.mask_decorder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "\n",
    "\n",
    "\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  ,task = None ,  mask_index =None):\n",
    "\n",
    "        if task == \"demasking\":\n",
    "            emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "\n",
    "            transformed = self.transformer_encoder(emb_x)\n",
    "            # (B , N+1 , EMB_DIM)\n",
    "            features_without_cls = transformed[:, 1:, :]\n",
    "            # (b , N , EMB_DIM)\n",
    "            result = self.mask_decorder(features_without_cls)\n",
    "            # (B , N , C * slice)\n",
    "            return result , targets\n",
    "\n",
    "\n",
    "        else :\n",
    "            return self.extract_features(x)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.alpha = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.beta = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.alpha2 = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.beta2 = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def merge1(self, x, y):\n",
    "        return self.alpha * x + self.beta * y\n",
    "    def merge2(self, x, y):\n",
    "        return self.alpha2 * x + self.beta2 * y\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        cls_token = enc[:, 0, :]\n",
    "        x = enc[: , 1: , :]\n",
    "        a = self.pool(x)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(x)             # (B, 512)\n",
    "        x = self.merge1(a, b)                  # (B, 512)\n",
    "        x= self.merge2(x , cls_token)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NRMSELoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse = torch.mean((y_pred - y_true) ** 2)\n",
    "        rmse = torch.sqrt(mse + self.eps)\n",
    "        std = y_true.std().clamp_min(self.eps)\n",
    "        return rmse / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed locating file data/38: file not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m save_path =\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdisque d\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mai_stuff\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mprojects\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpytorchtraining\u001b[39m\u001b[33m\\\u001b[39m\u001b[33meeg_competition\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlast_checkpoint2.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(save_path):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ckpt.keys)\n\u001b[32m      7\u001b[39m     model.load_state_dict(ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\serialization.py:1521\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\serialization.py:2119\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2117\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2118\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2119\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2122\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:532\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ):\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    530\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    534\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\serialization.py:2083\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2081\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2082\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\serialization.py:2036\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2029\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_offset != zip_file.get_record_offset(name):\n\u001b[32m   2030\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2031\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2032\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m expected \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2033\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file.get_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2034\u001b[39m             )\n\u001b[32m   2035\u001b[39m     storage = (\n\u001b[32m-> \u001b[39m\u001b[32m2036\u001b[39m         \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2037\u001b[39m         ._typed_storage()\n\u001b[32m   2038\u001b[39m         ._untyped_storage\n\u001b[32m   2039\u001b[39m     )\n\u001b[32m   2040\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[32m   2041\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: PytorchStreamReader failed locating file data/38: file not found"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path =r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\history\\last_checkpoint2.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    print(ckpt.keys)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    print(f\"model loaded from {save_path}\")\n",
    "else:\n",
    "    print(\"didnt load\")\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=True  ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "base_lr = 1e-4 \n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": base_lr*0.1},\n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.alpha, \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.beta, \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.alpha2, \"lr\": base_lr * 4},\n",
    "    {\"params\": rt_regressor_model.beta2, \"lr\": base_lr * 4},\n",
    "], weight_decay=2e-3)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 100\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(val_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(val_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9521489215412742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:12<00:00, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test result : 0.9521488786543082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    encoder = Vit_EEG_Encoder().to(device)\n",
    "    model = Vit_EEG_RT_Decoder(encoder , fine_tune_encoder=False  ).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "else:\n",
    "    raise(\"no best model found\")\n",
    "\n",
    "\n",
    "final_test = nrmse_over_data(model,test_ccd_dataloader , device)\n",
    "print(f\"final test result : {final_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens+1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "        self.CLS = nn.Parameter(torch.zeros(1, 1, self.emb_dim))\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.CLS)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N+1 , emb_dim)\n",
    "        x= torch.concat([self.CLS.expand(x.shape[0] , 1 , x.shape[2]) , x] , dim=1)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x \n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.mask_decorder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  ,task = None ,  mask_index =None):\n",
    "\n",
    "        if task == \"demasking\":\n",
    "            emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "\n",
    "            transformed = self.transformer_encoder(emb_x)\n",
    "            # (B , N+1 , EMB_DIM)\n",
    "            features_without_cls = transformed[:, 1:, :]\n",
    "            # (b , N , EMB_DIM)\n",
    "            result = self.mask_decorder(features_without_cls)\n",
    "            # (B , N , C * slice)\n",
    "            return result , targets\n",
    "\n",
    "\n",
    "        else :\n",
    "            return self.extract_features(x)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        cls_token = enc[:, 0, :]\n",
    "       \n",
    "        x = self.regressor(cls_token)                 # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\history\\last_checkpoint2.pt\n",
      "before training\n",
      "best model loaded with rnmse : 0.9482655338027726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [03:07<00:00,  7.75it/s]\n",
      "100%|██████████| 179/179 [00:15<00:00, 11.87it/s]\n",
      "100%|██████████| 179/179 [00:12<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.1826191779080383 , nRMSE : 1.0017368083655347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 153/1453 [00:18<02:33,  8.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m rt_regressor_model.train()\n\u001b[32m     53\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ccd_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     24\u001b[39m raw = X[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m     25\u001b[39m rt = Y[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tensor_raw = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     27\u001b[39m tensor_rt = torch.tensor(rt , dtype=torch.float32).to(device)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_raw , tensor_rt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path =r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\history\\last_checkpoint2.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    print(f\"model loaded from {save_path}\")\n",
    "else:\n",
    "    print(\"didnt load\")\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False  ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "base_lr = 2e-4 \n",
    "optimizer = torch.optim.AdamW([\n",
    "    #{\"params\": rt_regressor_model.encoder.parameters(), \"lr\": base_lr*0.1},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": base_lr * 4},\n",
    "\n",
    "], weight_decay=2e-3)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 100\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(val_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(val_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9521489215412742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:12<00:00, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test result : 0.9521488786543082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    encoder = Vit_EEG_Encoder().to(device)\n",
    "    model = Vit_EEG_RT_Decoder(encoder , fine_tune_encoder=False  ).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "else:\n",
    "    raise(\"no best model found\")\n",
    "\n",
    "\n",
    "final_test = nrmse_over_data(model,test_ccd_dataloader , device)\n",
    "print(f\"final test result : {final_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:15<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true -> mean: 1.606110 | std: 0.413031 | min: 0.000000 | max: 2.410000\n",
      "y_pred -> mean: 1.598042 | std: 0.000114 | min: 1.597352 | max: 1.598535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ys = [ ]\n",
    "y_preds = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        ys.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "ys = torch.cat(ys, dim=0)\n",
    "y_preds = torch.cat(y_preds, dim=0)\n",
    "# stats for y_true (ys) and y_pred (y_preds)\n",
    "def print_stats(t: torch.Tensor, name: str):\n",
    "    t = t.detach().float().cpu().flatten()\n",
    "    mean = t.mean().item()\n",
    "    std  = t.std(unbiased=False).item()  # population std to match their scoring\n",
    "    tmin = t.min().item()\n",
    "    tmax = t.max().item()\n",
    "    print(f\"{name} -> mean: {mean:.6f} | std: {std:.6f} | min: {tmin:.6f} | max: {tmax:.6f}\")\n",
    "\n",
    "print_stats(ys, \"y_true\")\n",
    "print_stats(y_preds, \"y_pred\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Embedding & Encoder (no masking/decoder)\n",
    "# -------------------------\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                 target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        # per-slice & per-channel positional hints (not the token index PE)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1, 1, 1, self.target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1, 1, self.target_c_dim, 1))\n",
    "\n",
    "        self.token_projection = nn.Linear(self.target_c_dim * self.target_t_dim, self.emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(self.nb_tokens, self.emb_dim))  # added optionally\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "    def zscore_bct(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def tokenize_pre_tokenPE(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,C,T) -> tokens BEFORE adding token positional embeddings (B, N, D)\n",
    "        Keeps local time/channel PEs (not index PE), so content is enriched but\n",
    "        the model can't trivially read the original token index.\n",
    "        \"\"\"\n",
    "        x = self.zscore_bct(x)\n",
    "        # patch: (B, C, T)-> (B, N, C, slice)\n",
    "        B, C, T = x.shape\n",
    "        x = x.reshape(B, C, T // self.slice_size, self.slice_size).permute(0, 2, 1, 3)\n",
    "        # project time then channels\n",
    "        x = self.time_projection(x)                 # (B,N,C,Tt)\n",
    "        x = x.permute(0, 1, 3, 2)                   # (B,N,Tt,C)\n",
    "        x = self.channel_projection(x)              # (B,N,Tt,Ct)\n",
    "        x = x.permute(0, 1, 3, 2)                   # (B,N,Ct,Tt)\n",
    "        # add local PEs\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # flatten to token vector\n",
    "        x = x.reshape(B, x.shape[1], -1)            # (B,N,Ct*Tt)\n",
    "        x = self.token_projection(x)                # (B,N,D)\n",
    "        return x\n",
    "\n",
    "    def add_token_pos(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # tokens: (B,N,D)\n",
    "        return self.pre_norm(tokens + self.token_positional_emb)\n",
    "\n",
    "class Vit_EEG_SSL(nn.Module):\n",
    "    def __init__(self, c_dim=129, t_dim=200, slice_size=10,\n",
    "                 emb_dim=512, nhead=8, nb_layers=12,\n",
    "                 target_c_dim=64, target_t_dim=6,\n",
    "                 proj_dim=256, temp=0.2, n_pos_bins=8, n_bands=5):\n",
    "        super().__init__()\n",
    "        assert t_dim % slice_size == 0\n",
    "        self.nb_tokens = t_dim // slice_size\n",
    "        self.embedding = Vit_EEG_Embedding(\n",
    "            nb_tokens=self.nb_tokens, c_dim=c_dim, t_dim=t_dim, slice_size=slice_size,\n",
    "            target_c_dim=target_c_dim, target_t_dim=target_t_dim, emb_dim=emb_dim\n",
    "        )\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead, dim_feedforward=emb_dim*4,\n",
    "            batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nb_layers, enable_nested_tensor=False)\n",
    "\n",
    "        # projection head for contrastive\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, proj_dim)\n",
    "        )\n",
    "        self.temp = temp\n",
    "\n",
    "        # Temporal Order Verification (binary: i before j?)\n",
    "        self.tov_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim*2),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Relative Position Regression (bucketed)\n",
    "        self.n_pos_bins = n_pos_bins\n",
    "        self.rpr_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim*2),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, n_pos_bins)\n",
    "        )\n",
    "\n",
    "        # Band-Notch Identification (δ, θ, α, β, low-γ)\n",
    "        self.n_bands = n_bands\n",
    "        self.bni_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, n_bands)\n",
    "        )\n",
    "\n",
    "    # ---------- safe EEG augmentations ----------\n",
    "    @staticmethod\n",
    "    def amp_scale(x, smin=0.9, smax=1.1):\n",
    "        s = torch.empty(x.size(0), 1, 1, device=x.device).uniform_(smin, smax)\n",
    "        return x * s\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(x, max_shift=20):\n",
    "        # circular shift along T by up to ±max_shift samples\n",
    "        if max_shift <= 0: return x\n",
    "        B, C, T = x.shape\n",
    "        shift = torch.randint(-max_shift, max_shift+1, (B,1,1), device=x.device)\n",
    "        idx = (torch.arange(T, device=x.device).view(1,1,T) - shift) % T\n",
    "        return x.gather(2, idx.expand(B, C, T))\n",
    "\n",
    "    @staticmethod\n",
    "    def channel_dropout(x, p=0.1):\n",
    "        if p <= 0: return x\n",
    "        B, C, T = x.shape\n",
    "        m = (torch.rand(B, C, 1, device=x.device) > p).float()\n",
    "        return x * m\n",
    "\n",
    "    def two_views(self, x):\n",
    "        v1 = self.amp_scale(self.time_shift(self.channel_dropout(x, p=0.1), max_shift=15))\n",
    "        v2 = self.amp_scale(self.time_shift(self.channel_dropout(x, p=0.1), max_shift=15))\n",
    "        return v1, v2\n",
    "\n",
    "    # ---------- band-notch augmentation ----------\n",
    "    @staticmethod\n",
    "    def butter_notch_like(x, sf=100., bands=((1,4),(4,8),(8,13),(13,30),(30,45)), attn=0.3):\n",
    "        \"\"\"\n",
    "        Attenuate one random band by ~attn fraction via FFT masking (simple, differentiable-ish).\n",
    "        Returns (x_notched, band_idx)\n",
    "        \"\"\"\n",
    "        B, C, T = x.shape\n",
    "        band_idx = torch.randint(0, len(bands), (B,), device=x.device)\n",
    "        X = torch.fft.rfft(x, dim=2)            # (B,C,F)\n",
    "        F = X.shape[-1]\n",
    "        freqs = torch.linspace(0, sf/2, F, device=x.device)\n",
    "        M = torch.ones(B, 1, F, device=x.device)\n",
    "        for b in range(B):\n",
    "            lo, hi = bands[band_idx[b]]\n",
    "            mask = (freqs >= lo) & (freqs <= hi)\n",
    "            M[b, :, mask] = (1.0 - attn)\n",
    "        X_notch = X * M\n",
    "        x_notch = torch.fft.irfft(X_notch, n=T, dim=2)\n",
    "        return x_notch, band_idx\n",
    "\n",
    "    # ---------- forward utilities ----------\n",
    "    def encode_tokens(self, x, add_token_pos=True):\n",
    "        # x: (B,C,T)\n",
    "        toks = self.embedding.tokenize_pre_tokenPE(x)       # (B,N,D)\n",
    "        if add_token_pos:\n",
    "            toks = self.embedding.add_token_pos(toks)\n",
    "        z = self.encoder(toks)                               # (B,N,D)\n",
    "        return z\n",
    "\n",
    "    def global_feat(self, z, mode=\"mean\"):\n",
    "        # z: (B,N,D)\n",
    "        if mode == \"mean\":\n",
    "            return z.mean(dim=1)\n",
    "        else:\n",
    "            # simple CLS as first token (you can add an explicit CLS token if desired)\n",
    "            return z[:, 0]\n",
    "\n",
    "    # ---------- losses ----------\n",
    "    def loss_contrastive(self, x):\n",
    "        v1, v2 = self.two_views(x)\n",
    "        z1 = self.encode_tokens(v1, add_token_pos=True)\n",
    "        z2 = self.encode_tokens(v2, add_token_pos=True)\n",
    "        g1 = F.normalize(self.proj(self.global_feat(z1)), dim=-1)\n",
    "        g2 = F.normalize(self.proj(self.global_feat(z2)), dim=-1)\n",
    "\n",
    "        # NT-Xent\n",
    "        logits = g1 @ g2.t() / self.temp                      # (B,B)\n",
    "        labels = torch.arange(g1.size(0), device=g1.device)\n",
    "        loss = F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)\n",
    "        return loss * 0.5\n",
    "\n",
    "    def loss_tov_rpr(self, x, n_pairs=4):\n",
    "        \"\"\"\n",
    "        Build a permuted token sequence (before token-PE), then add token-PE AFTER permutation,\n",
    "        pass encoder, and query pairs (i,j) from the encoded sequence to:\n",
    "          - TOV: classify if original_i < original_j (binary)\n",
    "          - RPR: classify bucketed |i-j|/N\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        N = self.nb_tokens\n",
    "        # tokens BEFORE token positional embedding\n",
    "        toks = self.embedding.tokenize_pre_tokenPE(x)              # (B,N,D)\n",
    "\n",
    "        # random permutation per sample\n",
    "        perm = torch.stack([torch.randperm(N, device=x.device) for _ in range(B)], dim=0)  # (B,N)\n",
    "        toks_perm = toks.gather(1, perm.unsqueeze(-1).expand(B, N, toks.size(-1)))         # (B,N,D)\n",
    "\n",
    "        # now add token positional embeddings of the *permuted* order (prevents trivial original index leakage)\n",
    "        z = self.encoder(self.embedding.add_token_pos(toks_perm))  # (B,N,D)\n",
    "\n",
    "        # sample pairs\n",
    "        i_idx = torch.randint(0, N, (B, n_pairs), device=x.device)\n",
    "        j_idx = torch.randint(0, N, (B, n_pairs), device=x.device)\n",
    "\n",
    "        # embeddings for pairs from the permuted sequence\n",
    "        zi = z.gather(1, i_idx.unsqueeze(-1).expand(B, n_pairs, z.size(-1)))\n",
    "        zj = z.gather(1, j_idx.unsqueeze(-1).expand(B, n_pairs, z.size(-1)))\n",
    "\n",
    "        pair = torch.cat([zi, zj], dim=-1).reshape(B * n_pairs, -1)    # (B*n_pairs, 2D)\n",
    "\n",
    "        # targets computed from original positions (before permutation)\n",
    "        # find original indices corresponding to perm[i], perm[j]\n",
    "        inv_perm = torch.zeros_like(perm)\n",
    "        inv_perm.scatter_(1, perm, torch.arange(N, device=x.device).unsqueeze(0).expand(B, N))\n",
    "        # i in permuted space corresponds to original index inv_perm[:, i]\n",
    "        oi = inv_perm.gather(1, i_idx)    # (B,n_pairs)\n",
    "        oj = inv_perm.gather(1, j_idx)    # (B,n_pairs)\n",
    "\n",
    "        # TOV target: oi < oj\n",
    "        y_tov = (oi < oj).long().reshape(-1)                         # (B*n_pairs,)\n",
    "        # RPR target: bucketed normalized |oi-oj|\n",
    "        dist = (oi - oj).abs().float() / max(1, N-1)                 # [0,1]\n",
    "        bins = torch.clamp((dist * self.n_pos_bins).long(), 0, self.n_pos_bins-1).reshape(-1)\n",
    "\n",
    "        logits_tov = self.tov_head(pair).squeeze(-1)                 # (B*n_pairs,)\n",
    "        logits_rpr = self.rpr_head(pair)                              # (B*n_pairs, n_bins)\n",
    "\n",
    "        loss_tov = F.binary_cross_entropy_with_logits(logits_tov, y_tov.float())\n",
    "        loss_rpr = F.cross_entropy(logits_rpr, bins)\n",
    "\n",
    "        return loss_tov, loss_rpr\n",
    "\n",
    "    def loss_bni(self, x):\n",
    "        x_notch, band_idx = self.butter_notch_like(x, sf=100., attn=0.3)\n",
    "        z = self.encode_tokens(x_notch, add_token_pos=True)          # (B,N,D)\n",
    "        g = self.global_feat(z)\n",
    "        logits = self.bni_head(g)                                    # (B,n_bands)\n",
    "        loss = F.cross_entropy(logits, band_idx)\n",
    "        return loss\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.alpha = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.beta = nn.Parameter(torch.randn(1 ,512))\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def merge(self, x, y):\n",
    "        return self.alpha * x + self.beta * y\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder.encode_tokens(x)\n",
    "        a = self.pool(enc)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(enc)             # (B, 512)\n",
    "        x = self.merge(a, b)                  # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\models_and_checkpoints\\best_nrmse_encoder.pt\n",
      "before training\n",
      "best model loaded with rnmse : 0.9540131504039352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [03:59<00:00,  6.07it/s]\n",
      "100%|██████████| 179/179 [00:15<00:00, 11.28it/s]\n",
      "100%|██████████| 179/179 [00:12<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.1832174371782295 , nRMSE : 1.003379740963796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [03:53<00:00,  6.23it/s]\n",
      "100%|██████████| 179/179 [00:15<00:00, 11.56it/s]\n",
      "100%|██████████| 179/179 [00:12<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , loss : 0.18297953672665457 , nRMSE : 1.0026841944999179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 908/1453 [02:14<01:20,  6.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m     optimizer.zero_grad()\n\u001b[32m     80\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     scheduler.step()\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m#nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m#scheduler1.step(nrmse_val)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[38;5;66;03m#print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m#print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\optim\\adam.py:773\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    771\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m    776\u001b[39m torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Vit_EEG_SSL().to(device)\n",
    "\n",
    "save_path =os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_nrmse_encoder.pt\")\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    #model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.load_state_dict(ckpt)\n",
    "    print(f\"model loaded from {save_path}\")\n",
    "else:\n",
    "    print(\"didnt load\")\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=True  ).to(device)\n",
    "for p in rt_regressor_model.pool.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in rt_regressor_model.regressor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in rt_regressor_model.temporal_conv.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "rt_regressor_model.alpha.requires_grad = False\n",
    "rt_regressor_model.beta.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_lr = 5e-4 \n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.embedding.parameters(), \"lr\": base_lr },\n",
    "    {\"params\": rt_regressor_model.encoder.encoder.parameters(),   \"lr\": base_lr},\n",
    "    #{\"params\": rt_regressor_model.pool.parameters(), \"lr\": base_lr * 4},\n",
    "    #{\"params\": rt_regressor_model.regressor.parameters(), \"lr\": base_lr * 4},\n",
    "    #{\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": base_lr * 4},\n",
    "    #{\"params\": rt_regressor_model.alpha, \"lr\": base_lr * 4},\n",
    "    #{\"params\": rt_regressor_model.beta, \"lr\": base_lr * 4},\n",
    "\n",
    "], weight_decay=2e-3)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 6\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 100\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = os.path.join(MODELS_AND_CHECKPOINTS_PATH,\"best_model.pt\")\n",
    "best_rnmse_path = os.path.join(MODELS_AND_CHECKPOINTS_PATH,\"best_nrmse.pt\")\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(val_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(val_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9540131504039352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:20<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test result : 0.9447903801508978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model_path = os.path.join(MODELS_AND_CHECKPOINTS_PATH,\"best_model.pt\")\n",
    "best_rnmse_path = os.path.join(MODELS_AND_CHECKPOINTS_PATH,\"best_nrmse.pt\")\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    encoder = Vit_EEG_SSL().to(device)\n",
    "    model = Vit_EEG_RT_Decoder(encoder , fine_tune_encoder=False  ).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "else:\n",
    "    raise(\"no best model found\")\n",
    "\n",
    "\n",
    "final_test = nrmse_over_data(model,test_ccd_dataloader , device)\n",
    "print(f\"final test result : {final_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:12<00:00, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true -> mean: 1.606110 | std: 0.413031 | min: 0.000000 | max: 2.410000\n",
      "y_pred -> mean: 1.599067 | std: 0.153812 | min: 1.093271 | max: 2.053640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ys = [ ]\n",
    "y_preds = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        ys.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "ys = torch.cat(ys, dim=0)\n",
    "y_preds = torch.cat(y_preds, dim=0)\n",
    "# stats for y_true (ys) and y_pred (y_preds)\n",
    "def print_stats(t: torch.Tensor, name: str):\n",
    "    t = t.detach().float().cpu().flatten()\n",
    "    mean = t.mean().item()\n",
    "    std  = t.std(unbiased=False).item()  # population std to match their scoring\n",
    "    tmin = t.min().item()\n",
    "    tmax = t.max().item()\n",
    "    print(f\"{name} -> mean: {mean:.6f} | std: {std:.6f} | min: {tmin:.6f} | max: {tmax:.6f}\")\n",
    "\n",
    "print_stats(ys, \"y_true\")\n",
    "print_stats(y_preds, \"y_pred\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# better encoder decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mahdi\\.cache\\huggingface\\hub\\models--facebook--dinov2-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(image.height, image.width)  # [480, 640]\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "patch_size = model.config.patch_size\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(inputs.pixel_values.shape)  # [1, 3, 224, 224]\n",
    "batch_size, rgb, img_height, img_width = inputs.pixel_values.shape\n",
    "num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n",
    "num_patches_flat = num_patches_height * num_patches_width\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states.shape)  # [1, 1 + 256, 768]\n",
    "assert last_hidden_states.shape == (batch_size, 1 + num_patches_flat, model.config.hidden_size)\n",
    "\n",
    "cls_token = last_hidden_states[:, 0, :]\n",
    "patch_features = last_hidden_states[:, 1:, :].unflatten(1, (num_patches_height, num_patches_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm dinov3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mahdi/.cache\\torch\\hub\\facebookresearch_dinov3_main\n",
      "c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (rope_embed): RopePositionEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # examples of available DINOv3 models:\n",
    "# MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "# MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "# MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "# MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "# MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "# # we take DINOv3 ViT-L\n",
    "DINOV3_LOCATION = \"facebookresearch/dinov3\"\n",
    "MODEL_NAME = MODEL_DINOV3_VITL\n",
    "\n",
    "\n",
    "# !pip -q install torchmetrics\n",
    "# !gdown 1F9q23snVirc6o2tc9lmvW9i95SBbPusk -O dinov3.pth\n",
    "\n",
    "checkpoint_path = \"dinov3.pth\"\n",
    "\n",
    "\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"github\",\n",
    "    pretrained=False\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_SIZE = 768\n",
    "\n",
    "# quantization filter for the given patch size\n",
    "patch_quant_filter = torch.nn.Conv2d(1, 1, PATCH_SIZE, stride=PATCH_SIZE, bias=False)\n",
    "patch_quant_filter.weight.data.fill_(1.0 / (PATCH_SIZE * PATCH_SIZE))\n",
    "\n",
    "# # image resize transform to dimensions divisible by patch size\n",
    "# def resize_transform(\n",
    "#     image_size: int = IMAGE_SIZE,\n",
    "#     patch_size: int = PATCH_SIZE,\n",
    "# ) -> torch.Tensor:\n",
    "#     w, h = mask_image.size\n",
    "#     h_patches = int(image_size / patch_size)\n",
    "#     w_patches = int((w * image_size) / (h * patch_size))\n",
    "#     return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = 24\n",
    "x , y= train_ccd_data[0]\n",
    "\n",
    "patch_mask_values = []\n",
    "patch_features = []\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "        for image in tqdm([x], desc=\"Processing images\"):\n",
    "            #image = image.convert('RGB')\n",
    "            # image_resized = image_resized.unsqueeze(0).cuda()\n",
    "            image=image.reshape( 1 , x.shape[0] , x.shape[1]).repeat(3,1,1).unsqueeze(0)\n",
    "            \n",
    "\n",
    "            image_resized = image\n",
    "\n",
    "            feats = model.get_intermediate_layers(image_resized, n=range(n_layers), reshape=True, norm=True)\n",
    "            #dim = feats[-1].shape[1]\n",
    "            #patch_features.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
    "            #patch_features.append(feats.detach().cpu())\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 8, 12])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for dim_ in range(7):\n",
    "    dim_ *= 4\n",
    "    # print(dim_)\n",
    "    # continue\n",
    "    if dim_ == len(feats):\n",
    "        dim_ -= 1\n",
    "    \n",
    "    print(dim_)\n",
    "    continue\n",
    "    images = feats[dim_].squeeze().detach().cpu()[:16]\n",
    "    print(images.shape)\n",
    "    rows = cols = 4\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i], cmap=\"YlGnBu\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12288])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images.permute(1, 0, 2).view(8)\n",
    "feats[0][0].permute(1, 0, 2).reshape(8, -1).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': {'cls_token': Parameter containing:\n",
       "  tensor([[[ 0.2189,  0.0040,  0.0059,  ...,  0.0265, -0.0764,  0.0073]]],\n",
       "         device='cuda:0', requires_grad=True),\n",
       "  'storage_tokens': Parameter containing:\n",
       "  tensor([[[ 4.6495e-02,  9.6616e-03,  1.4570e-02,  ..., -8.9848e-03,\n",
       "             5.6941e-03,  1.5191e-03],\n",
       "           [ 5.8214e-02,  1.3593e-04,  2.2814e-04,  ...,  2.4267e-04,\n",
       "             2.5758e-04,  2.4468e-03],\n",
       "           [ 2.7459e-01,  4.8452e-03, -3.1404e-02,  ..., -1.7464e-04,\n",
       "             1.9677e-02, -4.2825e-04],\n",
       "           [ 1.3190e-01, -1.3641e-02, -6.1484e-03,  ...,  6.4809e-04,\n",
       "            -4.8334e-03, -1.4708e-03]]], device='cuda:0', requires_grad=True),\n",
       "  'mask_token': Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'patch_embed': PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  ),\n",
       "  'rope_embed': RopePositionEmbedding(),\n",
       "  'blocks': ModuleList(\n",
       "    (0-23): 24 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  ),\n",
       "  'norm': LayerNorm((1024,), eps=1e-05, elementwise_affine=True),\n",
       "  'head': Identity()},\n",
       " 'num_features': 1024,\n",
       " 'embed_dim': 1024,\n",
       " 'n_blocks': 24,\n",
       " 'num_heads': 16,\n",
       " 'patch_size': 16,\n",
       " 'n_storage_tokens': 4,\n",
       " 'chunked_blocks': False,\n",
       " 'untie_cls_and_patch_norms': False,\n",
       " 'cls_norm': None,\n",
       " 'untie_global_and_local_cls_norm': False,\n",
       " 'local_cls_norm': None}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': {'cls_token': Parameter containing:\n",
       "  tensor([[[ 0.2189,  0.0040,  0.0059,  ...,  0.0265, -0.0764,  0.0073]]],\n",
       "         device='cuda:0', requires_grad=True),\n",
       "  'storage_tokens': Parameter containing:\n",
       "  tensor([[[ 4.6495e-02,  9.6616e-03,  1.4570e-02,  ..., -8.9848e-03,\n",
       "             5.6941e-03,  1.5191e-03],\n",
       "           [ 5.8214e-02,  1.3593e-04,  2.2814e-04,  ...,  2.4267e-04,\n",
       "             2.5758e-04,  2.4468e-03],\n",
       "           [ 2.7459e-01,  4.8452e-03, -3.1404e-02,  ..., -1.7464e-04,\n",
       "             1.9677e-02, -4.2825e-04],\n",
       "           [ 1.3190e-01, -1.3641e-02, -6.1484e-03,  ...,  6.4809e-04,\n",
       "            -4.8334e-03, -1.4708e-03]]], device='cuda:0', requires_grad=True),\n",
       "  'mask_token': Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'patch_embed': PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  ),\n",
       "  'rope_embed': RopePositionEmbedding(),\n",
       "  'blocks': ModuleList(\n",
       "    (0-23): 24 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  ),\n",
       "  'norm': LayerNorm((1024,), eps=1e-05, elementwise_affine=True),\n",
       "  'head': Identity()},\n",
       " 'num_features': 1024,\n",
       " 'embed_dim': 1024,\n",
       " 'n_blocks': 24,\n",
       " 'num_heads': 16,\n",
       " 'patch_size': 16,\n",
       " 'n_storage_tokens': 4,\n",
       " 'chunked_blocks': False,\n",
       " 'untie_cls_and_patch_norms': False,\n",
       " 'cls_norm': None,\n",
       " 'untie_global_and_local_cls_norm': False,\n",
       " 'local_cls_norm': None}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.5053e-05,  2.4553e-05,  1.3426e-05,  ..., -8.0320e-06,\n",
       "          -1.2587e-05, -1.0118e-05],\n",
       "         [ 5.4091e-06,  9.4370e-06,  2.4789e-06,  ..., -6.3967e-06,\n",
       "          -7.6753e-06, -5.6746e-06],\n",
       "         [ 9.9896e-07,  9.3363e-06, -4.9093e-07,  ..., -1.2388e-05,\n",
       "          -1.2062e-05, -1.1841e-05],\n",
       "         ...,\n",
       "         [ 3.6496e-05,  3.1739e-05,  1.7380e-05,  ...,  1.6393e-05,\n",
       "           1.1915e-05,  1.2792e-05],\n",
       "         [ 3.3281e-05,  2.7753e-05,  1.4544e-05,  ..., -1.3508e-06,\n",
       "          -5.7176e-06, -4.8709e-06],\n",
       "         [ 5.0000e-13,  5.0000e-13,  5.0000e-13,  ...,  5.0000e-13,\n",
       "           5.0000e-13,  5.0000e-13]], device='cuda:0'),\n",
       " tensor(1.5680, device='cuda:0'))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_ccd_data[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = 4\n",
    "x , y= train_ccd_data[0]\n",
    "\n",
    "patch_mask_values = []\n",
    "patch_features = []\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "        for image in tqdm([x], desc=\"Processing images\"):\n",
    "            #image = image.convert('RGB')\n",
    "            # image_resized = image_resized.unsqueeze(0).cuda()\n",
    "            image=image.reshape( 1 , x.shape[0] , x.shape[1]).repeat(3,1,1).unsqueeze(0)\n",
    "            \n",
    "\n",
    "            image_resized = image\n",
    "\n",
    "            feats = model.get_intermediate_layers(image_resized, n=range(n_layers), reshape=True, norm=True)\n",
    "            #dim = feats[-1].shape[1]\n",
    "            #patch_features.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
    "            #patch_features.append(feats.detach().cpu())\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAALfCAYAAAD8LlRvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ2dJREFUeJzt3QeUXWW9N/5nJjOT3ntCQkIqkNC79CJNQpEqXVRAiqig0sQLigX0IqgoohRFuoWmNAOI9E7oAdJII30ySWYy5b/2uS8sEe7L/H/Py5zcO5/PWlkhZ/aX/cw5u5zv2fvsXdHS0tKSAAAAMlTmhAEAAAqKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xWAPU19enb37zm2nIkCGpc+fOacstt0z33ntvuYcFZbF8+fJ03nnnpT322CP16dMnVVRUpKuvvrrcw4KyePLJJ9PJJ5+c1l9//dS1a9c0fPjwdPDBB6fXX3+93EODNvfSSy+lgw46KK2zzjqpS5cuqV+/fmn77bdPt99+e7mHxv+hWKwBjjnmmPSTn/wkHX744emnP/1p6tChQ9prr73Sww8/XO6hQZtbsGBBOv/889Mrr7ySNtxww3IPB8rqhz/8Ybr11lvTLrvsUto/fOlLX0oPPfRQ2mSTTdKUKVPKPTxoU9OnT0+1tbXp6KOPLq0P5557bunxSZMmpSuuuKLcwyOlVNHS0tJS7kG0Z0888UTpCMVFF12UTj/99NJjq1atShMmTEgDBgxIjzzySLmHCG1+BG/x4sVp0KBB6amnnkqbb755uuqqq0oFHNqbYh+w2WabpZqamvcfe+ONN9LEiRPTgQcemH7/+9+XdXxQbk1NTWnTTTctvXd69dVXyz2cds8RizK75ZZbSkcoik+h3tOpU6d03HHHpUcffTTNnDmzrOODttaxY8dSqQBS2mabbT5QKgpjxowpnRpVHNWD9q54DzVs2LC0ZMmScg8FxaL8nn322TR27NjUo0ePDzy+xRZblP5+7rnnyjQyANZExYkG8+bNK51fDu1RXV1d6bTZN998M/3nf/5n+utf/1o6XZDyqyr3ANq7OXPmpMGDB3/o8fcemz17dhlGBcCa6rrrrkvvvPNO6btI0B59/etfT7/61a9K/11ZWZkOOOCA9LOf/azcw0KxKL+VK1eWTv34d8XpUO/9HAAKxTnkJ510Utp6661LX2CF9ui0004rfceo+PD1pptuKn3PoqGhodzDwqlQ5VdcXrb4suq/K76E9N7PAWDu3Llp7733Tj179nz/+3nQHo0fPz7tuuuu6aijjkp33HFH6TLl++yzT+k0QcpLsSiz4pSn4nSof/feY8W9LQBo35YuXZr23HPP0hdU//a3v9k3wL8ojl4U93xxf5fyUyzKbKONNiqtCMuWLfvA448//vj7Pweg/SqOYBefxhb7iuLT2fXWW6/cQ4I1ynunjRcFnPJSLNaAll2cG/ivN3YpTo0qrttf3N+iuIQaAO1TsX845JBDSpcfv/nmm0vfrYD2av78+R96bPXq1enaa68tnTqudJefL2+XWVEeitvTn3nmmaUVZvTo0emaa65J06ZNS7/5zW/KPTwoi+LqHsUpH+9dFe32229Ps2bNKv33KaecUjrHHNrL1W9uu+220hGLRYsWfeiGeEcccUTZxgZt7fjjjy+d4bH99tunoUOHlr53VFwlrbiowY9//OPUrVu3cg+x3XPn7TXkMHdxW/pih1HccXiDDTZIF1xwQdp9993LPTQoixEjRqTp06d/5M/efvvt0s+hPdhxxx3Tgw8++N/+3C6c9uSGG24ofej64osvpoULF6bu3buX7rpdfOA0adKkcg8PxQIAAPh/wXcsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAoO3uvH3d1L+FZ3L46HXC2aMe/K8770Z07JB3i44X5sRvTL7+wKZw9rl3OoSzGw2Nz3f/tVeGs9e/3SXluOJT8deqqqJTONupaqtQbtShfwjPs3KnoeHs5uPC0TSy2+p4OKW0YZ94fvKc+GtU31QRzq5qjmc7VcaXyfsebUw5Vl7113C21+D4QvLG5C+FsyNP/lM4O/e2m8LZrX55Ujg763tTUo7+p08IZ2d8+75wduyPdg1nnzrp6nD2jFsPCWev+vmSlKPDG4vD2dXbrRXOTvvuXqFc1xFHhec54aIvhrMdO8a3edsNWZVy3PV6x3B20rj6cPbxBfH9y0nja8PZsx/sGs527Zb3uX6n+K+cZpzzZDj75jNf/dhpHLEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZKtq7YR/mdElPJMDRy4JZx99tSKc7de/1b/eR+rUOZ6d/GJ83M3NzeHsU6vjXfG+Z+O/8KDBeR11dfPScPbdhtpwdmT3WK6lS3zZeumLwZmmlG58c344u/+ITinHUwvqw9kTxteFsze9HR/3l9ddEc5e+Hy3cPbre8fX4cJ1I/cLZ0/aKP5c56hcsLIs8911SHy+V/XsmDXvLQc3hLNvN8Wza3VpDGdf6TQgnN2oT3y+zYO6phwtT08LZ6sfzFsfI4YM3Cqc/dPei8PZO2bEl+nRPeKvb+HRd+Pb6nW6x+e9oD6+Lq3dvSmc3WREfLka3zO+Py1MGr4qnN1nwuD0SXLEAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJCtoqWlpaU1E3Yeflh4JgO+cEw4W/X8/HC2y6S1U466h+Pzbu7VMZytemZuONu48cBwtnlw93C2+tn4mAsNz78RztZMHB3OvvmHz4Vym97wj/A8xwxsDmfv+dJN4WzvLbZNOZpfnhUPbz0qHK2cVxfOtvTpHM7OufvWcHbwLvulHPt8qX84W1XZqk36R/rxlruEs1e8enc4W1kRjqZ9164PZ6fX5n22NqxbfF2evzI+794d4/OdXRef7xYDxoazf58d38YXXl9aFc7eNzu+Hbhll+1DubXP+1t4nj3W6RLONjaFo2nMsIwVsViml8WzK1fEt1vL/zQtnO132Mhwds7s+JNdsSS+3SosvubmcLbfbnuEs2/96sCPncYRCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEC2qtZOuNaIHcMzOXH/inD2p9WDw9mRQ+LzLWx6Ys9w9tYpHcPZpd1rwtmJW3UJZ1/9+Vvh7O7fHJZyTFkUz2/UtyG1taoO8eyV2zaHsz+8Yf9wdklD3ucI90wZGs42NLSEs9Wb9Atnj9ysPpy9YoMTwtmN1svb9nSrji/T39tseCqH719aF86umPx4ONvjzvi+6VsXrUg5Lv5mfHv7jdPeDWe3/1r8Nf7Hua+Hs4f+pFM4+7tHu6UcHR55Jx5uim9/0i6xWNWU+Ot7zSm9w9klDfFtz85DxqQcT74bX7Y27bdOOPuFteP71N9uPzCc/epjs8LZE9fNe9+y/KT9wtkjbu+RPkmOWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyVbS0tLS0ZsKB634zPJOOJ+4VzlY/NSecPeIr/VKOq+6LZzv3qQlnG55dEM4O3m1AOFu7rDmcra9v1WL032p6aHY429K5Opx969cHhnJHPfhAeJ4Pv1wRzlbe8VY427he3vpQd3v8d+6x0abh7OKn/xnOdj/14HC27md/Dmd77LxtytF/5/h6PGnMqnD27I12C2cvfuGecDa+5Ulp3+H14exfZnTMmHNKOw9uCGdn1HUIZ4d3bQpnfze1Szh73iZ14ezds+Lb6UKP6vhScsfMzuHs5Z/aKZQb+fMHw/McNDi+bLwzszGc7ds/Pt/CHmPi6+IdL8ffMy1fnvf+I6pbt/i+vPbV2qx5Vz8wPZxt+MyYcHb62R+/j3DEAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALJVtXbCmuru4ZkMHhzvL/MGdg1nH5zbKeU4a1JdOPv9m1aGsxXr9Q1n33lueTibKivi0X55z3XzmD7h7MF7tnox/n/mwb+vCmcP3Ks6nP3TY13C2apRPVKOsT/YP5w9a5Nl4exFLx4Wzn5v06Xh7JFXrRXODtxtYMrRPf4yp4am+Hqc47Jbm8PZDjPir9PY8+LL9c8f7JhyTN80vu25+5QXw9ndLt0gnL3vW6+FsyffMTScPfPyhpSjckl9ONtSE99ep0/FYhVPzA3P8rKL4+97GlvC0dTQnLft+NEL8XVxvwnx17c543fOsXx1/L3tC/17Zs179N7rhrP3PLA6fZIcsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkq2rthB0+s3F4JqduXBfOnvV6p3D2rSmrUo6LfrcwnK2ZszycbeleE85WLFwRznY+cUI4W/eX6SnHTl8ZFs6u27M+tbWqt5eEsy8tWSucXeuzQ8PZadOaUo6u8VUxXfBk93B2zuz4uG/o0zmcrezfK5xdXtuccsy/Lr4+Tdt9eDj7H5uGo6l6cPy5bvzL8+Hsxc9/KpytmBVfjwt1G3aLz7tLx3C2vqkinl22KJwd0HlkONtl/fjyUVjx8tJwtrlf3rwj5kz+czh7zjMnhbPLM9729OnSEg+nlF58Or4vfqlHdTjb4d749rJ60ohwtqkx/nx16pS3j1i4oEM4u/L3d8VnfM6nP3YSRywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2apaO+HY7XuGZ7KyqS6cTQ1N4WhFS15vapzYP5yt3nFoONvy95nhbMNu64Wz4wZVhLNzDhqRcpy83tJwdkzPLqmtVSxcGc7OnNMczq49JP4afXv3VSnHswurw9lBA+Pr8d1VncPZ7QauCGenfGVcOPvWM/H5FmrmLA9nm//8VnzGX9ohHG2obwlnW5rqw9kzN14Wzp723ekpR+OeE8LZJe++Ec52qIwvm0tq48vHzLqJ4eyKlxtSjqoX3w1naw4ZndpadVXXcHbxsvi6NCdj/7J6cN57pi23rAlne3eM7yOe7T8ynD18vfi+/CeTO4azDb94NOWo+OJW4ezK+oXpk+SIBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACBbRUtLS0v+/wYAAGjPHLEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbFYA33ve99LFRUVacKECeUeCrS5Bx54oLT8f9Sfxx57rNzDg7J45pln0qRJk1KfPn1Sly5dSvuHSy+9tNzDgjZ3zDHH/Lf7iOLPO++8U+4htmtV5R4AHzRr1qx04YUXpq5du5Z7KFBWp556atp8880/8Njo0aPLNh4ol3vuuSfts88+aeONN07nnntu6tatW3rzzTdL+wtob44//vi06667fuCxlpaWdMIJJ6QRI0akoUOHlm1sKBZrnNNPPz1ttdVWqampKS1YsKDcw4Gy2W677dKBBx5Y7mFAWS1btiwdddRRae+990633HJLqqx0ogHt29Zbb136868efvjhtGLFinT44YeXbVz8F1uoNchDDz1U2nFccskl5R4KrBFqa2tTY2NjuYcBZfOHP/whzZs3r3SKbFEq6urqUnNzc7mHBWvcelKcBvW5z32u3ENp9xSLNURxhOKUU05JX/jCF9LEiRPLPRwou2OPPTb16NEjderUKe20007pqaeeKveQoM3dd999pfWgOG983LhxpdOgin+feOKJadWqVeUeHpTd6tWr00033ZS22Wab0qlQlJdTodYQv/zlL9P06dNLOxFoz2pqatJnP/vZtNdee6V+/fqll19+OV188cWlU6MeeeSR0nnm0F688cYbpaN2++67bzruuOPS97///dIFDi677LK0ZMmSdP3115d7iFBWd999d1q4cKHToNYQFS3FN14oq2KFGDt2bDrrrLPS17/+9dJjO+64Y+k7FlOmTCn38KDspk6dmjbYYIO0/fbbp7/97W/lHg60mVGjRqW33nqr9MXUyy+//P3Hi3//6le/Sq+//noaM2ZMWccI5VSc/lScRj5nzpzUt2/fcg+n3XMq1BrgnHPOKV1CsDgVCviw4mpQxSe2kydPLp02CO1F586dS38fdthhH3j8vXPJH3300bKMC9YEy5cvT3/5y1/S7rvvrlSsIRSLNeAw9xVXXFG6tObs2bPTtGnTSn+Kc2eL8waL/160aFG5hwllN2zYsNTQ0FD68iq0F0OGDCn9PXDgwA88PmDAgNLfixcvLsu4YE3w5z//2dWg1jCKRZkVX8grrvBRFIuRI0e+/+fxxx8vHeIu/vv8888v9zCh7IrTQYovchdfXoX2YtNNNy39/e83/So+iCr079+/LOOCNcF1111X2icUN49kzeDL22VW3D31T3/600eeHlVcavOnP/1p6RxbaC/efffdD71Zev7559Ntt92W9txzT9fxp105+OCD0w9+8IP0m9/8Ju28887vP37llVemqqqq0vfxoL3uK4oL3hSnCRZ3o2fNoFiUWXHVm/322+9Dj793L4uP+hn8b3bIIYeUzisvLh1YnO5RXBWqOF2w2HEUb7CgPSmugvb5z38+/fa3vy1dHWqHHXYoXRXq5ptvTmeeeeb7p0pBe3PjjTeW1gmnQa1ZXBVqDeWqULRXl156aenwdnElqOKuw8XRi1122SWdd955pS9xQ3tTfN/uwgsvTFdddVXpFKi11147nXTSSem0004r99CgbIq7bxenyBbrRIcOHco9HP4PxQIAAMjmZGUAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAABouztvjzr0D+GZrN46fmfQg7ZqCmf/csHMlKNpTO94dniPcLamc7zvde5cEc6uOywcTY8/uCIeTim1dK8JZ2vufTucnXr3caHcfzxzb3iery2L/64PPxVfH0atF59vYYt+9eHsbVOqw9kxw+LL9POPxpfLmoGdwtkhQ/Nu1nTwOnXh7Drd48vInsP2CmcPvP+hcPbOYy8PZ4ds/ZlwtuMeGRu9lNLK5xaFs9026RPOLl3SHM5WzlwWznYa3yuc3Wnd+HJZuOnYP4ezPXfYLpx969cHhnLb/Onh8Dzf/cEL4ezq+tpwdt6i51KOof23Cmdb1h0Qzla+uiCcbfxUfBuw9K74+4DuR0xKOSoXrQxnt9wv/t72hp12+NhpHLEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZKtq7YSVM5aGZ1K5do9w9uEZPcPZiqWrwtlSfsXqcLbmvmnhbOOGA8LZuneWh7MLDh0aztZMnp5yLJ0/NT7v8RNTW9tzrfpw9qjO8eVyxxfj69IlWy5JOU57vFc4+6u94tuPM/4R/53/+OX4OnzAr+Kfu5ywa1PKsWX/+LhfWdIhlcOh69SFs89uNymc7bH3WuFs7bLmlGPcXv3C2ZenxF/jrTaOv8aPL+8czjY1tYSzx42NLx+Fpy44MJzt3qMitbV+3eLPVd3JE8LZ2gfnhbPDt9w65Wh4fmE4u9fn4u/17vhrfN/0mxPir9Px8z4Vzu6xW3XK8fScLuHsi9Pjv3NrOGIBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyFbV2gmbh/cMz2SdHXuFs3/fc0A4e+eWtSnHur2awtkfvtg3nD1nw/i4D71vaDh7266N4ewXOo1POZ7+TkU4u+qtGamt3f1Ox3D2D8/Gs/MvujycPfB326YczWPjy+Uxf6yKz3dQfNk4YGqncLbylmfD2bPmjk459js8vr09esyKVA7ffqBrONvSrS6cXXbbzHB29eaDU46pP58Wzlb1jS+bj1z9SjjbaciwcLalKv5Z5MHP9Ug5mnsvC2e/eFR8+xP1yvkvhbPNI+PvmW64uFs4e+ELKcsxu8WX6Ttnxbfz5x8Zz148pXs4e/2lzeHsOc/UpByzp60OZ2vueSs+42O2/9hJHLEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZKtq7YT1L70ensmjx98ezh5w1Ynh7IK6HinHuwuaw9kV8xrC2fvPmx/ONm4a74r/uXbHcHZ2beeUY8E7z4azfTbdNrW1uSs7hLM37bcsnN338c+Gsy0b9085unePL1t9+lSEs6tWhaPp8+vVhbPfmz82nO2xSe+UY+sB8XH/c151OLtR33A0PXFEUzh7wOBB4Wyvmvh2+tyN4uti4YRRo8LZ5viw06rdhoSzYwa1hLNT58XX473H1qccTy+M75+2GrA0tbWKI9cNZ3ddP75wHHNnfN/0273z1ofbZ3QKZ3+0+Ypw9th/dA9nb9wpvq09ZHKvcPaePQakHE9vFn9P/tPN4tut1nDEAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJCtqrUTdhw3OjyTzY/aNZy9aPPF4ezh9/ZKOb617Ypw9pKnuoazf7hlYDi7+xWdw9lT1l8dzt4ypTnl6LPB1uHsihdfSm1tVPfGcPaEh+LLZcXymeHskCEdUo65c5vC2WXL4vPt3Tv++cdj8zvGZ7w6vv4PHpD3mc2zC6vD2a9PjI87x10zV4WztSviv++o7vHlcu/fdkk5arrF5904Y3k4W/nuynD26ZXx7XzT0O7h7C0re6QcXTJeqsFd8vZPES3XvBLO3juhXzi7/eHx7LeezHvPtMuQ+HL5q1drwtnN+9WHs4dMjr9XO2Xd+I7tgPvzlsmZC3qGs4uvnRqf8Q47fuwkjlgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABAtqpWT9mhIjyTORdNCWfrtxsaznbrGh9z4TuHvBjOvrv4pXB2j6mHhbMV1S3h7IzljeHs9qPj2cJdnVq/KP67bl0GZ807NM+M5/no8XXh7I9enR/Ozrot/hwXtjliQDh7wIgV4ew5k7uGsxUV8depw9tLwtmldf1TjkXdOoSzUxbHsztmrEoDOjeHsyvq4tk3aqvD2e/sE18XS/nb49mtdu0ezr44LZ6tztgMLFnYFM4esUl9fMYppclzO4ezfTuOSW2t/uDx4WzlvPhyuXx1/PPiLhn7tcKVD9eEs6NGx7dbVRlv9S7cNL6d//Xr8X3TGRNqU44XF8dX5AfOWCd9khyxAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGSrau2ElTOWpnK48Pke4ewzp1yfNe+unQeHs4OHbRXONr+2KJxtHNsnnF2wqiKcveN3S1KOppemhbMVFW3fjw8c2RDO1q5uCWd/8bVNwtkvbrwi5dht6LJw9ukF1eHs9ZPi831gTk04+9B+o8LZY9erSzk26LM6nO3bMb58lcvSpfExL30tvnz8uUP3lKOiLr5OzV8ZXzZXzIovX2O26BrO1v1leji742HxfVPhyn/Gn687Z8T3L/usvW4oVzl7eXiencbE3/c8d+nb4Wz1viNTjsqpi8PZiZvFf+fbr854z7RJp3D2nltrw9mHN+iVcgwcGH/fU7f8k91HOGIBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyFbV2glb+nQOz2T1JoPC2SdeD0dT7y23zWtdry4IZxt2HhHOVr0Sn++wjbqGs9dObQxnW3qlLNVjhoezS55/PLW1xfXx7F+mdwpntxwRf43W6d6Ucry8uNWbiw+pXV0Rzi6sj2eXZcz3c5vGX+R5K/M+s2lqjo97TM+xqRyueK1bONu3X/z5quzfPZy9doe6lGOHuV3C2b/sFp/3bnXx+f548yXh7D7PDw5nPzUovo0vNC98O5y96Pn4MrLP2rFczaj4PBuvfy2crZ8/J5xtumx+ylHVKf7+4445y8PZDm8vDWcPuXx0OFt7+23hbNOgA1KOrl1qwtm5M1anT5IjFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsFS0tLS35/xsAAKA9c8QCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWa4A33ngjHXrooWmttdZKXbp0SePHj0/nn39+WrFiRbmHBmXx9NNPpz322CP16NEjde/ePX36059Ozz33XLmHBZ+o5cuXp/POO6+07Pfp0ydVVFSkq6+++iOnfeWVV0rTdevWrTTtkUcemd599902HzOsCevEE088kb785S+nTTfdNFVXV5emozwUizKbOXNm2mKLLdJjjz2WTj755HTJJZekrbfeurQiHXbYYeUeHrS5Z555Jm277bbprbfeKq0H3/72t0vle4cddkivvfZauYcHn5gFCxaUPlQqSsOGG2743043a9astP3226epU6emCy+8MJ1++unpzjvvTLvttltqaGho0zHDJ6m168Rdd92VrrzyylKhWGedddp0jHxQ1b/9mzb2u9/9Li1ZsiQ9/PDDaf311y899qUvfSk1Nzena6+9Ni1evDj17t273MOENnPuueemzp07p0cffTT17du39NgRRxyRxo4dm84666x06623lnuI8IkYPHhwmjNnTho0aFB66qmn0uabb/6R0xVloq6urnRkb/jw4aXHig+oimJRfJpb7EOgPa0TJ554YvrmN79Z2ncUH9K+/vrrbT5W/osjFmW2bNmy0t8DBw780MpUWVmZampqyjQyKI9//OMfadddd32/VLy3PhRHLO64447SoXH436hjx46lN1AfpyjXn/nMZ94vFYVinSnK90033fQJjxLWvHWieA9VlArKT7Eosx133LH093HHHVc6h7w4NerGG29Ml19+eTr11FNT165dyz1EaFP19fUfuYMovn9UnOYxZcqUsowL1gTvvPNOmj9/ftpss80+9LPiqMWzzz5blnEBFJwKVWbFF5IuuOCC0qHt22677f3Hzz777PTd7363rGODchg3blzpO0dNTU2pQ4cOpceKQvH444+//8YK2qvitJD3juL9u+KxRYsWlcp58UkvQFtzxGINMGLEiNIX8a644orSIe7Pf/7zpaLxs5/9rNxDgzZXXNmjOD+2OIr38ssvl45QHHXUUe+/oVq5cmW5hwhl897y/1HFoVOnTh+YBqCtOWJRZjfccEPpi3bFG6nicrOFAw44oPTl7eKLSMWVof71XHP43+6EE04onRJ40UUXpWuuuab0WHHaxze+8Y30ve99r3R5TWiv3jtNsDgq8e9WrVr1gWkA2pojFmX2i1/8Im288cbvl4r3TJo0qXQfC+fL0h4VBWLevHmlL3K/8MIL6cknnyyV7ULxBVVor947Beq9I3j/qnisuNa/06CAcnHEosyKN08fdTnZ1atXl/5ubGwsw6ig/Ir1orifxXvuu+++UgEvbiAJ7dXQoUNT//79S5fe/KibhG200UZlGRdAwRGLMis+fS2OSvz7NZevv/760uVmN9hgg7KNDdYUxZXSiqMWp512Wmm9gPbss5/9bOnSy8Upg++5//77S/uRgw46qKxjA9o3RyzK7Iwzzkh//etf03bbbVe6qUvxfYpih1E89oUvfCENGTKk3EOENvXQQw+V7rT66U9/urQ+FFeIuuqqq0pXUPvKV75S7uHBJ6q4aEdx09TZs2eX/n377beX7rRdOOWUU1LPnj1LN4q8+eab00477VRaJ4p7uxTfSZo4cWI69thjy/wbQNuvE9OnTy/dcLjw3tG8966sufbaa6cjjzyybONvbypaWlpayj2I9q44fP2d73yndORi4cKFaeTIkenoo48ufVm1qkr3o3158803S1eGeuaZZ1Jtbe3768PXvvY1N4ykXVwlsHiT9FHefvvt0s8LL730UmmdePjhh0vrxd57751+/OMff+hmq9Ae1okHHnigVLQ/SnFz1eLntA3FAgAAyOZkZQAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMjW6ruvjdn51+GZLJn9ajjba/C4cLa5b5eUo/61qeFsx7Gjwtnal14IZ3sMHx+f78zXw9luG2yQcix46sFwtv+IzcPZqfd/MZSbs+K28Dx71PQOZx+eWxvOVlfm3bJm3soO4Wzt6opwdpuBDeHsswuqw9nOVfHna0lD3mc2uw+tD2fnr4rPe9N+nwlnc/YRQ7++bjh7/U6LwtnbZ3RMOa58Ib6PmTuzMZz98q7x7JXnzQlnr/ll33D2wJ3/mXKs++O9wtlPDVgVzl6w6a6h3Nrfuzc8z+8e3BTO3jmrczh79obx/Uvh60/0Cmdv3Tm+zTvkgfiNU+/6dPyGkpvfOC+cPXPL5SnHV6+L3zx54JhO4ewTB2/3sdM4YgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIVtXaCZv7dQ7PZPnU2eHsgKP3DWfrr38j5ajZakI8XNcQjnbbcrP4fBesCEc77799ONtY3SHlGDA3/juvendeamuvLY3/vpv3awlnT/hJYzh71Rl5r9HN07qEs18cuzycvWRK93D2oJHx9eHMf8Tnu8uo+Ppf2H1ofTg7vFtzKodZU/8ezt657aBwtnt1/HUa3SO+fBRu2nNJOHvU5N7h7GdHxNenX46Jz3f93hXhbOctN0w5tuy3Kpwd1rUptbW+63QKZ19fFl//X5wef41+VtM15ajNWJ0aW+Kv74a94/vU2oaZ4expm64OZ19c1Oq33x+pcm58G3DwPvHnqzUcsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkq2hpaWlpzYRjPvXL8Ewadh8Zzk7/djz73MLXU47RPbqGs997rimc3X/tVeHsWc/0Cmfv2LUqnD350fiYCw+e/Vo429IpPu6p9xwXyv32tbvD87z8uS7h7JQzfhHODtlsz5SjYvnqcLZ5aLdwtsP0ZeFs/d6jw9lFl10Tzvbfa1LKccqJ8edr/d6N4ezua+0Vzr617PZwtk+n6nC2V8064Wxtw8yUo6oyvi4vqn83nO1SFf9McGlDq3b5H2lwl4Hh7CtL5qcco7p3DmeXra4LZ4d2ja3LJ/5zcnieT78T36ftMrI+nH1yYaeUY0z3+D7i/pfiy3T/AR3C2VlP14az4z7VPZydM6855ejXP/58LV4c3wY8d/h2HzuNIxYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbFWtnbBpQr/wTJb/7o5wdvSAA8PZhrquKUfFytXhbIe+HcPZmy94J5xtHrAynB3/cPz5GrBB95SlKt5xK5bWp7b2ytJWrzofctlOS8PZQ75wTDj7/SOaU46/zeocznbs0BLOjuzeO5zdZ/jycPboYceFsz16VKQcG/RZFs6+VdshlcNuX44/1188I75/OXX9+eHsA3MaUo4R3ePb299PjW9vPzsiPt8j/twjnH3qmPi268sP9Uo5frxNfN7da+Lr49Dgy3TGxPj6sGRcfLxH3xl/fW/ad3HKsc8x8fwrf5wYzm73p/hz/fJ53cLZ7W+J79f+euCSlOPIB/uEsz/coTZ9khyxAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGSrau2Eqx5+LjyT7gfsHs7+8eDacPaQP/dIOa49qi6c/dK9ncPZE380KJy98KrV4ewfT2sOZ4+5qyXlaNxwQDhb9dg7qa0dus7KcPbsp3uFsy0dV4WzV0/tknK89su3w9lOk0aEs3dPnh/OvnBsfF1a/LOXw9mlR66bcpw8q1s4e/7O8e1Wjkd/3TWcnVVXH86+uSyeXba61bvAj9SrJr7N3HFwfNyVFeFoOnvH+LZrRl18/3LVTvFtV+Ef82rC2W0HNqS2duTfe4ezc2+YEc4OPCS+f9lkgz+kHIN32S+cXWfD28PZ6q3WC2eHfWtyONv/G4eEs5/6VHz/Uuj45a3D2eMfic932nc/fhpHLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACBbVWsn7LTtRuGZVExdHM6uaOwdzg4bktebDvrPinC2cv7ccPbi5QPC2dRUH45e80a3cLZHz7znuraxOZxtHt4ztbXbpncKZ08YVxvOfv03C8PZl5b3TTk+dera4WzXqqZw9plOQ8LZ/p1Wh7MtveOvcf1b8de4MGGnHuHsgM7xdSnHFt9r9e7kQ/55ZmM427djfHvZ0Lwg5bh2audw9s+vxZev3+++JJz9zye7hrOTD1wazh72QK+U44ebxX/nJQ3xfXnUcevWhbPf7VYdzjY3t4Szp//puJTj6mMfC2evuGuzcPZbD8Tfu9z8/T3D2ZMfir/vOfXW+Hvqwg8fi897s13j24DWcMQCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkK2qtROu+MfT4Zm0tDSHs5+/c+9wdsUbS1OWllQWq5Y0hrPVc+rC2Yl9Ooazf7ltZcpR/fjscLalKf58RX15vRXh7O0z4s9z09g+4ezQTbqnHDsPji9bt83oEs7+cpf4enz6473C2aah8eerZUi3lGO/tePLV3VFKouW6g7h7KPz44M+YETPcLa5ZUHKcc0TNeHsZzeuD2cvnhJfvuqWx/fHl74U33a98lTePqLb1vEd8pTF1eHsVgNiuQsndwrPs3GDeHbhgvjre9Oi+PJcqPziduFsr5rl4eyyZfFloyXjfV5zUzx887T4PrGw/KrXwtmnDh4Tn/GOHz+JIxYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbFWtnbDLVhuHZzLnvj+Fs10fmBPOVs1alnJUrGwMZ1u6VIezHd9aEs6mxuZw9Ptnrg5nK/t2STmWLpsWznbfYMPU1lY0VoSzXxg3MpytPPLtcHarAYtTjleXtnpz8SEnjq8NZ7tVt4SzJ6y7PJx9rG+vcLZHTXxdKmzeL55fujq+bOao+NNz4ez3x20Tzu41bFE4e9o/469x4c8HxdepvS6Mz/fqr8U/E/z7JfExn3bsgHD296e8lXL8aMdx4eybtfH98eGjY7mm15eG57nvQd3C2Xkr49vpEzK204Wv3Ns9nK3pEN/OD12rQzjbv3P8PdOctxvC2S3WyttOP7t+v3C2W9dP9piCIxYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbBUtLS0t+f8bAACgPXPEAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFoo0tX748nXfeeWmPPfZIffr0SRUVFenqq6/+wDTNzc2lxyZNmpSGDRuWunbtmiZMmJC++93vplWrVpVt7FCO9aHw61//Ou2www5p4MCBqWPHjmnkyJHp2GOPTdOmTSvLuKGc68O/Wr16dVpvvfVK01588cVtNlZYk9aJY445pvSzf/8zfvz4soy7Pasq9wDamwULFqTzzz8/DR8+PG244YbpgQce+NA0K1asKL1p2mqrrdIJJ5yQBgwYkB599NHSynX//fenv//976UVBtrD+lB49tlnS2WiKNu9e/dOb7/9dqls3HHHHen5559PQ4YMafOxQ7nWh3912WWXpRkzZrTJ+GBNXieKD52uvPLKDzzWs2fPNhgl/0qxaGODBw9Oc+bMSYMGDUpPPfVU2nzzzT80TU1NTfrnP/+Zttlmm/cf++IXv5hGjBjxfrnYdddd23jkUJ71ofCLX/ziQ4/tt99+abPNNkvXXntt+ta3vtUGo4U1Y314z/z580tvur75zW+mb3/72202TlgT14mqqqp0xBFHtOn4+DCnQrWxolEXK8j/TVEs/rVUvGf//fcv/f3KK698YuODNW19+O8URbuwZMmS/8ejgv8Z60NRqMeNG+fNFP9r/f9dJ5qamtKyZcs+0THxf6dY/A8yd+7c0t/9+vUr91CgLBYuXFj6lLb45Ko4XbCwyy67lHtY0OaeeOKJdM0116RLLrnEqbHwf04j79GjR+n0p+L7GCeddFLpOxq0LadC/Q/yox/9qLTS7LnnnuUeCpTF0KFDU319fem/+/btmy699NK02267lXtY0KZaWlrSKaeckg455JC09dZbu4gB7V5xytQ3vvGNtMkmm5QugPO3v/2tdApt8R284nsZxWlStA3P9P8QF154YbrvvvtKK0qvXr3KPRwoi7/+9a+lK6MVpwP+/ve/T3V1deUeErS54qo4L774YrrlllvKPRRYI3z/+9//wL8PPfTQNHbs2HT22WeX1pPi37QNp0L9D3DjjTemc845Jx133HHpxBNPLPdwoGx22mmn0hG7r33ta+nmm29O//Ef/5F+9rOflXtY0GaK88fPPPPMdMYZZ5QuRw58tK9+9aupsrKy9KEsbUexWMPde++96aijjkp77713+uUvf1nu4cAaY9SoUWnjjTdO1113XbmHAm2muFdFQ0ND6TSo4hSo4s+sWbNKP1u8eHHp38XPob3r3Llz6ZTZRYsWlXso7YpisQZ7/PHHS1eCKi6pedNNNzlHEP7NypUr09KlS8s9DGgzxT0rigKx/vrrl+7tUvzZbrvt3j9ltvj3yy+/XO5hQtnV1taW7oPRv3//cg+lXfFOdQ1VnENeHKUoLqlZ3ASsaN7QHjU2NpZ2EMWN8f79qjjFeeaf+9znyjY2aGunnnpq6R4u/6q4Utrxxx9fuvvwvvvuWyoX0F4U37sr7kDfvXv3Dzx+wQUXlC50UNy1m7ajWJRBcU54ce392bNnl/59++23v38ou7jSR3FO4O677176VKo4j/bOO+/80CkgxZVAoD2sD8WOoTiXvDj1o/iUtmvXrqVCcdVVV5UuK3juueeW+TeAtlsfiqveFH/+1XtXhSrWj38vHfC/fZ0o3isVp8Uedthhafz48aXH77777nTXXXeVSkVRtmk7FS3FXps2VRyFmD59+kf+7O233y79/X/7xOnoo48uXRUE2sP6MGTIkNJlBCdPnlx6A1Wc/lQ8Vtx9vriowXs3yoP2sD581PJerBfFPuOiiy5Kp59+ehuMEtacdaK4UmZRMB577LFS+Shukjd69Oh0+OGHl9aH6urqNh9ze6ZYAAAA2Xx5GwAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAANruztvXTf1beCZPvFsTzt71tVfD2T5fm5hyzH1lRTg7ZELXcHb+df91k7yIY84ZFM5e/eMF4ezXzuqTckzs0xjOnja5Rzj7zGHbhXLr/vah8DxXzKkPZ3sO6xjOLn0zvjwXPr1bfN5vLI3foKiqQziaamvjt+nZZEh8mXx4asagU0orV8bH3fTSonD27cv2D2dfW3J7OFu7uiKcXbdXfP3/+SurUo7Pj43nn17Q6t3vh+wyZK1w9pH5M8PZTfp2CWfrGmtTjh883y2cPXHdunB2bM99Qrneo08Mz7NPj7HhbPOOo8LZioUrw9lSvja+b6tYvjo+4y4ZN8BbFd/ON++b8VxPjq+HhcpZ8fVp1qx/hLMrZ/zhY6dxxAIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQraq1E/aobgnP5LBRK8LZe744IZzdZtjqlONPzzaGsxMGxOd9/4Au4ezAzs3h7K4nDgpnPzeqLuXo2KFXOLvvuJWprQ0YEO/kMx9fGs6eeWi3cPasJ/Kep19vu044u95Vc8PZ/Sc2hLN/nF0Tzvbp2BTO1q3I/MzmuXfD0ZaJ/VM5XPpy13D2oJHxZXNR/ZJw9qqn4tudwp5rxZfNL/2mQzh7wO4Lwtk7X4hvQ3ZYL75/2XZAfF0s3P96q9+ufEifjp3C2XM3juWqq+LPc2WH+HPV2D/+/qFqQfy9WklN/DVqHtoxnK18N779aOkZn2+OprF9svKVc+PvuTpU5q2LH8cRCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEC2qtZO+JUfrAjPpKVTq2fzIXNvvTycfehHX045qh98PZz9+4ye4eyiRx8MZy/stnc429KnQzh7zy1NKceBh9WHszf+I758nbdJLDf7nfjv2zSgazh79g+Wx+c7sX/KMXqvf4azg44fE87+5IDfhLP9Tz8hnL3hB3PD2eZRvVKOqrl14WzfnQemcvjSuPg+4revdwlntxsU33asfKM25Tj1sYzXuU98GzJnZXy29fGnK728oDqcrWvM+xzzuE1WhbO7r5XxSwd122HLcLZxg/i2+tFjl4az3ar6phxLG+IL5hvL4u8/Vjd3D2cfmx9fps/bZHA4e9lL01KOnywdEM4O7rBN+iQ5YgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIVtXaCXvvMCA8k5H9W8LZ55/eMZxdPHleylExtFs8vGJ1ONp7373C2aa1uoezu28af512G1KTchy8zrBw9tbfvJDaWv3y5nC2olOrV7sPOeGUHuHsz+6Nj7nw5R8ODWf/+FZ8vqMvODGcra+PL9NbnBxfJv/xcH3K0Ti2Tzg7tG/8d87x29e7hLM9auLL5k1vdw1n5113ecqx2a7xZfPZS64PZ5/+xmHhbFq0KhydtbQinO28WXz5KDy+oGM4u9OQhtTWKhasDGcr75kWzj62d/y92uQ5HVKOYV07hbOPzI9nn3k5vs2rnLE0nF3eODOcfW1p55Sj8p254eyKd2elT5IjFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsVa2dcMltM8MzeWKroeHsyvmvh7OjT9s/5Zh3+zvhbNOo3uFshzcWh7PjP90nnN1zrdpw9tH5NSnHhN6vhbN9dx6Y2lrl/Lp4duHKcPbXX4kvG70/Pz7l+Pl3ZoWzg44cGc5O/49rw9neXz0ynH3skunhbMsOw1KO6gdmhLOVe/RM5bDP8FXh7Ffu7RbOtrSEo2nQgYfFw8W+7enV4WzPz382nK1f0RzOdhrcKZxteDO+j1iwsHM4W8pnfA5av35qc40T+oez47aOrw8b9YnvI558tzrl+Myw+nC2Y8bH3CeObwxnz/ln/L3amRsuCWcveLYi5Zh1UHyfWnFDxkazFRyxAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGyKBQAAkE2xAAAAsikWAABANsUCAADIplgAAADZFAsAACCbYgEAAGRTLAAAgGxVrZ2wcVzf8Ey2WL8inJ3c3BjOzrt3XsrSqdVPz4dM3KJzOPvKKwvD2eUrw9H0rXu6hLO9euV11DMm1oWznxmzKrW1lrW6h7OVry0KZyeeMz6cferp1SnHPl8fGs4+Mq0lnP3KzYeGs3e+Hl8ut/7GWuHsvVPy1oemgV3D2SUN5fm8aPKcmnD24l2Wh7O3Totvtx748+KU49OfjW8HHn2zOpxdMj++XyyX9YY2Z+UHd47/ztOXdwhnN+8fy1U/Pjs8z6kz4uv/gk3i2UGd816jvf/QIx5+Jb5f7LFl/P3p4jkN4ezj8+PL1dPz49vLwuJH3g1nK2YuSJ8kRywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2apaP2W8g6xqqghnux2yRzh79P6t//U+yl9e6xTOjumxOpx9c5eh4ewvtl8Szn73uR7h7Anja1OOX7/WOZy96pGacPa8TWK5lpbwLFPjen3D2ddmxuebKuPrYeGVpfHnuUd80UrX3tEUzg6eEJ/v7Q/HX+T+w/M+s1kwrk84u3RpxsKZoTljtr97s2s4e8DaK8LZN48flXJMGrYsnH1yRvdw9of7N4Sz37orvq3dcvf4irzf8LqU4+qp3cLZZavj6+OBI2O55vmLw/Os7BDfVh98UZf4fBfG3z8UKp6L76Aq+/UKZ1c8PjucrR4WXw+f331wOPvOjTk785SqZiwNZ5etnJ8+SY5YAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALJVtXbCrbepCc+kd01jOPvyjGXh7P2zhqYcix9bEM5WrtM9nF0xe1U4+/zCVr+kH/LkQyvC2W9v3JxyHLZO/Hf+/TMdU1vr0qUinG26f3o4233TieFs/WUvphwrztoknH33xvjvfNI5A8LZnxx6Vzj7tRt2Cmd/92xLylGxKr7NXPLH+HOdDt0uHJ1eVx3Obj8wvv4fOHJUODt1WcZzlVLadlB8e7vbqIZwdp/h8W3eOzvUh7MnrBt/jZ9fmLePGNCpKZz9/Jj4vi1qWd2scLb3gs7hbNVri8LZlk4dUo6KQX3i4Yb465tq4uOuWBpfHy7/Y3zM1Yvj27ySjNVpWd2M9ElyxAIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQTbEAAACyKRYAAEA2xQIAAMimWAAAANkUCwAAIJtiAQAAZFMsAACAbIoFAACQraKlpaUl/38DAAC0Z45YAAAA2RQLAAAgm2IBAABkUywAAIBsigUAAJBNsQAAALIpFgAAQDbFAgAAyKZYAAAAKdf/B7f/T2NbACn3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for dim_ in range(n_layers):\n",
    "    images = feats[dim_].squeeze().detach().cpu()[:16]\n",
    "    rows = cols = 4\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i], cmap=\"YlGnBu\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(str(i))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    image = image_left\n",
    "    image_resized = resize_transform(image)\n",
    "    image_resized = TF.normalize(image_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    image_resized = image_resized.unsqueeze(0).cuda()\n",
    "\n",
    "    feats = model.get_intermediate_layers(image_resized, n=range(n_layers), reshape=True, norm=True)\n",
    "\n",
    "\n",
    "for dim_ in range(7):\n",
    "    dim_ *= 4\n",
    "    # print(dim_)\n",
    "    # continue\n",
    "    if dim_ == len(feats):\n",
    "        dim_ -= 1\n",
    "    # print(dim_)\n",
    "    images = feats[dim_].squeeze().detach().cpu()[:16]\n",
    "    rows = cols = 4\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i], cmap=\"YlGnBu\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 129, 200).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dinov2_eeg_rt(nn.Module):\n",
    "    def __init__(self, c_dim=129, t_dim=200 , freeze_encoder = True ,model = None  , n_layers = 24) :\n",
    "        super().__init__()\n",
    "        self.dino_model = model\n",
    "        if freeze_encoder:\n",
    "            for param in self.dino_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.dino_model.eval()\n",
    "        else: \n",
    "            for param in self.dino_model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.fc  = nn.Linear(1024*12*8, 1, bias=False)\n",
    "        nn.init.kaiming_normal(self.fc.weight, mode=\"fan_in\")\n",
    "        \n",
    "    def forward(self ,x):\n",
    "            with torch.inference_mode():\n",
    "\n",
    "                x=x.unsqueeze(1).repeat(1,3,1,1)\n",
    "\n",
    "                feats = self.dino_model.get_intermediate_layers(x, n=range(4), reshape=True, norm=True)[3]\n",
    "                premuted_x = feats.permute(0 , 2 , 1 ,3).reshape(feats.shape[0] , -1)\n",
    "            \n",
    "            return self.fc(premuted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_10432\\415625701.py:14: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  nn.init.kaiming_normal(self.fc.weight, mode=\"fan_in\")\n"
     ]
    }
   ],
   "source": [
    "n_layers = 24\n",
    "model.compile()\n",
    "model_ = Dinov2_eeg_rt(freeze_encoder=True , model=model , n_layers=n_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        #{\"params\": model_.dino_model_.parameters(), \"lr\": 1e-5 , \"weight_decay\":0},\n",
    "        {\"params\": model_.fc.parameters(), \"lr\": 1e-4 , \"weight_decay\":1e-3},\n",
    "    ])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, factor=0.5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m warmup = step_count/\u001b[32m20\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbefore training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m test_nrmse = \u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ccd_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest nrmse : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_nrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m best_rnmse = \u001b[32m100000\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m     19\u001b[39m y_pred = model(x).view_as(y)\n\u001b[32m     20\u001b[39m diff = y_pred - y\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m se_sum += \u001b[43mdiff\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m sum_y  += y.sum().item()\n\u001b[32m     24\u001b[39m sum_y2 += y.pow(\u001b[32m2\u001b[39m).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_f = nn.MSELoss()\n",
    "\n",
    "epochs = 8 \n",
    "step_count = len(train_ccd_dataloader) * epochs\n",
    "warmup = step_count/20\n",
    "\n",
    "print(\"before training\")\n",
    "\n",
    "\n",
    "test_nrmse = nrmse_over_data(model_ , test_ccd_dataloader ,device)\n",
    "print(f\"test nrmse : {test_nrmse}\")\n",
    "\n",
    "\n",
    "best_rnmse = 100000\n",
    "best_model__path = r\"best_model_.pt\"\n",
    "best_rnmse_path = r\"best_rnmse.pt\"\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = torch.load(best_rnmse_path)\n",
    "    print(f\"best nrmse :{best_rnmse}\")\n",
    "\n",
    "total_steps = 480\n",
    "for epoch in range(4):\n",
    "    step = 0\n",
    "    cumulative_loss = 0\n",
    "    pb = tqdm(train_ccd_dataloader)\n",
    "    with torch.autocast(device_type=\"cuda\", enabled=False):\n",
    "        for batch in pb:\n",
    "            step += 1\n",
    "            if step > total_steps // 4:\n",
    "                break\n",
    "            x , y = batch\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model_(x).view_as(y)\n",
    "            loss= torch.sqrt(loss_f(y_hat, y))\n",
    "\n",
    "            # ⭐️ ⭐️ Scale Gradients\n",
    "            loss.backward()\n",
    "            # ⭐️ ⭐️ Update Optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            cumulative_loss += loss.item()\n",
    "            pb.set_postfix(loss=loss.item())\n",
    "        \n",
    "\n",
    "    print(f\"epoch {epoch} loss : {cumulative_loss/(total_steps//4.0):.8f}\")\n",
    "    cumulative_loss=0\n",
    "    nrmse_over_test = nrmse_over_data(model_ , test_ccd_dataloader ,device)\n",
    "    print(f\"test nrmse : {nrmse_over_test}\")\n",
    "    if nrmse_over_test < best_rnmse:\n",
    "        best_rnmse = nrmse_over_test\n",
    "        print(f\"new best nrmse : {best_rnmse}\")\n",
    "        torch.save(model_.state_dict(), best_model__path)\n",
    "        torch.save(best_rnmse, best_rnmse_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:33<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nrmse : 3.3372093055994547\n",
      "best nrmse : 0.9269812395697398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 120/132 [11:21<01:08,  5.68s/it, loss=0.374] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 mean KL loss : 34.27652197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:38<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nrmse : 1.841243541231882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 120/132 [11:25<01:08,  5.71s/it, loss=0.288] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 mean KL loss : 0.48066439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:36<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nrmse : 1.7883517445967099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/132 [00:17<18:26,  8.51s/it, loss=0.0823]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     loss.backward()\n\u001b[32m     56\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     cumulative_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     pb.set_postfix(loss=loss.item())\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mean KL loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcumulative_loss/(total_steps//\u001b[32m4.0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 8\n",
    "step_count = len(train_ccd_dataloader) * epochs\n",
    "warmup = step_count / 20\n",
    "\n",
    "print(\"before training\")\n",
    "\n",
    "test_nrmse = nrmse_over_data(model_, test_ccd_dataloader, device)\n",
    "print(f\"test nrmse : {test_nrmse}\")\n",
    "\n",
    "best_rnmse = 100000\n",
    "best_model__path = r\"best_model_.pt\"\n",
    "best_rnmse_path = r\"best_rnmse.pt\"\n",
    "\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = torch.load(best_rnmse_path)\n",
    "    print(f\"best nrmse : {best_rnmse}\")\n",
    "\n",
    "total_steps = 480\n",
    "\n",
    "for epoch in range(4):\n",
    "    step = 0\n",
    "    cumulative_loss = 0.0\n",
    "    pb = tqdm(train_ccd_dataloader)\n",
    "\n",
    "    for batch in pb:\n",
    "        step += 1\n",
    "        if step > total_steps // 4:\n",
    "            break\n",
    "\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model_(x).view_as(y)\n",
    "\n",
    "        # --- Compute KL divergence between Gaussian(y) and Gaussian(y_hat) ---\n",
    "        mean_y = y.mean()\n",
    "        std_y = y.std() + 1e-8\n",
    "        mean_y_hat = y_hat.mean()\n",
    "        std_y_hat = y_hat.std() + 1e-8\n",
    "\n",
    "        kl_loss = (\n",
    "            torch.log(std_y_hat / std_y)\n",
    "            + (std_y**2 + (mean_y - mean_y_hat)**2) / (2 * std_y_hat**2)\n",
    "            - 0.5\n",
    "        )\n",
    "\n",
    "        loss = kl_loss  # pure KL divergence optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulative_loss += loss.item()\n",
    "        pb.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"epoch {epoch} mean KL loss : {cumulative_loss/(total_steps//4.0):.8f}\")\n",
    "\n",
    "    nrmse_over_test = nrmse_over_data(model_, test_ccd_dataloader, device)\n",
    "    print(f\"test nrmse : {nrmse_over_test}\")\n",
    "\n",
    "    if nrmse_over_test < best_rnmse:\n",
    "        best_rnmse = nrmse_over_test\n",
    "        print(f\"new best nrmse : {best_rnmse}\")\n",
    "        torch.save(model_.state_dict(), best_model__path)\n",
    "        torch.save(best_rnmse, best_rnmse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.3916, device='cuda:0', grad_fn=<MinBackward1>),\n",
       " tensor(1.7225, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(1.5916, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(0.2380, device='cuda:0', grad_fn=<StdBackward0>),\n",
       " tensor(0.1400, device='cuda:0'),\n",
       " tensor(2.4000, device='cuda:0'),\n",
       " tensor(1.6772, device='cuda:0'),\n",
       " tensor(0.3638, device='cuda:0'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.min(), y_hat.max(), y_hat.mean(), y_hat.std(), y.min(), y.max(), y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f64d81b8c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(model.dino_model.parameters())).weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1051 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_nrmse = \u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ccd_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnrmse over train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_nrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m     16\u001b[39m x = x.to(device, dtype=torch.float32)\n\u001b[32m     17\u001b[39m y = y.to(device, dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.view_as(y)\n\u001b[32m     20\u001b[39m diff = y_pred - y\n\u001b[32m     22\u001b[39m se_sum += diff.pow(\u001b[32m2\u001b[39m).sum().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mDinov2_eeg_rt.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m ,x):\n\u001b[32m     17\u001b[39m         x=x.unsqueeze(\u001b[32m1\u001b[39m).repeat(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         feats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdino_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m         premuted_x = feats.permute(\u001b[32m0\u001b[39m , \u001b[32m2\u001b[39m , \u001b[32m1\u001b[39m ,\u001b[32m3\u001b[39m).reshape(feats.shape[\u001b[32m0\u001b[39m] , -\u001b[32m1\u001b[39m)\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(premuted_x)\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train_nrmse = nrmse_over_data(model_ , train_ccd_dataloader ,device)\n",
    "print(f\"nrmse over train {train_nrmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss\u001b[49m.item())\n",
      "\u001b[31mNameError\u001b[39m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "final_r5_test =pairs_to_fast_loading_shards([] ,\"final_r5_test\" , shard_size)\n",
    "r5_test_ccd_data = EEGDataset(final_r5_test ,  shard_size)\n",
    "\n",
    "r5_test_ccd_dataloader = DataLoader(r5_test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challange 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preproccessing and sharding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_participants_p_factor_files(data_dir)->dict[str , str]: \n",
    "    result = {}\n",
    "    for release in os.listdir(data_dir):\n",
    "        release_dir = os.path.join(data_dir, release)\n",
    "        participants_description_path = os.path.join(release_dir, \"participants.tsv\")\n",
    "        result[release_dir] = participants_description_path\n",
    "    return result\n",
    "\n",
    "def valid_participant(participant_dir_path)->bool:\n",
    "    participant_dir_path = os.path.join(participant_dir_path , \"eeg\")\n",
    "    required_rec_lenght = 15*60\n",
    "    cumulative_sec = 0\n",
    "    for file in os.listdir(participant_dir_path):\n",
    "        if file.endswith(\".bdf\"):\n",
    "            eeg = mne.io.read_raw_bdf(os.path.join(participant_dir_path , file) , preload=False, verbose=False)\n",
    "            if int(eeg.info[\"nchan\"])!=129:\n",
    "                continue\n",
    "            n_samples = eeg.n_times\n",
    "            sfreq = eeg.info[\"sfreq\"]   # sampling frequency in Hz\n",
    "            length_seconds = n_samples / sfreq\n",
    "            cumulative_sec += length_seconds\n",
    "            if cumulative_sec >= required_rec_lenght:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_valid_subjects_scores(data_dic : dict[str , str],history_path  = r\"history\\challange2_files.json\" )->dict[str , float]:\n",
    "    result = {}\n",
    "    total_participants = 0\n",
    "    valid_participants = 0\n",
    "    if os.path.exists(history_path):\n",
    "        print(\"loading_history\")\n",
    "        with open(history_path , \"r\") as f:\n",
    "            result = json.load(f)\n",
    "    else:\n",
    "        print(\"no history\")\n",
    "        os.makedirs(os.path.dirname(history_path) , exist_ok=True)\n",
    "        with open(history_path , \"w\") as f:\n",
    "            json.dump(result , f , indent=2)\n",
    "\n",
    "    for release , participants_description_path in (data_dic.items()):\n",
    "\n",
    "        df = pd.read_csv(participants_description_path , sep = \"\\t\")\n",
    "        \n",
    "        total_participants += len(df)\n",
    "        print(f\"release : {release} , total participants : {len(df)}\")\n",
    "\n",
    "        for index , row in tqdm(df.iterrows()):\n",
    "            p = pd.to_numeric(row[\"p_factor\"], errors=\"coerce\")\n",
    "            if pd.isna(p):\n",
    "                continue\n",
    "\n",
    "            participant_dir_path = os.path.join(release , row[\"participant_id\"])\n",
    "            if participant_dir_path in result:\n",
    "                continue\n",
    "            if valid_participant(participant_dir_path):\n",
    "                result[participant_dir_path] = row[\"p_factor\"]\n",
    "                valid_participants += 1\n",
    "                with open(history_path , \"w\") as f:\n",
    "                    json.dump(result , f , indent=2)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"total participants : {total_participants} , valid participants : {valid_participants}\")\n",
    "    return result\n",
    "\n",
    "def p_factor_train_test_split(data_dict : dict[str , float] , test_size : float = 0.2)->tuple[dict[str , float] , dict[str , float]]:\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    participants = list(data_dict.keys())\n",
    "    train_set , test_set = train_test_split(participants , test_size=0.15) \n",
    "    for participant in train_set:\n",
    "        train_dict[participant] = data_dict[participant]\n",
    "    for participant in test_set:\n",
    "        test_dict[participant] = data_dict[participant]\n",
    "    return train_dict , test_dict\n",
    "\n",
    "def challange2_sharding(data : dict[str , float] , shard_size : int = 1000 , output_dir = r\"challange2_shards\" ,split = \"train\" ,  window_per_subject = 100):\n",
    "\n",
    "    os.makedirs(output_dir , exist_ok=True)\n",
    "    split_path = os.path.join(output_dir , split)\n",
    "    #if os.path.exists(split_path):\n",
    "    #    return\n",
    "\n",
    "    os.makedirs(split_path , exist_ok=True)\n",
    "    curr_size = 0\n",
    "    curr_windows = []\n",
    "    curr_pfactor = []\n",
    "    window_len_sec = 2  \n",
    "    stride_sec = 2 \n",
    "    num_shard = 0     \n",
    "\n",
    "    print(f\"sharding from {len(data)} participants\")\n",
    "    for participant, p_factor in tqdm(data.items()):\n",
    "        participant_files_path = os.path.join(participant, \"eeg\")\n",
    "        files = [os.path.join(participant_files_path, f) \n",
    "                for f in os.listdir(participant_files_path) if f.endswith(\".bdf\")]\n",
    "        \n",
    "\n",
    "        cutting = []\n",
    "        for file in files:\n",
    "            raw = mne.io.read_raw_bdf(file, preload=False, verbose=False)\n",
    "            rec_length_sec = int(raw.n_times // raw.info[\"sfreq\"]) \n",
    "            if rec_length_sec< 3:\n",
    "                continue\n",
    "            segments = [\n",
    "                (file, start, start + window_len_sec)\n",
    "                for start in range(0, rec_length_sec - window_len_sec , stride_sec)\n",
    "            ]\n",
    "            cutting.extend(segments)\n",
    "\n",
    "        if len(cutting) >= window_per_subject:\n",
    "            chosen = random.sample(cutting, window_per_subject)\n",
    "        else:\n",
    "            chosen = cutting \n",
    "\n",
    "        chosen = sorted(chosen, key=lambda x: (x[0], x[1]))\n",
    "        chosen_dict = {}\n",
    "        for file, start, end in chosen:\n",
    "            chosen_dict.setdefault(file, []).append((start, end))\n",
    "        for file , segments in chosen_dict.items():\n",
    "            eeg_data = mne.io.read_raw_bdf(file, verbose=False)\n",
    "        \n",
    "            for start, end in segments:\n",
    "                copied_eeg_data = eeg_data.copy()\n",
    "                croped_eeg = copied_eeg_data.crop(tmin=start, tmax=end)\n",
    "                eeg_data_array = np.array(croped_eeg.get_data()[: , :200])\n",
    "                eeg_data_array = eeg_data_array.astype(np.float32)\n",
    "                eeg_data_array = np.expand_dims(eeg_data_array, axis=0)\n",
    "                curr_windows.append(eeg_data_array)\n",
    "                curr_size += 1\n",
    "            pfactor_array =np.array([p_factor] * len(segments)).astype(np.float32)\n",
    "            curr_pfactor.append(pfactor_array)\n",
    "        if curr_size >= shard_size:\n",
    "            shard_path = os.path.join(split_path , f\"window_{num_shard}.npy\")\n",
    "            pfactor_path  = os.path.join(split_path , f\"pfactor_{num_shard}.npy\")\n",
    "            concatenated_windows = np.concatenate(curr_windows, axis=0)\n",
    "            concatenated_pfactor = np.concatenate(curr_pfactor , axis=0)\n",
    "            windows_to_be_added = concatenated_windows[: shard_size]\n",
    "            pfactor_to_be_added = concatenated_pfactor[: shard_size]\n",
    "            if concatenated_windows.shape(0) == shard_size:\n",
    "                    curr_windows = []\n",
    "                    curr_pfactor = []\n",
    "\n",
    "            else :\n",
    "                    curr_windows = [concatenated_windows[shard_size:]]\n",
    "                    curr_pfactor = [concatenated_pfactor[shard_size:]]\n",
    "            \n",
    "            X = np.lib.format.open_memmap(shard_path , mode=\"w+\" , dtype=np.float32 , shape=(windows_to_be_added.shape[0] , windows_to_be_added.shape[1] , windows_to_be_added.shape[2]))\n",
    "            X[:] = windows_to_be_added\n",
    "            X.flush()\n",
    "            del X\n",
    "            X = np.lib.format.open_memmap(pfactor_path , mode=\"w+\" , dtype=np.float32 , shape=(pfactor_to_be_added.shape[0] , 1))\n",
    "            X[:] = pfactor_to_be_added\n",
    "            X.flush()\n",
    "            del X\n",
    "            curr_size = 0\n",
    "            num_shard += 1\n",
    "    \n",
    "    if curr_size > 0:\n",
    "        shard_path = os.path.join(split_path , f\"window_{num_shard}.npy\")\n",
    "        pfactor_path  = os.path.join(split_path , f\"pfactor_{num_shard}.npy\")\n",
    "        concatenated_windows = np.concatenate(curr_windows, axis=0)\n",
    "        concatenated_pfactor = np.concatenate(curr_pfactor , axis=0)\n",
    "        X = np.lib.format.open_memmap(shard_path , mode=\"w+\" , dtype=np.float32 , shape=(concatenated_windows.shape[0] , concatenated_windows.shape[1] , concatenated_windows.shape[2]))\n",
    "        X[:] = concatenated_windows\n",
    "        X.flush()\n",
    "        del X\n",
    "        X = np.lib.format.open_memmap(pfactor_path , mode=\"w+\" , dtype=np.float32 , shape=(concatenated_pfactor.shape[0] , 1))\n",
    "        X[:] = concatenated_pfactor\n",
    "        X.flush()\n",
    "        del X\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import mne\n",
    "from tqdm import tqdm\n",
    "\n",
    "def challenge2_sharding(\n",
    "    data: Dict[str, float],              # {participant_dir: p_factor}\n",
    "    shard_size: int = 1000,\n",
    "    output_dir: str = \"challenge2_shards\",\n",
    "    split: str = \"train\",\n",
    "    window_per_subject: int = 100,\n",
    "    window_len_sec: int = 2,\n",
    "    stride_sec: int = 2,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    random.seed(seed)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    split_path = os.path.join(output_dir, split)\n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"split {split} already exists\")\n",
    "        return\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "    \n",
    "    num_shard = 0\n",
    "    curr_windows, curr_targets, curr_subjects = [], [], []\n",
    "    curr_size = 0\n",
    "\n",
    "    print(f\"sharding from {len(data)} participants\")\n",
    "\n",
    "    for participant_dir, p_factor in tqdm(data.items()):\n",
    "        eeg_dir = os.path.join(participant_dir, \"eeg\")\n",
    "        files = [\n",
    "            os.path.join(eeg_dir, f)\n",
    "            for f in os.listdir(eeg_dir) if f.endswith(\".bdf\")\n",
    "        ]\n",
    "\n",
    "        # Build candidate windows (file, start_sec, end_sec) ONLY from 129-ch recordings\n",
    "        candidates = []\n",
    "        for file in files:\n",
    "            raw_hdr = mne.io.read_raw_bdf(file, preload=False, verbose=False)\n",
    "\n",
    "            # HARD GUARD: only accept recordings with exactly 129 channels\n",
    "            if len(raw_hdr.ch_names) != 129:\n",
    "                continue\n",
    "\n",
    "            dur_sec = int(raw_hdr.last_samp // raw_hdr.info[\"sfreq\"])\n",
    "            if dur_sec < window_len_sec + 1:\n",
    "                continue\n",
    "\n",
    "            for start in range(0, dur_sec - window_len_sec + 1, stride_sec):\n",
    "                candidates.append((file, start, start + window_len_sec))\n",
    "\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        # Sample windows for this participant\n",
    "        k = min(window_per_subject, len(candidates))\n",
    "        chosen = random.sample(candidates, k)\n",
    "\n",
    "        # Group by file to open each once\n",
    "        per_file = {}\n",
    "        for file, s, e in chosen:\n",
    "            per_file.setdefault(file, []).append((s, e))\n",
    "        for file in per_file:\n",
    "            per_file[file].sort(key=lambda t: t[0])\n",
    "\n",
    "        # Extract windows\n",
    "        for file, segs in per_file.items():\n",
    "            raw = mne.io.read_raw_bdf(file, preload=False, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "            sfreq = raw.info[\"sfreq\"]\n",
    "            win_samp = int(round(window_len_sec * sfreq))\n",
    "\n",
    "            for start_sec, _end_sec in segs:\n",
    "                start_samp = int(round(start_sec * sfreq))\n",
    "                stop_samp  = start_samp + win_samp\n",
    "                X = raw.get_data(start=start_samp, stop=stop_samp)  # (C, T), non-mutating\n",
    "                if X.shape[0] != 129 or X.shape[1] != win_samp:\n",
    "                    continue\n",
    "                curr_windows.append(X.astype(np.float32))\n",
    "                curr_targets.append(np.float32(p_factor))\n",
    "                curr_subjects.append(participant_dir)\n",
    "                curr_size += 1\n",
    "\n",
    "                if curr_size >= shard_size:\n",
    "                    _flush_shard(split_path, num_shard, curr_windows, curr_targets, curr_subjects)\n",
    "                    num_shard += 1\n",
    "                    curr_windows, curr_targets, curr_subjects = [], [], []\n",
    "                    curr_size = 0\n",
    "\n",
    "    if curr_size > 0:\n",
    "        _flush_shard(split_path, num_shard, curr_windows, curr_targets, curr_subjects)\n",
    "\n",
    "\n",
    "def _flush_shard(split_path, shard_idx, windows_list, targets_list, subjects_list):\n",
    "    X = np.stack(windows_list, axis=0)             # (N, 129, T)\n",
    "    y = np.asarray(targets_list, dtype=np.float32) # (N,)\n",
    "    subj = subjects_list\n",
    "\n",
    "    win_path  = os.path.join(split_path, f\"windows_{shard_idx}.npy\")\n",
    "    y_path    = os.path.join(split_path, f\"pfactor_{shard_idx}.npy\")\n",
    "\n",
    "    Xm = np.lib.format.open_memmap(win_path, mode=\"w+\", dtype=np.float32, shape=X.shape)\n",
    "    Xm[:] = X; Xm.flush(); del Xm\n",
    "\n",
    "    Ym = np.lib.format.open_memmap(y_path, mode=\"w+\", dtype=np.float32, shape=(y.shape[0], 1))\n",
    "    Ym[:, 0] = y; Ym.flush(); del Ym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading_history\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"D:\\projects\\pytorch_training\\data_1\"\n",
    "data_dict = get_participants_p_factor_files(data_dir)\n",
    "valid_participants_pf = extract_valid_subjects_scores(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dict , test_dict = p_factor_train_test_split(valid_participants_pf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharding from 1876 participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1876/1876 [2:42:49<00:00,  5.21s/it]  \n"
     ]
    }
   ],
   "source": [
    "challange2_shard_output=\"challange2_shards\"\n",
    "challenge2_sharding(train_dict , split=\"train\" , output_dir=challange2_shard_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_sharding(test_dict ,split=\"test\" , output_dir=challange2_shard_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Challange2_Dataset(Dataset):\n",
    "    def __init__(self , shards_dir  ,split = \"train\", shard_size  =1000 ):\n",
    "        self.shards_path = os.path.join(shards_dir , split)\n",
    "        self.numbers_of_shards = int( len(os.listdir(self.shards_path)) / 2)\n",
    "        self.shard_size = shard_size\n",
    "        length =0\n",
    "        for shard_path in os.listdir(self.shards_path):\n",
    "            if \"window\" in shard_path:\n",
    "                window_shard_path = os.path.join(self.shards_path , shard_path)\n",
    "                shard = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "                size = shard.shape[0]\n",
    "                length += size\n",
    "        self.length = length\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        shard_pos = index // self.shard_size \n",
    "        window_shard_path = os.path.join(self.shards_path , f\"windows_{shard_pos}.npy\")\n",
    "        X = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "        rt_shard_path = os.path.join(self.shards_path , f\"pfactor_{shard_pos}.npy\")\n",
    "        Y = np.lib.format.open_memmap(rt_shard_path , mode=\"r\")\n",
    "        raw = X[index % self.shard_size]\n",
    "        rt = Y[index % self.shard_size]\n",
    "        return raw , rt\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_train_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"train\" , shard_size=1000)\n",
    "c2_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"test\" , shard_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "\n",
    "c2_train_loader = DataLoader(c2_train_dataset , batch_size=batch_size , shuffle=False)\n",
    "c2_test_loader = DataLoader(c2_test_dataset , batch_size=batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1876/1876 [02:28<00:00, 12.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x ,y in tqdm(c2_train_loader):\n",
    "\n",
    "    if len(np.unique(y)) != 1:\n",
    "        print(len(np.unique(y)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challange 2 Model building and training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, dim=512, hidden=256, bias_to_x=0.5):\n",
    "        super().__init__()\n",
    "        self.ln_x = nn.LayerNorm(dim)\n",
    "        self.ln_y = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, dim)\n",
    "        )\n",
    "        # Optional: bias gate toward x at init (sigmoid(bias_to_x))\n",
    "        nn.init.constant_(self.mlp[-1].bias, bias_to_x)\n",
    "\n",
    "    def forward(self, x, y):           # x,y: (B, dim)\n",
    "        x_n = self.ln_x(x)\n",
    "        y_n = self.ln_y(y)\n",
    "        h = torch.cat([x_n, y_n], dim=-1)\n",
    "        g = torch.sigmoid(self.mlp(h)) # (B, dim)\n",
    "        fused = g * x + (1.0 - g) * y  # convex blend per feature\n",
    "        return fused, g                # returning g helps you log/inspect\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Challange2_Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.fusion = GatedFusion(dim=512, hidden=256)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        a = self.pool(x)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(x)             # (B, 512)\n",
    "        x , _= self.fusion(a, b)                  # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 501/5863 [00:24<04:17, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train nRMSE : 1.0044566321501807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 501/907 [00:18<00:14, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0014614872935472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5863/5863 [05:19<00:00, 18.36it/s]\n",
      "  9%|▊         | 501/5863 [00:20<03:38, 24.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 1 , loss : 0.0 , nRMSE : 0.9945003492095315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 907/907 [00:31<00:00, 28.42it/s]\n",
      " 55%|█████▌    | 501/907 [00:15<00:12, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.6311436260112406 , nRMSE : 1.016806804298285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5863/5863 [05:52<00:00, 16.65it/s]\n",
      "  9%|▊         | 501/5863 [00:22<03:56, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 2 , loss : 0.0 , nRMSE : 0.9955796006177127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 907/907 [00:37<00:00, 24.18it/s]\n",
      " 55%|█████▌    | 501/907 [00:15<00:12, 32.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , loss : 0.6276227744016899 , nRMSE : 1.0084521462331582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4644/5863 [04:37<01:12, 16.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 44\u001b[39m\n",
      "\u001b[32m     42\u001b[39m pfactor_regressor.train()\n",
      "\u001b[32m     43\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc2_train_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n",
      "\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n",
      "\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n",
      "\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n",
      "\u001b[32m    714\u001b[39m ):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n",
      "\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n",
      "\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n",
      "\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n",
      "\u001b[32m    340\u001b[39m \n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n",
      "\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n",
      "\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n",
      "\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n",
      "\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n",
      "\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:159\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n",
      "\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcollate_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n",
      "\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.Mapping):\n",
      "\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:285\u001b[39m, in \u001b[36mcollate_numpy_array_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n",
      "\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern.search(elem.dtype.str) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem.dtype))\n",
      "\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n",
      "\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n",
      "\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n",
      "\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n",
      "\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n",
      "\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"6layer_encoder.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "\n",
    "pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": pfactor_regressor.encoder.parameters(), \"lr\": 1e-4},    \n",
    "    {\"params\": pfactor_regressor.pool.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": pfactor_regressor.temporal_conv.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": pfactor_regressor.fusion.parameters(), \"lr\": 5e-4},\n",
    "    {\"params\": pfactor_regressor.regressor.parameters(), \"lr\": 5e-4},\n",
    "], weight_decay=0.02)\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loss_f = nn.SmoothL1Loss(beta=0.2)  \n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(pfactor_regressor , c2_test_loader ,device)}\")\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pfactor_regressor.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(c2_train_loader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = pfactor_regressor(x)\n",
    "        loss = loss_f(y_pred , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    nrmse_train = nrmse_over_data(pfactor_regressor , c2_train_loader ,device)\n",
    "\n",
    "\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(c2_train_loader)} , nRMSE : {nrmse_train}\")\n",
    "    with torch.inference_mode() :\n",
    "        pfactor_regressor.eval()\n",
    "        for batch in tqdm(c2_test_loader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = pfactor_regressor(x)\n",
    "            loss = loss_f(y_pred , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_test = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(c2_test_loader)} , nRMSE : {nrmse_test}\")\n",
    "    if nrmse_test < best_rnmse:\n",
    "        print(f\"new best model found with rnmse : {nrmse_test}\")\n",
    "        best_rnmse = nrmse_test\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no history\n",
      "release : D:\\projects\\pytorch_training\\test_data\\R5_L100_bdf , total participants : 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [08:26,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total participants : 330 , valid participants : 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = r\"D:\\projects\\pytorch_training\\test_data\"\n",
    "data_dict = get_participants_p_factor_files(data_dir)\n",
    "valid_participants_pf = extract_valid_subjects_scores(data_dict)\n",
    "len(valid_participants_pf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharding from 312 participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [28:04<00:00,  5.40s/it]\n"
     ]
    }
   ],
   "source": [
    "challange2_shard_output=\"challange2_shards\"\n",
    "challenge2_sharding(valid_participants_pf , split=\"final_r5_test\" , output_dir=challange2_shard_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 501/975 [00:26<00:24, 19.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test nRMSE : 1.0038246483784488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    pfactor_regressor.load_state_dict(torch.load(best_model_path))\n",
    "    pfactor_regressor.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_final_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"final_r5_test\" , shard_size=1000)\n",
    "c2_final_test_loader = DataLoader(c2_final_test_dataset , batch_size=batch_size , shuffle=True)\n",
    "\n",
    "nrmse_final_test = nrmse_over_data(dataloader=c2_final_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"final test nRMSE : {nrmse_final_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self,nb_tokens , c_dim = 129   ,  t_dim = 200 , slice_size = 10 ,target_c_dim = 64 , target_t_dim = 6  , emb_dim = 512     ):\n",
    "        super().__init__()\n",
    "        self.time_projection = nn.Linear(slice_size  ,target_t_dim)\n",
    "        self.channel_projection = nn.Linear(c_dim , target_c_dim)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1 , 1 , 1 , target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1 , 1 , target_c_dim , 1))\n",
    "        self.token_projection = nn.Linear(target_c_dim*target_t_dim , emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(  nb_tokens , emb_dim))\n",
    "        self.mask_paramater = nn.Parameter(torch.zeros( c_dim , slice_size))\n",
    "        nn.init.xavier_uniform_(self.time_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.channel_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mask_paramater)\n",
    "\n",
    "        self.slice_size = slice_size\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        # x (B , N , C , slice)  N = T // slice\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_paramater\n",
    "\n",
    "\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size , target_c_dim = self.c_dim , target_t_dim = self.slice_size , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        masked_index = torch.randint(low=0 , high=self.nb_tokens , size=(x.shape[0],int(self.nb_tokens * mask_ratio),))\n",
    "        return   masked_index\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "\n",
    "\n",
    "class Challange2_Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.normilize = nn.LayerNorm(512)\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        x = self.normilize(x)\n",
    "        x = self.pool(x)                      # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:54<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.003198086239821\n",
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:27<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0026235428985073\n",
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:31<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0015730216609884\n",
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:46<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0116842326565905\n",
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:47<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0053054048802552\n",
      "best model loaded with rnmse : 0.9999174924321194\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:44<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 0.9998998786514097\n",
      "new best model found with rnmse : 0.9998998786514097\n",
      "best model loaded with rnmse : 0.9998998786514097\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:46<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.006349593747937\n",
      "best model loaded with rnmse : 0.9998998786514097\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:46<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.00084751477638\n",
      "best model loaded with rnmse : 0.9998998786514097\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:46<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0065342413859786\n",
      "best model loaded with rnmse : 0.9998998786514097\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:47<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0006873196021535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "        \n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "    save_path = \"last_checkpoint.pt\"\n",
    "    if os.path.isfile(save_path):\n",
    "        ckpt = torch.load(save_path, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        start_epoch = ckpt.get(\"epoch\", 0)\n",
    "        start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "        global_steps = ckpt.get(\"steps\", 0)\n",
    "        train_losses = ckpt.get(\"train_losses\", [])\n",
    "        val_losses = ckpt.get(\"val_losses\", [])\n",
    "        \n",
    "\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "    pfactor_regressor.eval()\n",
    "\n",
    "\n",
    "\n",
    "    best_model_path = \"challange2_best_model.pt\"\n",
    "    best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "    best_rnmse = 1000\n",
    "    if os.path.exists(best_rnmse_path):\n",
    "        best_rnmse = float(torch.load(best_rnmse_path))\n",
    "        print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "    print(\"before training\")\n",
    "    ##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "    test_nrmse = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test nRMSE : {test_nrmse}\")\n",
    "    if test_nrmse < best_rnmse:\n",
    "        print(f\"new best model found with rnmse : {test_nrmse}\")\n",
    "        best_rnmse = test_nrmse\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9999735322769453\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:29<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0035679923311043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2932/2932 [05:37<00:00,  8.69it/s]\n",
      " 17%|█▋        | 501/2932 [00:42<03:26, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 1 , loss : 0.6228586677233161 , nRMSE : 0.9823486164805656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:39<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.0 , nRMSE : 1.0057243055191498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 592/2932 [01:14<04:55,  7.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 53\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     51\u001b[39m pfactor_regressor.train()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     52\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc2_train_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    714\u001b[39m ):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mChallange2_Dataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     18\u001b[39m shard_pos = index // \u001b[38;5;28mself\u001b[39m.shard_size \n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     19\u001b[39m window_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwindows_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_memmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_shard_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     21\u001b[39m rt_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpfactor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m     22\u001b[39m Y = np.lib.format.open_memmap(rt_shard_path , mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\pytorch_training\\Lib\\site-packages\\numpy\\lib\\format.py:944\u001b[39m, in \u001b[36mopen_memmap\u001b[39m\u001b[34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    941\u001b[39m         offset = fp.tell()\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    943\u001b[39m     \u001b[38;5;66;03m# Read the header of the file first.\u001b[39;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m:\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_check_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "    \n",
    "\n",
    "pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": pfactor_regressor.encoder.parameters(), \"lr\": 5e-5 ,  \"weight_decay\": 0.01},    \n",
    "    {\"params\": pfactor_regressor.normilize.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.0},   # LayerNorm → no decay\n",
    "    {\"params\": pfactor_regressor.pool.parameters(),      \"lr\": 1e-4, \"weight_decay\": 0.02},\n",
    "    {\"params\": pfactor_regressor.regressor.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.02},\n",
    "], weight_decay=0.02)\n",
    "\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "pfactor_regressor.eval()\n",
    "\n",
    "\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "print(\"before training\")\n",
    "##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "test_nrmse = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {test_nrmse}\")\n",
    "if test_nrmse < best_rnmse:\n",
    "    print(f\"new best model found with rnmse : {test_nrmse}\")\n",
    "    best_rnmse = test_nrmse\n",
    "    torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "    torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pfactor_regressor.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(c2_train_loader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = pfactor_regressor(x)\n",
    "\n",
    "        loss = loss_f(y_pred , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pfactor_regressor.eval()\n",
    "\n",
    "    nrmse_train = nrmse_over_data(pfactor_regressor , c2_train_loader ,device)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(c2_train_loader)} , nRMSE : {nrmse_train}\")\n",
    "    cumulative_loss = 0\n",
    "    with torch.inference_mode() :\n",
    "        nrmse_test = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(c2_test_loader)} , nRMSE : {nrmse_test}\")\n",
    "    if nrmse_test < best_rnmse:\n",
    "        print(f\"new best model found with rnmse : {nrmse_test}\")\n",
    "        best_rnmse = nrmse_test\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:52<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test nRMSE : 0.9999527320695456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    pfactor_regressor.load_state_dict(torch.load(best_model_path))\n",
    "    pfactor_regressor.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_final_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"final_r5_test\" , shard_size=1000)\n",
    "c2_final_test_loader = DataLoader(c2_final_test_dataset , batch_size=batch_size , shuffle=True)\n",
    "\n",
    "nrmse_final_test = nrmse_over_data(dataloader=c2_final_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"final test nRMSE : {nrmse_final_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self,nb_tokens , c_dim = 129   ,  t_dim = 200 , slice_size = 10 ,target_c_dim = 64 , target_t_dim = 6  , emb_dim = 512     ):\n",
    "        super().__init__()\n",
    "        self.time_projection = nn.Linear(slice_size  ,target_t_dim)\n",
    "        self.channel_projection = nn.Linear(c_dim , target_c_dim)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1 , 1 , 1 , target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1 , 1 , target_c_dim , 1))\n",
    "        self.token_projection = nn.Linear(target_c_dim*target_t_dim , emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(  nb_tokens , emb_dim))\n",
    "        self.mask_paramater = nn.Parameter(torch.zeros( c_dim , slice_size))\n",
    "        nn.init.xavier_uniform_(self.time_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.channel_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mask_paramater)\n",
    "\n",
    "        self.slice_size = slice_size\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        # x (B , N , C , slice)  N = T // slice\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_paramater\n",
    "\n",
    "\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size , target_c_dim = self.c_dim , target_t_dim = self.slice_size , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        masked_index = torch.randint(low=0 , high=self.nb_tokens , size=(x.shape[0],int(self.nb_tokens * mask_ratio),))\n",
    "        return   masked_index\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Challange2_Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.normilize = nn.LayerNorm(512)\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.regressor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        x = self.normilize(x)\n",
    "        x = self.pool(x)                      # (B, 512)\n",
    "        x = self.regressor(x)*4                 # (B, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "for i in range(10000):\n",
    "        \n",
    "\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "    pfactor_regressor.eval()\n",
    "\n",
    "\n",
    "\n",
    "    ##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "    test_nrmse = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test nRMSE : {test_nrmse}\")\n",
    "    if test_nrmse < best_rnmse:\n",
    "        print(f\"new best model found\")\n",
    "        print(f\"old rnmse : { best_rnmse}\")\n",
    "        print(f\"new rnmse : {test_nrmse}\")\n",
    "        best_rnmse = test_nrmse\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9982996004650344\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 11/29 [00:20<00:32,  1.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 41\u001b[39m\n",
      "\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbefore training\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     40\u001b[39m \u001b[38;5;66;03m##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m test_nrmse = \u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfactor_regressor\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2_test_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest nRMSE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_nrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_nrmse < best_rnmse:\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n",
      "\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n",
      "\u001b[32m      9\u001b[39m     index = \u001b[32m0\u001b[39m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m>\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n",
      "\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n",
      "\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n",
      "\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n",
      "\u001b[32m    740\u001b[39m ):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n",
      "\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n",
      "\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mChallange2_Dataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n",
      "\u001b[32m     20\u001b[39m X = np.lib.format.open_memmap(window_shard_path , mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     21\u001b[39m rt_shard_path = os.path.join(\u001b[38;5;28mself\u001b[39m.shards_path , \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpfactor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m Y = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_memmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrt_shard_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     23\u001b[39m raw = X[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n",
      "\u001b[32m     24\u001b[39m rt = Y[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:972\u001b[39m, in \u001b[36mopen_memmap\u001b[39m\u001b[34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001b[39m\n",
      "\u001b[32m    969\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Read the header of the file first.\u001b[39;00m\n",
      "\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.fspath(filename), \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m         version = \u001b[43mread_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    973\u001b[39m         _check_version(version)\n",
      "\u001b[32m    975\u001b[39m         shape, fortran_order, dtype = _read_array_header(\n",
      "\u001b[32m    976\u001b[39m                 fp, version, max_header_size=max_header_size)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:242\u001b[39m, in \u001b[36mread_magic\u001b[39m\u001b[34m(fp)\u001b[39m\n",
      "\u001b[32m    229\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumpy.lib.format\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_magic\u001b[39m(fp):\n",
      "\u001b[32m    231\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read the magic string to get the version of the file format.\u001b[39;00m\n",
      "\u001b[32m    232\u001b[39m \n",
      "\u001b[32m    233\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m \u001b[33;03m    minor : int\u001b[39;00m\n",
      "\u001b[32m    241\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     magic_str = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAGIC_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmagic string\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m magic_str[:-\u001b[32m2\u001b[39m] != MAGIC_PREFIX:\n",
      "\u001b[32m    244\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mthe magic string is not correct; expected \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:1013\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n",
      "\u001b[32m   1008\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m   1009\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n",
      "\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n",
      "\u001b[32m   1011\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n",
      "\u001b[32m   1012\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1014\u001b[39m         data += r\n",
      "\u001b[32m   1015\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "    \n",
    "\n",
    "pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": pfactor_regressor.encoder.parameters(), \"lr\": 5e-5 ,  \"weight_decay\": 0.01},    \n",
    "    {\"params\": pfactor_regressor.normilize.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.0},   # LayerNorm → no decay\n",
    "    {\"params\": pfactor_regressor.pool.parameters(),      \"lr\": 1e-4, \"weight_decay\": 0.02},\n",
    "    {\"params\": pfactor_regressor.regressor.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.02},\n",
    "], weight_decay=0.02)\n",
    "\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "pfactor_regressor.eval()\n",
    "\n",
    "\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "print(\"before training\")\n",
    "##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "test_nrmse = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {test_nrmse}\")\n",
    "if test_nrmse < best_rnmse:\n",
    "    print(f\"new best model found with rnmse : {test_nrmse}\")\n",
    "    best_rnmse = test_nrmse\n",
    "    torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "    torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pfactor_regressor.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(c2_train_loader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = pfactor_regressor(x)\n",
    "\n",
    "        loss = loss_f(y_pred , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pfactor_regressor.eval()\n",
    "\n",
    "    nrmse_train = nrmse_over_data(pfactor_regressor , c2_train_loader ,device)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(c2_train_loader)} , nRMSE : {nrmse_train}\")\n",
    "    cumulative_loss = 0\n",
    "    with torch.inference_mode() :\n",
    "        nrmse_test = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(c2_test_loader)} , nRMSE : {nrmse_test}\")\n",
    "    if nrmse_test < best_rnmse:\n",
    "        print(f\"new best model found with rnmse : {nrmse_test}\")\n",
    "        best_rnmse = nrmse_test\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:37<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test nRMSE : 1.0008846904566875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "    \n",
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    pfactor_regressor.load_state_dict(torch.load(best_model_path))\n",
    "    pfactor_regressor.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_final_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"test\" , shard_size=1000)\n",
    "c2_final_test_loader = DataLoader(c2_final_test_dataset , batch_size=batch_size , shuffle=False)\n",
    "\n",
    "nrmse_final_test = nrmse_over_data(dataloader=c2_final_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"final test nRMSE : {nrmse_final_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self,nb_tokens , c_dim = 129   ,  t_dim = 200 , slice_size = 10 ,target_c_dim = 64 , target_t_dim = 6  , emb_dim = 512     ):\n",
    "        super().__init__()\n",
    "        self.time_projection = nn.Linear(slice_size  ,target_t_dim)\n",
    "        self.channel_projection = nn.Linear(c_dim , target_c_dim)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1 , 1 , 1 , target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1 , 1 , target_c_dim , 1))\n",
    "        self.token_projection = nn.Linear(target_c_dim*target_t_dim , emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(  nb_tokens , emb_dim))\n",
    "        self.mask_paramater = nn.Parameter(torch.zeros( c_dim , slice_size))\n",
    "        nn.init.xavier_uniform_(self.time_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.channel_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mask_paramater)\n",
    "\n",
    "        self.slice_size = slice_size\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        # x (B , N , C , slice)  N = T // slice\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_paramater\n",
    "\n",
    "\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size , target_c_dim = self.c_dim , target_t_dim = self.slice_size , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        masked_index = torch.randint(low=0 , high=self.nb_tokens , size=(x.shape[0],int(self.nb_tokens * mask_ratio),))\n",
    "        return   masked_index\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Challange2_Dataset(Dataset):\n",
    "    def __init__(self , shards_dir  ,split = \"train\", shard_size  =1000 ):\n",
    "        self.shards_path = os.path.join(shards_dir , split)\n",
    "        self.numbers_of_shards = int( len(os.listdir(self.shards_path)) / 2)\n",
    "        self.shard_size = shard_size\n",
    "        length =0\n",
    "        for shard_path in os.listdir(self.shards_path):\n",
    "            if \"window\" in shard_path:\n",
    "                window_shard_path = os.path.join(self.shards_path , shard_path)\n",
    "                shard = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "                size = shard.shape[0]\n",
    "                length += size\n",
    "        self.length = length\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        shard_pos = index // self.shard_size \n",
    "        window_shard_path = os.path.join(self.shards_path , f\"windows_{shard_pos}.npy\")\n",
    "        X = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "        rt_shard_path = os.path.join(self.shards_path , f\"pfactor_{shard_pos}.npy\")\n",
    "        Y = np.lib.format.open_memmap(rt_shard_path , mode=\"r\")\n",
    "        raw = X[index % self.shard_size]\n",
    "        rt = Y[index % self.shard_size]\n",
    "        return raw , rt\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.empty(1, 1, dim))\n",
    "        nn.init.kaiming_normal_(self.query, nonlinearity=\"relu\")\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "\n",
    "\n",
    "class Challange2_Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.normilize = nn.LayerNorm(512)\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        for m in self.regressor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        x = self.normilize(x)\n",
    "        x = self.pool(x)                      # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:23<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0002384573891525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:34<00:00,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test nRMSE : 1.0003417589108892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"test\" , shard_size=1000)\n",
    "c2_test_loader = DataLoader(c2_test_dataset , batch_size=batch_size , shuffle=False )\n",
    "\n",
    "c2_final_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"final_r5_test\" , shard_size=1000)\n",
    "c2_final_test_loader = DataLoader(c2_final_test_dataset , batch_size=batch_size , shuffle=False)\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\n",
    "    \n",
    "\n",
    "pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    pfactor_regressor.load_state_dict(torch.load(best_model_path))\n",
    "    pfactor_regressor.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "\n",
    "\n",
    "nrmse_test = nrmse_over_data(dataloader=c2_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"test nRMSE : {nrmse_test}\")\n",
    "nrmse_final_test = nrmse_over_data(dataloader=c2_final_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"final test nRMSE : {nrmse_final_test}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self,nb_tokens , c_dim = 129   ,  t_dim = 200 , slice_size = 10 ,target_c_dim = 64 , target_t_dim = 6  , emb_dim = 512     ):\n",
    "        super().__init__()\n",
    "        self.time_projection = nn.Linear(slice_size  ,target_t_dim)\n",
    "        self.channel_projection = nn.Linear(c_dim , target_c_dim)\n",
    "        self.time_positional_emb = nn.Parameter(torch.zeros(1 , 1 , 1 , target_t_dim))\n",
    "        self.channel_positional_emb = nn.Parameter(torch.zeros(1 , 1 , target_c_dim , 1))\n",
    "        self.token_projection = nn.Linear(target_c_dim*target_t_dim , emb_dim)\n",
    "        self.token_positional_emb = nn.Parameter(torch.zeros(  nb_tokens , emb_dim))\n",
    "        self.mask_paramater = nn.Parameter(torch.zeros( c_dim , slice_size))\n",
    "        nn.init.xavier_uniform_(self.time_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.channel_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mask_paramater)\n",
    "\n",
    "        self.slice_size = slice_size\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        # x (B , N , C , slice)  N = T // slice\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_paramater\n",
    "\n",
    "\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size , target_c_dim = self.c_dim , target_t_dim = self.slice_size , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        masked_index = torch.randint(low=0 , high=self.nb_tokens , size=(x.shape[0],int(self.nb_tokens * mask_ratio),))\n",
    "        return   masked_index\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGConvRegressor(nn.Module):\n",
    "    def __init__(self, c_dim=129, t_dim=200, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, emb_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 129, 200)\n",
    "        x = x.unsqueeze(1)  # (B, 1, 129, 200)\n",
    "        x = self.conv_layer(x)  # (B, emb_dim, H, W)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # (B, emb_dim, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, emb_dim)\n",
    "        x = self.regressor(x)  # (B, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9998998786514097\n",
      "before training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:37<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 1.0110001349853748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2932/2932 [04:12<00:00, 11.63it/s]\n",
      " 17%|█▋        | 501/2932 [00:46<03:43, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 1 , loss : 0.8070236326768323 , nRMSE : 1.0011212676437142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:37<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , loss : 0.0 , nRMSE : 1.0029461556580133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2932/2932 [04:02<00:00, 12.10it/s]\n",
      " 17%|█▋        | 501/2932 [00:50<04:04,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 2 , loss : 0.806183681888112 , nRMSE : 1.0001858077409806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:38<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , loss : 0.0 , nRMSE : 1.0013392686968718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2932/2932 [03:55<00:00, 12.47it/s]\n",
      " 17%|█▋        | 501/2932 [00:45<03:38, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 3 , loss : 0.8055304524375667 , nRMSE : 1.0009761674353357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [00:36<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 3 , loss : 0.0 , nRMSE : 1.0039193488879208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/2932 [00:02<04:02, 11.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 55\u001b[39m\n",
      "\u001b[32m     52\u001b[39m y_pred = pfactor_regressor(x)\n",
      "\u001b[32m     54\u001b[39m loss = loss_f(y_pred , y)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m cumulative_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     56\u001b[39m optimizer.zero_grad()\n",
      "\u001b[32m     57\u001b[39m loss.backward()\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    start_it = ckpt.get(\"it_in_epoch\", 0)\n",
    "    global_steps = ckpt.get(\"steps\", 0)\n",
    "    train_losses = ckpt.get(\"train_losses\", [])\n",
    "    val_losses = ckpt.get(\"val_losses\", [])\"\"\"\n",
    "    \n",
    "\n",
    "pfactor_regressor = EEGConvRegressor().to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW( pfactor_regressor.parameters(), weight_decay=0.02)\n",
    "\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loss_f = nn.MSELoss()\n",
    "\n",
    "pfactor_regressor.eval()\n",
    "\n",
    "\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "print(\"before training\")\n",
    "##print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor , c2_train_loader  ,device)}\")\n",
    "test_nrmse = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {test_nrmse}\")\n",
    "if test_nrmse < best_rnmse:\n",
    "    print(f\"new best model found with rnmse : {test_nrmse}\")\n",
    "    best_rnmse = test_nrmse\n",
    "    torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "    torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    pfactor_regressor.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(c2_train_loader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = pfactor_regressor(x)\n",
    "\n",
    "        loss = loss_f(y_pred , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pfactor_regressor.eval()\n",
    "\n",
    "    nrmse_train = nrmse_over_data(pfactor_regressor , c2_train_loader ,device)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(c2_train_loader)} , nRMSE : {nrmse_train}\")\n",
    "    cumulative_loss = 0\n",
    "    with torch.inference_mode() :\n",
    "        nrmse_test = nrmse_over_data(pfactor_regressor , c2_test_loader ,device)\n",
    "    print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(c2_test_loader)} , nRMSE : {nrmse_test}\")\n",
    "    if nrmse_test < best_rnmse:\n",
    "        print(f\"new best model found with rnmse : {nrmse_test}\")\n",
    "        best_rnmse = nrmse_test\n",
    "        torch.save(pfactor_regressor.state_dict() , best_model_path)\n",
    "        torch.save(best_rnmse , best_rnmse_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:43<00:00, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test nRMSE : 1.000165676315455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor = Challange2_Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    pfactor_regressor.load_state_dict(torch.load(best_model_path))\n",
    "    pfactor_regressor.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "\n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_final_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"final_r5_test\" , shard_size=1000)\n",
    "c2_final_test_loader = DataLoader(c2_final_test_dataset , batch_size=batch_size , shuffle=True)\n",
    "\n",
    "nrmse_final_test = nrmse_over_data(dataloader=c2_final_test_loader , model=pfactor_regressor , device=device)\n",
    "print(f\"final test nRMSE : {nrmse_final_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Challange2_Dataset(Dataset):\n",
    "    def __init__(self , shards_dir  ,split = \"train\", shard_size  =1000 ):\n",
    "        self.shards_path = os.path.join(shards_dir , split)\n",
    "        self.numbers_of_shards = int( len(os.listdir(self.shards_path)) / 2)\n",
    "        self.shard_size = shard_size\n",
    "        length =0\n",
    "        for shard_path in os.listdir(self.shards_path):\n",
    "            if \"window\" in shard_path:\n",
    "                window_shard_path = os.path.join(self.shards_path , shard_path)\n",
    "                shard = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "                size = shard.shape[0]\n",
    "                length += size\n",
    "        self.length = length\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        shard_pos = index // self.shard_size \n",
    "        window_shard_path = os.path.join(self.shards_path , f\"windows_{shard_pos}.npy\")\n",
    "        X = np.lib.format.open_memmap(window_shard_path , mode=\"r\")\n",
    "        rt_shard_path = os.path.join(self.shards_path , f\"pfactor_{shard_pos}.npy\")\n",
    "        Y = np.lib.format.open_memmap(rt_shard_path , mode=\"r\")\n",
    "        raw = X[index % self.shard_size]\n",
    "        rt = Y[index % self.shard_size]\n",
    "        return raw , rt\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "challange2_shard_output=\"challange2_shards\"\n",
    "c2_train_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"train\" , shard_size=1000)\n",
    "c2_test_dataset = Challange2_Dataset(shards_dir=challange2_shard_output , split=\"test\" , shard_size=1000)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "c2_train_loader = DataLoader(c2_train_dataset , batch_size=batch_size , shuffle=False)\n",
    "c2_test_loader = DataLoader(c2_test_dataset , batch_size=batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "indices = np.random.choice(len(c2_train_dataset), len(c2_train_dataset)//20, replace=False)\n",
    "c2_train_dataset = Subset(c2_train_dataset, indices)\n",
    "\n",
    "indices = np.random.choice(len(c2_test_dataset), len(c2_test_dataset)//10, replace=False)\n",
    "c2_test_dataset = Subset(c2_test_dataset, indices)\n",
    "batch_size = 128\n",
    "\n",
    "c2_train_loader = DataLoader(c2_train_dataset , batch_size=batch_size , shuffle=False)\n",
    "c2_test_loader = DataLoader(c2_test_dataset , batch_size=batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result    \n",
    "\n",
    "class Vit_EEG_PF_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False  , dim_emb = 512) :\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.dim_emb = dim_emb\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(self.dim_emb),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.dim_emb*20, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def merge(self, x, y):\n",
    "        return self.alpha * x + self.beta * y\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "\n",
    "        x = self.regressor(x)                 # (B,1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 0.9931397606567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1876/1876 [03:37<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 1 , loss : 0.8187817580009593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:35<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 1 , nRMSE : 1.006282768920647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1876/1876 [03:52<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 2 , loss : 0.8105901838924906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:35<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 2 , nRMSE : 1.001669849690916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1876/1876 [03:51<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch : 3 , loss : 0.8101093808781152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:35<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch : 3 , nRMSE : 1.0005146902750246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1215/1876 [03:02<01:39,  6.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m y_pred = pfactor_regressor_model(x)\n\u001b[32m     56\u001b[39m loss = loss_f(y_pred , y)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m cumulative_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m optimizer.zero_grad()\n\u001b[32m     59\u001b[39m loss.backward()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "\n",
    "pfactor_regressor_model = Vit_EEG_PF_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    #{\"params\": pfactor_regressor_model.encoder.embedding.parameters(), \"lr\": 8e-5},\n",
    "    #{\"params\": pfactor_regressor_model.encoder.transformer_encoder.parameters(), \"lr\": 3e-4},\n",
    "    {\"params\": pfactor_regressor_model.regressor.parameters(), \"lr\": 2e-4},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "loss_f = nn.MSELoss()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(f\"train nRMSE : {nrmse_over_data(pfactor_regressor_model  , c2_train_loader ,device)}\")\n",
    "#print(f\"test nRMSE : {nrmse_over_data(pfactor_regressor_model , c2_test_loader ,device)}\")\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\"\"\"print(\"before training\")\n",
    "nrmse_over_test = nrmse_over_data(pfactor_regressor_model , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {nrmse_over_test}\")\n",
    "if nrmse_over_test < best_rnmse:\n",
    "    print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "    best_rnmse = nrmse_over_test\n",
    "    torch.save(pfactor_regressor_model.state_dict(), best_model_path)\n",
    "    torch.save(best_rnmse, best_rnmse_path)\n",
    "\"\"\"\n",
    "for epoch in range(epochs):\n",
    "    pfactor_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    index = 0\n",
    "    for  batch in tqdm(c2_train_loader) :\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = pfactor_regressor_model(x)\n",
    "        loss = loss_f(y_pred , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        index+=1\n",
    "\n",
    "        #if index%100 == 0:\n",
    "    print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/index}\")\n",
    "    cumulative_loss = 0\n",
    "    index=0\n",
    "        \n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1}  , nRMSE : {nrmse_train}\")\n",
    "    \"\"\"    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(c2_test_loader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\"\"\"\n",
    "    pfactor_regressor_model.eval()\n",
    "    nrmse_over_test = nrmse_over_data(pfactor_regressor_model , c2_test_loader ,device)\n",
    "    print(f\"test epoch : {epoch +1} , nRMSE : {nrmse_over_test}\")\n",
    "    if nrmse_over_test < best_rnmse:\n",
    "        print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "        best_rnmse = nrmse_over_test\n",
    "        torch.save(pfactor_regressor_model.state_dict(), best_model_path)\n",
    "        torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:37<00:00,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 0.9958256953662974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = Vit_EEG_Encoder().to(device)\n",
    "pfactor_regressor_model = Vit_EEG_PF_Decoder(encoder , fine_tune_encoder=False).to(device)\n",
    "if os.path.exists(best_model_path):\n",
    "    pfactor_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "test_nrmse = nrmse_over_data(pfactor_regressor_model  , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {test_nrmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958256953662974"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nrmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overfitting check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loaded with rnmse : 1.0000151774052566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 52/100000 [00:06<3:23:58,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6654376983642578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/100000 [00:12<3:20:26,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7554861307144165\n",
      "train epoch :  , loss : 1.188334839642048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 150/100000 [00:18<3:52:56,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.9157272577285767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 202/100000 [00:25<3:19:53,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6883599758148193\n",
      "train epoch :  , loss : 0.4955803567171097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 250/100000 [00:31<3:54:45,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5177764892578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 302/100000 [00:37<3:19:24,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6647286415100098\n",
      "train epoch :  , loss : 0.4167928493022919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 352/100000 [00:43<3:20:33,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8179818391799927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 400/100000 [00:49<3:58:10,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5922091007232666\n",
      "train epoch :  , loss : 0.404636595249176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 450/100000 [00:56<4:03:01,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7616353034973145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/100000 [01:02<4:07:47,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7367831468582153\n",
      "train epoch :  , loss : 0.39563126146793365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 550/100000 [01:09<4:05:20,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5933514833450317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 600/100000 [01:15<3:55:11,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8882521390914917\n",
      "train epoch :  , loss : 0.38173151820898055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 652/100000 [01:22<3:17:59,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7258011102676392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 700/100000 [01:28<3:53:48,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7813329696655273\n",
      "train epoch :  , loss : 0.3965982636809349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 752/100000 [01:34<3:19:19,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.946253776550293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 800/100000 [01:40<3:52:37,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.3995035886764526\n",
      "train epoch :  , loss : 0.39077990740537644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 850/100000 [01:46<3:53:01,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6915987730026245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 900/100000 [01:52<3:52:38,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7369526624679565\n",
      "train epoch :  , loss : 0.36744388729333877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 950/100000 [01:59<3:52:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.669135332107544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/100000 [02:05<3:53:02,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5485835075378418\n",
      "train epoch :  , loss : 0.3713801616430283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1050/100000 [02:11<3:53:29,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8623570203781128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1100/100000 [02:17<3:53:30,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6752097606658936\n",
      "train epoch :  , loss : 0.35598811864852903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1150/100000 [02:23<3:53:13,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5619451999664307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1200/100000 [02:29<3:53:03,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7277448177337646\n",
      "train epoch :  , loss : 0.3372748690843582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1250/100000 [02:36<3:53:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7444112300872803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1300/100000 [02:42<3:52:09,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6556552648544312\n",
      "train epoch :  , loss : 0.34455414831638337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1350/100000 [02:48<3:50:11,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5556594133377075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1400/100000 [02:54<3:53:40,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6959800720214844\n",
      "train epoch :  , loss : 0.34096224218606946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1450/100000 [03:00<3:53:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8155763149261475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1500/100000 [03:07<3:51:56,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6545417308807373\n",
      "train epoch :  , loss : 0.3363021492958069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1550/100000 [03:13<3:51:02,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7735693454742432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1602/100000 [03:19<3:17:28,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.685703992843628\n",
      "train epoch :  , loss : 0.3317495137453079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1652/100000 [03:25<3:16:42,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6822046041488647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1700/100000 [03:31<3:51:57,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.557279109954834\n",
      "train epoch :  , loss : 0.32252987325191496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1750/100000 [03:38<3:51:14,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7146492004394531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1800/100000 [03:44<3:50:34,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8631792068481445\n",
      "train epoch :  , loss : 0.3244608411192894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1850/100000 [03:50<3:50:32,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7094513177871704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1900/100000 [03:56<3:51:24,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6958616971969604\n",
      "train epoch :  , loss : 0.3182846572995186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1950/100000 [04:02<3:50:06,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7665950059890747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2000/100000 [04:09<3:50:49,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5983905792236328\n",
      "train epoch :  , loss : 0.31506547272205354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2050/100000 [04:15<3:51:05,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7140189409255981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2100/100000 [04:21<3:48:27,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6434144973754883\n",
      "train epoch :  , loss : 0.3086600399017334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2150/100000 [04:27<3:57:16,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7019011974334717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2200/100000 [04:34<3:51:07,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.667975664138794\n",
      "train epoch :  , loss : 0.30908914655447006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2252/100000 [04:40<3:24:24,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.648815631866455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2300/100000 [04:46<3:55:23,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.586353063583374\n",
      "train epoch :  , loss : 0.296323239505291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2352/100000 [04:53<3:19:14,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.767605185508728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2400/100000 [04:59<3:53:46,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.634353756904602\n",
      "train epoch :  , loss : 0.30459113091230394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2450/100000 [05:05<3:54:15,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.631405234336853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2502/100000 [05:12<3:20:09,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5733141899108887\n",
      "train epoch :  , loss : 0.2943501777946949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2550/100000 [05:18<3:52:32,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.765359878540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2602/100000 [05:25<3:19:53,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.53056001663208\n",
      "train epoch :  , loss : 0.3014505051076412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2652/100000 [05:31<3:18:34,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7286155223846436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2700/100000 [05:37<3:56:44,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.660512089729309\n",
      "train epoch :  , loss : 0.2877578201889992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2750/100000 [05:43<3:55:54,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5808327198028564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2800/100000 [05:50<3:56:44,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.602076530456543\n",
      "train epoch :  , loss : 0.2872641675174236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2850/100000 [05:56<3:52:21,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7140713930130005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2900/100000 [06:03<3:50:38,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.716802954673767\n",
      "train epoch :  , loss : 0.28649466365575793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2950/100000 [06:09<3:56:24,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8593124151229858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3000/100000 [06:16<3:58:53,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.839946985244751\n",
      "train epoch :  , loss : 0.28153896138072015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3050/100000 [06:22<3:59:40,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.664186954498291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3102/100000 [06:29<3:23:08,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7147259712219238\n",
      "train epoch :  , loss : 0.27070811882615087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3152/100000 [06:35<3:17:17,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7966872453689575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3202/100000 [06:42<3:17:35,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6379101276397705\n",
      "train epoch :  , loss : 0.27692636623978617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3250/100000 [06:48<3:50:07,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.644790768623352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3300/100000 [06:54<3:49:11,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7150427103042603\n",
      "train epoch :  , loss : 0.2705675412714481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3352/100000 [07:00<3:16:20,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7911336421966553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3400/100000 [07:06<3:49:07,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.617485523223877\n",
      "train epoch :  , loss : 0.26961606830358503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3450/100000 [07:13<3:47:33,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7907500267028809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3502/100000 [07:19<3:20:20,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6624116897583008\n",
      "train epoch :  , loss : 0.2655559065937996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3552/100000 [07:26<3:19:36,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.786391019821167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3600/100000 [07:32<3:54:40,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.569275140762329\n",
      "train epoch :  , loss : 0.2675901375710964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3650/100000 [07:38<3:56:00,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6153361797332764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3702/100000 [07:45<3:16:41,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6664198637008667\n",
      "train epoch :  , loss : 0.2616743257641792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3750/100000 [07:51<3:52:36,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8457833528518677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3802/100000 [07:57<3:14:08,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.8245126008987427\n",
      "train epoch :  , loss : 0.2664396171271801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3850/100000 [08:03<3:56:40,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7283196449279785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3900/100000 [08:10<3:53:37,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.882623553276062\n",
      "train epoch :  , loss : 0.25401645749807356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3950/100000 [08:16<3:53:59,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7277135848999023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4000/100000 [08:22<3:54:24,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5593514442443848\n",
      "train epoch :  , loss : 0.25546120539307593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4050/100000 [08:29<3:56:20,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7284610271453857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4102/100000 [08:35<3:19:38,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7040189504623413\n",
      "train epoch :  , loss : 0.2526117366552353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4150/100000 [08:42<3:52:19,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.4989113807678223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4200/100000 [08:48<3:54:06,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7148412466049194\n",
      "train epoch :  , loss : 0.2500940996408463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4250/100000 [08:55<3:54:01,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7500278949737549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4300/100000 [09:01<3:54:07,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5819860696792603\n",
      "train epoch :  , loss : 0.24830472707748413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4352/100000 [09:08<3:18:28,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7126168012619019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4400/100000 [09:14<3:54:40,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7431550025939941\n",
      "train epoch :  , loss : 0.245592972189188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4450/100000 [09:20<3:52:33,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6735808849334717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4502/100000 [09:27<3:21:34,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.608973741531372\n",
      "train epoch :  , loss : 0.24973857909440994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4550/100000 [09:33<3:52:41,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6742289066314697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4600/100000 [09:40<3:53:25,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7578144073486328\n",
      "train epoch :  , loss : 0.24515712529420852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4652/100000 [09:46<3:12:38,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.541129231452942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4702/100000 [09:53<3:14:18,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7371094226837158\n",
      "train epoch :  , loss : 0.24431461364030838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4750/100000 [09:59<3:46:54,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.718785047531128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4800/100000 [10:05<3:43:33,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.849604845046997\n",
      "train epoch :  , loss : 0.24248696967959404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4850/100000 [10:11<3:43:29,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7721720933914185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4902/100000 [10:17<3:11:08,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6068720817565918\n",
      "train epoch :  , loss : 0.24276169374585152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4950/100000 [10:24<3:45:30,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5961589813232422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5000/100000 [10:30<3:43:41,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7431325912475586\n",
      "train epoch :  , loss : 0.24085130989551545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5050/100000 [10:36<3:43:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6645491123199463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5102/100000 [10:42<3:13:31,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.639453649520874\n",
      "train epoch :  , loss : 0.23769626006484032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5152/100000 [10:49<3:17:31,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7601526975631714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5202/100000 [10:55<3:18:21,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7324045896530151\n",
      "train epoch :  , loss : 0.234760100543499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5252/100000 [11:02<3:17:03,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6520726680755615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5300/100000 [11:08<3:51:05,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7969774007797241\n",
      "train epoch :  , loss : 0.2361370025575161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5352/100000 [11:15<3:16:52,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.5898358821868896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5402/100000 [11:21<3:15:50,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6592048406600952\n",
      "train epoch :  , loss : 0.23410477951169015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5450/100000 [11:27<3:47:22,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6336917877197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5500/100000 [11:34<3:45:04,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7404145002365112\n",
      "train epoch :  , loss : 0.22961560547351836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5550/100000 [11:40<3:45:23,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6710330247879028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5600/100000 [11:46<3:49:57,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6890456676483154\n",
      "train epoch :  , loss : 0.2301808676123619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5650/100000 [11:53<3:50:04,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6923019886016846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5700/100000 [11:59<3:50:13,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.7510689496994019\n",
      "train epoch :  , loss : 0.2318277595937252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5752/100000 [12:05<3:14:46,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual : 1.7020000219345093 , pred : 1.6416987180709839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5772/100000 [12:08<3:20:27,  7.83it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model = Vit_EEG_Encoder().to(device)\n",
    "\n",
    "save_path = \"last_checkpoint.pt\"\n",
    "if os.path.isfile(save_path):\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "\n",
    "pfactor_regressor_model = Vit_EEG_PF_Decoder(model , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": pfactor_regressor_model.encoder.embedding.parameters(), \"lr\": 5e-5},\n",
    "    {\"params\": pfactor_regressor_model.encoder.transformer_encoder.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": pfactor_regressor_model.regressor.parameters(), \"lr\": 5e-4},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "epochs = 6\n",
    "total_steps= len(c2_train_loader) * epochs\n",
    "warmup_steps = 100\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()  \n",
    "def nrmse_loss(pred, target, eps=1e-8):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    n = target.numel()\n",
    "\n",
    "    se_sum = torch.sum((pred - target) ** 2)\n",
    "    sum_y  = torch.sum(target)\n",
    "    sum_y2 = torch.sum(target ** 2)\n",
    "\n",
    "    rmse = torch.sqrt(se_sum / n)\n",
    "\n",
    "    var  = (sum_y2 / n) - (sum_y / n) ** 2\n",
    "    std  = torch.sqrt(torch.clamp(var, min=eps))\n",
    "\n",
    "    return rmse / std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"challange2_best_model.pt\"\n",
    "best_rnmse_path = \"challange2_best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\"\"\"print(\"before training\")\n",
    "nrmse_over_test = nrmse_over_data(pfactor_regressor_model , c2_test_loader ,device)\n",
    "print(f\"test nRMSE : {nrmse_over_test}\")\n",
    "if nrmse_over_test < best_rnmse:\n",
    "    print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "    best_rnmse = nrmse_over_test\n",
    "    torch.save(pfactor_regressor_model.state_dict(), best_model_path)\n",
    "    torch.save(best_rnmse, best_rnmse_path)\n",
    "\"\"\"\n",
    "first_batch = next(iter(c2_train_loader))\n",
    "pfactor_regressor_model.train()\n",
    "cumulative_loss = 0\n",
    "index = 0\n",
    "x , y = first_batch\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "for  i in tqdm(range(100000)):\n",
    "\n",
    "    y_pred = pfactor_regressor_model(x)\n",
    "    loss = nrmse_loss(y_pred , y)\n",
    "    cumulative_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    index+=1\n",
    "    if index%50 == 0:\n",
    "        print(f\"actual : {y[0].item()} , pred : {y_pred[0].item()}\")\n",
    "    if index%100 == 0:\n",
    "        print(f\"train epoch :  , loss : {cumulative_loss/100}\")\n",
    "        cumulative_loss = 0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(best_model_path):\n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "final_r5_test =pairs_to_fast_loading_shards([] ,\"final_r5_test\" , shard_size)\n",
    "r5_test_ccd_data = EEGDataset(final_r5_test ,  shard_size)\n",
    "\n",
    "r5_test_ccd_dataloader = DataLoader(r5_test_ccd_data , batch_size=batch_size , shuffle=False )\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , r5_test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "FINAL_DATA_DIR = Path(r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\final_kaggle_data\")\n",
    "\n",
    "def make_subject_splits(data_dir: Path, seed: int = 42,\n",
    "                        test_size: float = 0.10, val_size: float = 0.10):\n",
    "    \"\"\"\n",
    "    Split subjects listed in full_meta_data.csv (column 'participant_id')\n",
    "    into train/val/test groups.\n",
    "    \"\"\"\n",
    "    meta_path = data_dir / \"full_meta_data.csv\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "\n",
    "    df = pd.read_csv(meta_path)\n",
    "    if \"participant_id\" not in df.columns:\n",
    "        raise ValueError(\"full_meta_data.csv must contain 'participant_id' column\")\n",
    "\n",
    "    subjects = sorted(df[\"participant_id\"].unique())\n",
    "\n",
    "    # first split off temp (val + test)\n",
    "    train_subj, temp_subj = train_test_split(\n",
    "        subjects,\n",
    "        test_size=(test_size + val_size),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # now split temp into val and test\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    val_subj, test_subj = train_test_split(\n",
    "        temp_subj,\n",
    "        test_size=(1 - val_ratio),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_subj,\n",
    "        \"val\": val_subj,\n",
    "        \"test\": test_subj\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    return splits\n",
    "splits = make_subject_splits(FINAL_DATA_DIR, seed=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "class EEGShardDataset_FULL(Dataset):\n",
    "    def __init__(self, data_dir: Path, subject_list: list[str]):\n",
    "        \"\"\"\n",
    "        EEG dataset loader for preprocessed shards with fast metadata lookup.\n",
    "        Builds an index for fast window access and loads full_meta_data.csv in memory.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.shards_dir = self.data_dir / \"processed_shards\"\n",
    "        self.subject_list = set(subject_list)\n",
    "\n",
    "        # ---------------- Load metadata ----------------\n",
    "        meta_path = self.data_dir / \"full_meta_data.csv\"\n",
    "        if not meta_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "        if \"participant_id\" not in meta_df.columns:\n",
    "            raise ValueError(\"full_meta_data.csv must contain 'participant_id' column\")\n",
    "\n",
    "        # convert to dict for O(1) access\n",
    "        self.meta_dict = meta_df.set_index(\"participant_id\").to_dict(orient=\"index\")\n",
    "        print(f\"✅ Loaded metadata for {len(self.meta_dict)} participants\")\n",
    "\n",
    "        # ---------------- Scan shards ----------------\n",
    "        self.files = []\n",
    "        self.starts = []\n",
    "        self.n_windows = []\n",
    "        total_windows = 0\n",
    "\n",
    "        all_files = sorted(\n",
    "            f for f in os.listdir(self.shards_dir) if f.endswith(\".npy\")\n",
    "        )\n",
    "\n",
    "        for fname in tqdm(all_files, desc=\"Indexing shards\"):\n",
    "            # only include shards belonging to our selected subjects\n",
    "            if not any(subj in fname for subj in self.subject_list):\n",
    "                continue\n",
    "\n",
    "            fpath = self.shards_dir / fname\n",
    "            try:\n",
    "                arr = np.load(fpath, mmap_mode=\"r\")\n",
    "                n_w = arr.shape[0]\n",
    "                self.files.append(fpath)\n",
    "                self.starts.append(total_windows)\n",
    "                self.n_windows.append(n_w)\n",
    "                total_windows += n_w\n",
    "                del arr\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping {fname}: {e}\")\n",
    "\n",
    "        self.total_windows = total_windows\n",
    "        print(f\"✅ Indexed {len(self.files)} shards ({self.total_windows} windows total)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_windows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Locate which shard and local window correspond to the given global index.\n",
    "        Returns (window_data, info_dict).\n",
    "        \"\"\"\n",
    "        # locate shard and local index\n",
    "        shard_idx = bisect.bisect_right(self.starts, index) - 1\n",
    "        local_idx = index - self.starts[shard_idx]\n",
    "        shard_path = self.files[shard_idx]\n",
    "\n",
    "        # load window\n",
    "        data = np.load(shard_path, mmap_mode=\"r\")[local_idx]  # (channels, time)\n",
    "\n",
    "        # parse filename: release_task_subject_run-{run}_shard-{id}.npy\n",
    "        fname = shard_path.stem\n",
    "        parts = fname.split(\"_\")\n",
    "        subject = parts[2]\n",
    "\n",
    "        info = {\n",
    "            \"release\": parts[0],\n",
    "            \"task\": parts[1],\n",
    "            \"subject\": subject,\n",
    "            \"run\": parts[3]\n",
    "        }\n",
    "\n",
    "        # attach subject metadata (age, sex, etc.)\n",
    "        if subject in self.meta_dict:\n",
    "            info.update(self.meta_dict[subject])\n",
    "\n",
    "        return data, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded metadata for 1259 participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing shards: 100%|██████████| 11033/11033 [00:50<00:00, 220.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 8917 shards (1327695 windows total)\n",
      "1327695\n",
      "(129, 200)\n",
      "{'release': 'R1', 'task': 'DespicableMe', 'subject': 'sub-NDARCW071AU5', 'run': 'run-1', 'Unnamed: 0': 19, 'release_number': 'R1', 'sex': 'M', 'age': 9.7374, 'ehq_total': 45.62, 'commercial_use': 'Yes', 'full_pheno': 'Yes', 'p_factor': 0.313, 'attention': 0.483, 'internalizing': 0.436, 'externalizing': 1.185, 'RestingState': 'available', 'DespicableMe': 'available', 'FunwithFractals': 'available', 'ThePresent': 'available', 'DiaryOfAWimpyKid': 'available', 'contrastChangeDetection_1': 'available', 'contrastChangeDetection_2': 'available', 'contrastChangeDetection_3': 'available', 'surroundSupp_1': 'available', 'surroundSupp_2': 'available', 'seqLearning6target': 'available', 'seqLearning8target': 'unavailable', 'symbolSearch': 'caution'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ds = EEGShardDataset_FULL(FINAL_DATA_DIR, splits[\"train\"])\n",
    "\n",
    "print(len(train_ds))  \n",
    "x, info = train_ds[1234]\n",
    "print(x.shape)        \n",
    "print(info)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_subject_splits(data_dir: Path, seed: int = 42,\n",
    "                        test_size: float = 0.10, val_size: float = 0.10):\n",
    "    \"\"\"\n",
    "    Split subjects listed in full_meta_data.csv (column 'participant_id')\n",
    "    into train/val/test groups.\n",
    "    \"\"\"\n",
    "    meta_path = data_dir / \"full_meta_data.csv\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "\n",
    "    df = pd.read_csv(meta_path)\n",
    "    if \"participant_id\" not in df.columns:\n",
    "        raise ValueError(\"full_meta_data.csv must contain 'participant_id' column\")\n",
    "\n",
    "    subjects = sorted(df[\"participant_id\"].unique())\n",
    "\n",
    "    # first split off temp (val + test)\n",
    "    train_subj, temp_subj = train_test_split(\n",
    "        subjects,\n",
    "        test_size=(test_size + val_size),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # now split temp into val and test\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    val_subj, test_subj = train_test_split(\n",
    "        temp_subj,\n",
    "        test_size=(1 - val_ratio),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_subj,\n",
    "        \"val\": val_subj,\n",
    "        \"test\": test_subj\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, bisect\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EEGShardDataset_FULL(Dataset):\n",
    "    \"\"\"\n",
    "    Shards: each file is [n_base, C=129, W=200].\n",
    "    We create *virtual* windows of length `window_size` with step `stride`\n",
    "    inside each shard (NO cross-shard windows). Return is SAME as original:\n",
    "      -> (np.ndarray [C, window_size], info: dict)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        subject_list: List[str],\n",
    "        window_size: int = 200,\n",
    "        stride: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.shards_dir = self.data_dir / \"processed_shards\"\n",
    "        self.subject_list = set(subject_list)\n",
    "        self.window_size = int(window_size)\n",
    "        self.stride = int(stride)\n",
    "\n",
    "        # ---- metadata ----\n",
    "        meta_path = self.data_dir / \"full_meta_data.csv\"\n",
    "        if not meta_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "        if \"participant_id\" not in meta_df.columns:\n",
    "            raise ValueError(\"full_meta_data.csv must contain 'participant_id' column\")\n",
    "        self.meta_dict: Dict[str, Dict] = meta_df.set_index(\"participant_id\").to_dict(orient=\"index\")\n",
    "        print(f\"✅ Loaded metadata for {len(self.meta_dict)} participants\")\n",
    "\n",
    "        # ---- discover shards filtered by subject ----\n",
    "        all_files = sorted(f for f in os.listdir(self.shards_dir) if f.endswith(\".npy\"))\n",
    "        self.shard_paths: List[Path] = []\n",
    "        for fname in all_files:\n",
    "            if any(subj in fname for subj in self.subject_list):\n",
    "                self.shard_paths.append(self.shards_dir / fname)\n",
    "        if not self.shard_paths:\n",
    "            raise RuntimeError(\"No shard files after subject filtering.\")\n",
    "\n",
    "        # ---- index virtual windows per shard ----\n",
    "        self.shard_meta = []          # dicts: path, n_base, C, W, n_virtual, fname\n",
    "        self.prefix_virtual = [0]     # prefix sum of n_virtual\n",
    "        total_virtual = 0\n",
    "\n",
    "        for path in tqdm(self.shard_paths, desc=\"Indexing shards (strided)\"):\n",
    "            arr = np.load(path, mmap_mode=\"r\")           # [n_base, C, W]\n",
    "            if arr.ndim != 3:\n",
    "                raise ValueError(f\"{path.name}: expected 3D [n, C, W], got {arr.shape}\")\n",
    "            n_base, C, W = arr.shape\n",
    "            if C != 129:\n",
    "                raise ValueError(f\"{path.name}: C={C}, expected 129\")\n",
    "            total_samples = n_base * W\n",
    "            if total_samples >= self.window_size:\n",
    "                n_virtual = ((total_samples - self.window_size) // self.stride) + 1\n",
    "            else:\n",
    "                n_virtual = 0\n",
    "            self.shard_meta.append({\n",
    "                \"path\": path,\n",
    "                \"n_base\": int(n_base),\n",
    "                \"C\": int(C),\n",
    "                \"W\": int(W),\n",
    "                \"n_virtual\": int(n_virtual),\n",
    "                \"fname\": path.stem,\n",
    "            })\n",
    "            total_virtual += n_virtual\n",
    "            self.prefix_virtual.append(total_virtual)\n",
    "\n",
    "        self.total_virtual = total_virtual\n",
    "        if self.total_virtual == 0:\n",
    "            raise RuntimeError(\"No virtual windows possible with given window_size/stride.\")\n",
    "        print(f\"✅ Indexed {len(self.shard_meta)} shards -> {self.total_virtual} virtual windows (stride={self.stride})\")\n",
    "\n",
    "        # tiny cache\n",
    "        self._cache_key = None\n",
    "        self._cache_arr = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_virtual\n",
    "\n",
    "    def _locate(self, global_index: int) -> Tuple[int, int]:\n",
    "        s = bisect.bisect_right(self.prefix_virtual, global_index) - 1\n",
    "        local_idx = global_index - self.prefix_virtual[s]\n",
    "        return s, local_idx\n",
    "\n",
    "    def _load_shard(self, shard_idx: int):\n",
    "        path = self.shard_meta[shard_idx][\"path\"]\n",
    "        key = str(path)\n",
    "        if self._cache_key != key:\n",
    "            self._cache_arr = np.load(path, mmap_mode=\"r\")  # [n_base, C, W]\n",
    "            self._cache_key = key\n",
    "        return self._cache_arr\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index >= self.total_virtual:\n",
    "            raise IndexError\n",
    "\n",
    "        shard_idx, local_idx = self._locate(index)\n",
    "        meta = self.shard_meta[shard_idx]\n",
    "        shard = self._load_shard(shard_idx)\n",
    "\n",
    "        n_base, C, W = meta[\"n_base\"], meta[\"C\"], meta[\"W\"]\n",
    "\n",
    "        # start/end in shard's concatenated timeline\n",
    "        start = local_idx * self.stride\n",
    "        end   = start + self.window_size\n",
    "\n",
    "        # stitch within shard (may span adjacent base windows IN THE SAME SHARD)\n",
    "        base_idx = start // W\n",
    "        offset   = start % W\n",
    "        remaining = self.window_size\n",
    "        pieces = []\n",
    "        cur_b = base_idx\n",
    "        cur_off = offset\n",
    "\n",
    "        while remaining > 0:\n",
    "            take = min(remaining, W - cur_off)\n",
    "            if cur_b >= n_base:\n",
    "                piece = np.zeros((C, take), dtype=np.float32)\n",
    "            else:\n",
    "                piece = shard[cur_b, :, cur_off:cur_off + take]  # [C, take]\n",
    "            pieces.append(piece)\n",
    "            remaining -= take\n",
    "            cur_b += 1\n",
    "            cur_off = 0\n",
    "\n",
    "        x = np.concatenate(pieces, axis=1)                 # [C, window_size]\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "\n",
    "        fname = meta[\"fname\"]\n",
    "        parts = fname.split(\"_\")\n",
    "        subject = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "        info = {\n",
    "            \"release\": parts[0] if len(parts) > 0 else \"\",\n",
    "            \"task\":    parts[1] if len(parts) > 1 else \"\",\n",
    "            \"subject\": subject,\n",
    "            \"run\":     parts[3] if len(parts) > 3 else \"\",\n",
    "        }\n",
    "        if subject in self.meta_dict:\n",
    "            info.update(self.meta_dict[subject])\n",
    "\n",
    "        return x, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded metadata for 1259 participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing shards (strided): 100%|██████████| 8917/8917 [00:04<00:00, 1912.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 8917 shards -> 2646473 virtual windows (stride=100)\n",
      "✅ Loaded metadata for 1259 participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing shards (strided): 100%|██████████| 1036/1036 [00:00<00:00, 1908.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 1036 shards -> 310908 virtual windows (stride=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FINAL_DATA_DIR = Path(r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\final_kaggle_data\")\n",
    "\n",
    "# subject splits\n",
    "splits = make_subject_splits(FINAL_DATA_DIR, seed=1337)\n",
    "\n",
    "# datasets\n",
    "train_ds = EEGShardDataset_FULL(FINAL_DATA_DIR, splits[\"train\"])\n",
    "val_ds   = EEGShardDataset_FULL(FINAL_DATA_DIR, splits[\"val\"])\n",
    "\n",
    "# loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=False, persistent_workers=False,\n",
    "        drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=False, persistent_workers=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 471867/661618 [48:12<17:44, 178.22it/s]  "
     ]
    }
   ],
   "source": [
    "keys =['release', 'task', 'subject', 'run', 'Unnamed: 0', 'release_number', 'sex', 'age', 'ehq_total', 'commercial_use', 'full_pheno', 'p_factor', 'attention', 'internalizing', 'externalizing', 'RestingState', 'DespicableMe', 'FunwithFractals', 'ThePresent', 'DiaryOfAWimpyKid', 'contrastChangeDetection_1', 'contrastChangeDetection_2', 'contrastChangeDetection_3', 'surroundSupp_1', 'surroundSupp_2', 'seqLearning6target', 'seqLearning8target', 'symbolSearch']\n",
    "for x , info in tqdm(train_loader):\n",
    "    if list(info.keys()) != keys :\n",
    "        print(info)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=384, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3458 [00:00<?, ?it/s]c:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "  0%|          | 0/3458 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_dataloader):\n",
    "    x , info = batch\n",
    "    print(len(info[\"subject\"]))\n",
    "    print(len(set( info[\"subject\"])))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def triplet_loss (x, subjects,    margin = 0.2):\n",
    "\n",
    "    subject_dict  = {}\n",
    "    subject_repeating = set()\n",
    "    losses = []\n",
    "\n",
    "    for index , subject in enumerate(subjects):\n",
    "        if subject in subject_dict:\n",
    "            subject_dict[subject].append(x[index])\n",
    "            subject_repeating.add(subject)\n",
    "        else:\n",
    "            subject_dict[subject] = [x[index]]\n",
    "        \n",
    "\n",
    "    accumulated_loss = 0\n",
    "    for subject in subject_repeating:\n",
    "        original = subject_dict[subject][0]\n",
    "        positive  = subject_dict[subject][1]\n",
    "        random_negative_subject = random.choice(subjects)\n",
    "        if random_negative_subject == subject:\n",
    "            random_negative_subject = random.choice(subjects)\n",
    "\n",
    "        negative = subject_dict[random_negative_subject ][0]\n",
    "        value = (torch.cosine_similarity(original,positive,dim=0) - torch.cosine_similarity(original  , negative,dim=0)+ margin)\n",
    "\n",
    "        loss = torch.clamp(value ,0)\n",
    "        accumulated_loss+=loss\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=device)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss_fast(x: torch.Tensor, subjects, margin: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (B, D) embeddings (on GPU/CPU)\n",
    "    subjects: list[str] length B (class IDs)\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    B = x.size(0)\n",
    "    if B <= 1:\n",
    "        return x.new_tensor(0.0)\n",
    "\n",
    "    # 1) map subject strings -> integer labels (vectorizable & cheap)\n",
    "    # dict.fromkeys preserves order and is faster than set+list\n",
    "    label_map = {s: i for i, s in enumerate(dict.fromkeys(subjects))}\n",
    "    labels = torch.tensor([label_map[s] for s in subjects], device=device)\n",
    "\n",
    "    # 2) L2-normalize once; cosine similarity == dot product\n",
    "    x = F.normalize(x, dim=1)\n",
    "\n",
    "    # 3) pairwise cosine similarities in one matmul\n",
    "    # sim[i, j] = cos(x_i, x_j)\n",
    "    sim = x @ x.T  # (B, B)\n",
    "\n",
    "    # 4) masks\n",
    "    same = labels.unsqueeze(0) == labels.unsqueeze(1)     # (B, B)\n",
    "    eye = torch.eye(B, dtype=torch.bool, device=device)\n",
    "    pos_mask = same & ~eye                                # exclude self\n",
    "    neg_mask = ~same\n",
    "\n",
    "    pos_counts = pos_mask.sum(dim=1)          # #positives per anchor\n",
    "    neg_counts = neg_mask.sum(dim=1)          # #negatives per anchor\n",
    "    valid = (pos_counts > 0) & (neg_counts > 0)\n",
    "    if not valid.any():\n",
    "        return x.new_tensor(0.0)\n",
    "\n",
    "    # 5) average positive similarity per anchor\n",
    "    pos_sum = (sim * pos_mask).sum(dim=1)\n",
    "    sim_ap = pos_sum / pos_counts.clamp(min=1)\n",
    "\n",
    "    # 6) hardest negative (largest similarity among negatives)\n",
    "    sim_neg = sim.masked_fill(~neg_mask, -1e9)\n",
    "    sim_an = sim_neg.max(dim=1).values\n",
    "\n",
    "    # 7) margin triplet loss\n",
    "    loss = F.relu(sim_ap - sim_an + margin)\n",
    "\n",
    "    return loss[valid].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(384, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1959)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss(x ,subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5015623569488525\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    triplet_loss(x, subjects)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the old best encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N , emb_dim)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x , None\n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "    def add_gaussian_noise(self,x, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Additive Gaussian noise.\n",
    "        \"\"\"\n",
    "        return x + sigma * torch.randn_like(x)\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x ,_= self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  , mask_index =None):\n",
    "        emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "        transformed = self.transformer_encoder(emb_x)\n",
    "        result = self.decoder(transformed)\n",
    "        if targets is not None:\n",
    "            return result , targets\n",
    "        return result\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, dim)) \n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim)\n",
    "        B = x.size(0)\n",
    "        q = self.query.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        out, _ = self.attn(q, x, x)       # attend over all tokens (b 1 n )* (b n d) = (b 1 d)\n",
    "        return out.squeeze(1)             # (B, dim)\n",
    "    \n",
    "class TemporalConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.transpose(1, 2)\n",
    "\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "class Vit_EEG_RT_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, c_dim=129, t_dim=200 , fine_tune_encoder = False):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.encoder = encoder\n",
    "        if fine_tune_encoder :\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.pool = AttentionPool(512, num_heads=4)\n",
    "        self.temporal_conv = TemporalConv(512, kernel_size=3, stride=1, padding=1)\n",
    "        self.alpha = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.beta = nn.Parameter(torch.randn(1 ,512))\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def merge(self, x, y):\n",
    "        return self.alpha * x + self.beta * y\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.extract_features(x)  # (B, N, 512)\n",
    "        a = self.pool(x)                      # (B, 512)\n",
    "\n",
    "        b = self.temporal_conv(x)             # (B, 512)\n",
    "        x = self.merge(a, b)                  # (B, 512)\n",
    "        x = self.regressor(x)                 # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:13<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test nRMSE : 0.9083325734208045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 128/179 [00:12<00:04, 10.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloaded model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest nRMSE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrmse_over_data(rt_regressor_model\u001b[38;5;250m \u001b[39m,\u001b[38;5;250m \u001b[39mtest_ccd_dataloader,device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval nrmse : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrt_regressor_model\u001b[49m\u001b[38;5;250;43m  \u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mval_ccd_dataloader\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain nrmse : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnrmse_over_data(rt_regressor_model\u001b[38;5;250m \u001b[39m,\u001b[38;5;250m \u001b[39mtrain_ccd_dataloader\u001b[38;5;250m \u001b[39m,\u001b[38;5;250m \u001b[39mdevice)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m     17\u001b[39m y_pred = model(x).view_as(y)\n\u001b[32m     18\u001b[39m diff = y_pred - y\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m se_sum += \u001b[43mdiff\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m sum_y  += y.sum().item()\n\u001b[32m     22\u001b[39m sum_y2 += y.pow(\u001b[32m2\u001b[39m).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = Vit_EEG_Encoder().to(device)\n",
    "best_model__path = os.path.join(MODELS_AND_CHECKPOINTS_PATH , \"old_best_challange1_model.pt\")\n",
    "\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "if os.path.exists(best_model__path):\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model__path))\n",
    "    print(f\"loaded model\")\n",
    "\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader,device)}\")\n",
    "print(f\"val nrmse : {nrmse_over_data(rt_regressor_model  , val_ccd_dataloader , device)}\")\n",
    "print(f\"train nrmse : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader , device)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking which check point is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder =  Vit_EEG_Encoder().to(device)\n",
    "last_checkpoint_path = r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\history\\last_checkpoint.pt\"\n",
    "ckpt = torch.load(last_checkpoint_path)\n",
    "\n",
    "encoder.load_state_dict(ckpt[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "best model loaded with rnmse : 0.9482655338027726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 83/1453 [00:12<03:24,  6.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m rt_regressor_model.train()\n\u001b[32m     43\u001b[39m cumulative_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ccd_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     24\u001b[39m raw = X[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m     25\u001b[39m rt = Y[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tensor_raw = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     27\u001b[39m tensor_rt = torch.tensor(rt , dtype=torch.float32).to(device)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_raw , tensor_rt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rt_regressor_model = Vit_EEG_RT_Decoder(encoder , fine_tune_encoder=True).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": rt_regressor_model.encoder.parameters(), \"lr\": 7e-5},    \n",
    "    {\"params\": rt_regressor_model.pool.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.regressor.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.temporal_conv.parameters(), \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.alpha, \"lr\": 2e-4},\n",
    "    {\"params\": rt_regressor_model.beta, \"lr\": 2e-4},\n",
    "], weight_decay=2e-2)\n",
    "\n",
    "epochs = 4\n",
    "total_steps= len(train_ccd_dataloader) * epochs\n",
    "warmup_steps = 300\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(1, warmup_steps) \n",
    "    progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))  \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loss_f = nn.MSELoss()  \n",
    "\n",
    "\n",
    "print(\"before training\")\n",
    "\"\"\"print(f\"train nRMSE : {nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)}\")\n",
    "print(f\"test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)}\")\n",
    "print(f\"val nRMSE : {nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)}\")\"\"\"\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "best_rnmse = 1000\n",
    "if os.path.exists(best_rnmse_path):\n",
    "    best_rnmse = float(torch.load(best_rnmse_path))\n",
    "    print(f\"best model loaded with rnmse : {best_rnmse}\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rt_regressor_model.train()\n",
    "    cumulative_loss = 0\n",
    "    for  batch in tqdm(train_ccd_dataloader):\n",
    "        x , y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = rt_regressor_model(x)\n",
    "        loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    #nrmse_train = nrmse_over_data(rt_regressor_model , train_ccd_dataloader ,device)\n",
    "    #nrmse_val = nrmse_over_data(rt_regressor_model , val_ccd_dataloader ,device)\n",
    "    #scheduler1.step(nrmse_val)\n",
    "\n",
    "\n",
    "    #print(f\"train epoch : {epoch +1} , loss : {cumulative_loss/len(train_ccd_dataloader)} , nRMSE : {nrmse_train}\")\n",
    "    #print(f\"val epoch : {epoch +1} , nRMSE : {nrmse_val}\")\n",
    "    rt_regressor_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        cumulative_loss = 0\n",
    "        for batch in tqdm(test_ccd_dataloader):\n",
    "            x , y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = rt_regressor_model(x)\n",
    "            loss = loss_f(y_pred.squeeze(-1) , y)\n",
    "            cumulative_loss += loss.item()\n",
    "        nrmse_over_test = nrmse_over_data(rt_regressor_model , test_ccd_dataloader ,device)\n",
    "        print(f\"test epoch : {epoch +1} , loss : {cumulative_loss/len(test_ccd_dataloader)} , nRMSE : {nrmse_over_test}\")\n",
    "        if nrmse_over_test < best_rnmse:\n",
    "            print(f\"new best achieved nrmse {nrmse_over_test}\")\n",
    "            best_rnmse = nrmse_over_test\n",
    "            torch.save(rt_regressor_model.state_dict(), best_model_path)\n",
    "            torch.save(best_rnmse, best_rnmse_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n",
    "best_rnmse_path = \"best_rnmse.pt\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model = Vit_EEG_Encoder().to(device)\n",
    "    rt_regressor_model = Vit_EEG_RT_Decoder(model , fine_tune_encoder=False).to(device)\n",
    "    rt_regressor_model.load_state_dict(torch.load(best_model_path))\n",
    "    rt_regressor_model.eval()\n",
    "    print(\"best model loaded\")\n",
    "else:\n",
    "    print(\"best model not found\")\n",
    "    raise Exception(\"best model not found\")\n",
    "\n",
    "print(f\"r5 test nRMSE : {nrmse_over_data(rt_regressor_model , test_ccd_dataloader,device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking correlation between cls of close subjects from encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_subject_splits(data_dir: Path, seed: int = 42,\n",
    "                        test_size: float = 0.10, val_size: float = 0.10):\n",
    "    \"\"\"\n",
    "    Split subjects listed in full_meta_data.csv (column 'participant_id')\n",
    "    into train/val/test groups.\n",
    "    \"\"\"\n",
    "    meta_path = data_dir / \"full_meta_data.csv\"\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "\n",
    "    df = pd.read_csv(meta_path)\n",
    "    if \"participant_id\" not in df.columns:\n",
    "        raise ValueError(\"full_meta_data.csv must contain 'participant_id' column\")\n",
    "\n",
    "    subjects = sorted(df[\"participant_id\"].unique())\n",
    "\n",
    "    # first split off temp (val + test)\n",
    "    train_subj, temp_subj = train_test_split(\n",
    "        subjects,\n",
    "        test_size=(test_size + val_size),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # now split temp into val and test\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    val_subj, test_subj = train_test_split(\n",
    "        temp_subj,\n",
    "        test_size=(1 - val_ratio),\n",
    "        random_state=seed,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_subj,\n",
    "        \"val\": val_subj,\n",
    "        \"test\": test_subj\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, bisect\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EEGShardDataset_FULL(Dataset):\n",
    "    \"\"\"\n",
    "    Shards: each file is [n_base, C=129, W=200].\n",
    "    We create *virtual* windows of length `window_size` with step `stride`\n",
    "    inside each shard (NO cross-shard windows). Return is SAME as original:\n",
    "      -> (np.ndarray [C, window_size], info: dict)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        subject_list: List[str],\n",
    "        window_size: int = 200,\n",
    "        stride: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.shards_dir = self.data_dir / \"processed_shards\"\n",
    "        self.subject_list = set(subject_list)\n",
    "        self.window_size = int(window_size)\n",
    "        self.stride = int(stride)\n",
    "\n",
    "        # ---------- metadata + subject filtering ----------\n",
    "        meta_path = self.data_dir / \"full_meta_data.csv\"\n",
    "        if not meta_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "\n",
    "        REQUIRED = [\"age\", \"ehq_total\", \"p_factor\", \"attention\",\n",
    "                    \"internalizing\", \"externalizing\", \"sex\"]\n",
    "\n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "        # 1. drop any row missing a required column\n",
    "        clean_df = meta_df.dropna(subset=REQUIRED)\n",
    "        # 2. build clean subject set\n",
    "        clean_subjects = set(clean_df[\"participant_id\"].unique())\n",
    "        # 3. intersect with caller list\n",
    "        self.subject_list = set(self.subject_list) & clean_subjects\n",
    "        if not self.subject_list:\n",
    "            raise RuntimeError(\"No subjects left after NaN filter.\")\n",
    "        # 4. fast lookup dict (only clean rows)\n",
    "        self.meta_dict: Dict[str, Dict] = (\n",
    "            clean_df\n",
    "            .loc[clean_df[\"participant_id\"].isin(self.subject_list)]\n",
    "            .set_index(\"participant_id\")\n",
    "            .to_dict(orient=\"index\")\n",
    "        )\n",
    "        print(f\"Kept {len(self.subject_list)} subjects with complete meta.\")\n",
    "        # ---- discover shards filtered by subject ----\n",
    "        all_files = sorted(f for f in os.listdir(self.shards_dir) if f.endswith(\".npy\"))\n",
    "        self.shard_paths: List[Path] = []\n",
    "        for fname in all_files:\n",
    "            if any(subj in fname for subj in self.subject_list):\n",
    "                self.shard_paths.append(self.shards_dir / fname)\n",
    "        if not self.shard_paths:\n",
    "            raise RuntimeError(\"No shard files after subject filtering.\")\n",
    "\n",
    "        # ---- index virtual windows per shard ----\n",
    "        self.shard_meta = []          # dicts: path, n_base, C, W, n_virtual, fname\n",
    "        self.prefix_virtual = [0]     # prefix sum of n_virtual\n",
    "        total_virtual = 0\n",
    "\n",
    "        for path in tqdm(self.shard_paths, desc=\"Indexing shards (strided)\"):\n",
    "            arr = np.load(path, mmap_mode=\"r\")           # [n_base, C, W]\n",
    "            if arr.ndim != 3:\n",
    "                raise ValueError(f\"{path.name}: expected 3D [n, C, W], got {arr.shape}\")\n",
    "            n_base, C, W = arr.shape\n",
    "            if C != 129:\n",
    "                raise ValueError(f\"{path.name}: C={C}, expected 129\")\n",
    "            total_samples = n_base * W\n",
    "            if total_samples >= self.window_size:\n",
    "                n_virtual = ((total_samples - self.window_size) // self.stride) + 1\n",
    "            else:\n",
    "                n_virtual = 0\n",
    "            self.shard_meta.append({\n",
    "                \"path\": path,\n",
    "                \"n_base\": int(n_base),\n",
    "                \"C\": int(C),\n",
    "                \"W\": int(W),\n",
    "                \"n_virtual\": int(n_virtual),\n",
    "                \"fname\": path.stem,\n",
    "            })\n",
    "            total_virtual += n_virtual\n",
    "            self.prefix_virtual.append(total_virtual)\n",
    "\n",
    "        self.total_virtual = total_virtual\n",
    "        if self.total_virtual == 0:\n",
    "            raise RuntimeError(\"No virtual windows possible with given window_size/stride.\")\n",
    "        print(f\"✅ Indexed {len(self.shard_meta)} shards -> {self.total_virtual} virtual windows (stride={self.stride})\")\n",
    "\n",
    "        # tiny cache\n",
    "        self._cache_key = None\n",
    "        self._cache_arr = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_virtual\n",
    "\n",
    "    def _locate(self, global_index: int) -> Tuple[int, int]:\n",
    "        s = bisect.bisect_right(self.prefix_virtual, global_index) - 1\n",
    "        local_idx = global_index - self.prefix_virtual[s]\n",
    "        return s, local_idx\n",
    "\n",
    "    def _load_shard(self, shard_idx: int):\n",
    "        path = self.shard_meta[shard_idx][\"path\"]\n",
    "        key = str(path)\n",
    "        if self._cache_key != key:\n",
    "            self._cache_arr = np.load(path, mmap_mode=\"r\")  # [n_base, C, W]\n",
    "            self._cache_key = key\n",
    "        return self._cache_arr\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index >= self.total_virtual:\n",
    "            raise IndexError\n",
    "\n",
    "        shard_idx, local_idx = self._locate(index)\n",
    "        meta = self.shard_meta[shard_idx]\n",
    "        shard = self._load_shard(shard_idx)\n",
    "\n",
    "        n_base, C, W = meta[\"n_base\"], meta[\"C\"], meta[\"W\"]\n",
    "\n",
    "        # start/end in shard's concatenated timeline\n",
    "        start = local_idx * self.stride\n",
    "        end   = start + self.window_size\n",
    "\n",
    "        # stitch within shard (may span adjacent base windows IN THE SAME SHARD)\n",
    "        base_idx = start // W\n",
    "        offset   = start % W\n",
    "        remaining = self.window_size\n",
    "        pieces = []\n",
    "        cur_b = base_idx\n",
    "        cur_off = offset\n",
    "\n",
    "        while remaining > 0:\n",
    "            take = min(remaining, W - cur_off)\n",
    "            if cur_b >= n_base:\n",
    "                piece = np.zeros((C, take), dtype=np.float32)\n",
    "            else:\n",
    "                piece = shard[cur_b, :, cur_off:cur_off + take]  # [C, take]\n",
    "            pieces.append(piece)\n",
    "            remaining -= take\n",
    "            cur_b += 1\n",
    "            cur_off = 0\n",
    "\n",
    "        x = np.concatenate(pieces, axis=1)                 # [C, window_size]\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "\n",
    "        fname = meta[\"fname\"]\n",
    "        parts = fname.split(\"_\")\n",
    "        subject = parts[2] if len(parts) > 2 else \"unknown\"\n",
    "        info = {\n",
    "            \"release\": parts[0] if len(parts) > 0 else \"\",\n",
    "            \"task\":    parts[1] if len(parts) > 1 else \"\",\n",
    "            \"subject\": subject,\n",
    "            \"run\":     parts[3] if len(parts) > 3 else \"\",\n",
    "        }\n",
    "        if subject in self.meta_dict:\n",
    "            info.update(self.meta_dict[subject])\n",
    "\n",
    "        return x, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 1503 subjects with complete meta.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing shards (strided): 100%|██████████| 9655/9655 [01:46<00:00, 90.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 9655 shards -> 2708259 virtual windows (stride=100)\n",
      "Kept 187 subjects with complete meta.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing shards (strided): 100%|██████████| 1187/1187 [00:13<00:00, 89.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 1187 shards -> 331249 virtual windows (stride=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# subject splits\n",
    "FINAL_DATA_DIR = Path(r\"C:\\disque d\\ai_stuff\\projects\\pytorchtraining\\eeg_competition\\final_kaggle_data2\")\n",
    "\n",
    "splits = make_subject_splits(FINAL_DATA_DIR, seed=1337)\n",
    "\n",
    "# datasets\n",
    "train_ds = EEGShardDataset_FULL(FINAL_DATA_DIR, splits[\"train\"])\n",
    "val_ds   = EEGShardDataset_FULL(FINAL_DATA_DIR, splits[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331249/331249 [00:24<00:00, 13722.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DespicableMe', 'DiaryOfAWimpyKid', 'FunwithFractals', 'RestingState', 'ThePresent', 'contrastChangeDetection', 'surroundSupp', 'symbolSearch', 'seqLearning6target', 'seqLearning8target']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tasks =[]\n",
    "for x, infor in tqdm(val_ds):\n",
    "    if infor['task'] not in tasks:\n",
    "        tasks.append(infor['task'])\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing guided ema model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Vit_EEG_Embedding(nn.Module):\n",
    "    def __init__(self, nb_tokens, c_dim=129, t_dim=200, slice_size=10,\n",
    "                target_c_dim=64, target_t_dim=6, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.slice_size = slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.time_projection = nn.Linear(self.slice_size, self.target_t_dim)\n",
    "        self.channel_projection = nn.Linear(self.c_dim, self.target_c_dim)\n",
    "\n",
    "        self.time_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, 1, self.target_t_dim)\n",
    "        )\n",
    "        self.channel_positional_emb = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.target_c_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.token_projection = nn.Linear(\n",
    "            self.target_c_dim * self.target_t_dim, self.emb_dim\n",
    "        )\n",
    "        self.token_positional_emb = nn.Parameter(\n",
    "            torch.zeros(self.nb_tokens+1, self.emb_dim)\n",
    "        )\n",
    "\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.zeros(1, 1, self.emb_dim)\n",
    "        )\n",
    "        self.CLS = nn.Parameter(torch.zeros(1, 1, self.emb_dim))\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(self.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.token_positional_emb)\n",
    "        nn.init.xavier_uniform_(self.CLS)\n",
    "\n",
    "\n",
    "\n",
    "    def zscore_bct(self ,x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Per-window, per-channel z-score along time: (B, C, T) -> (B, C, T).\n",
    "        \"\"\"\n",
    "        if not x.is_floating_point():\n",
    "            x = x.float()\n",
    "        mu = x.mean(dim=2, keepdim=True)\n",
    "        sigma = x.std(dim=2, keepdim=True, unbiased=False).clamp_min(eps)\n",
    "        return (x - mu) / sigma\n",
    "\n",
    "    def patching (self,x : Tensor):\n",
    "        # x starting shape (B , C , T)\n",
    "        x = x.reshape(x.shape[0] , x.shape[1] ,   x.shape[2] // self.slice_size , self.slice_size)\n",
    "        x = x.permute(0 , 2 , 1 , 3)\n",
    "        return x\n",
    "    def forward(self,x ,mask_index =  None):\n",
    "        \n",
    "        # x starting shape (B , C , T)\n",
    "        # normilizing\n",
    "        x = self.zscore_bct(x)\n",
    "        x = self.patching(x)\n",
    "        if mask_index is not None:\n",
    "            origin = x.clone()\n",
    "        # x (B ,N , C , slice)  N = T // slice\n",
    "        x = self.time_projection(x)\n",
    "        # x (B , N , C , Target_t_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_t_dim , C)\n",
    "        x = self.channel_projection(x)\n",
    "        # x (B , N , Target_t_dim , Target_c_dim)\n",
    "        x = x.permute(0 , 1 , 3 , 2)\n",
    "        # x (B , N , Target_c_dim , Target_t_dim)\n",
    "        x = x + self.time_positional_emb\n",
    "        x = x + self.channel_positional_emb\n",
    "        # Flatten \n",
    "        x= x.reshape(x.shape[0] , x.shape[1] , x.shape[2] * x.shape[3])\n",
    "        # x (B , N , Target_c_dim * Target_t_dim)\n",
    "        x = self.token_projection(x)\n",
    "        if mask_index is not None:\n",
    "            r = mask_index.shape[1]\n",
    "            batch_index = torch.arange(x.shape[0],device=x.device).unsqueeze(1).expand(x.shape[0] , r)\n",
    "            x[batch_index , mask_index ] = self.mask_token.expand(x.shape[0] , r , x.shape[2])\n",
    "        # x (B , N+1 , emb_dim)\n",
    "        x= torch.concat([self.CLS.expand(x.shape[0] , 1 , x.shape[2]) , x] , dim=1)\n",
    "        x = x + self.token_positional_emb\n",
    "        x = self.pre_norm(x)\n",
    "        if mask_index is not None:\n",
    "            return x , origin\n",
    "        return x \n",
    "class Vit_EEG_Encoder(nn.Module):\n",
    "    def __init__(self,c_dim = 129  ,  t_dim = 200   , slice_size = 10 , emb_dim = 512 , nhead = 8 ,  nb_layers = 12  , target_c_dim = 64 , target_t_dim = 6 ):\n",
    "        super().__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.slice_size = slice_size\n",
    "        assert self.t_dim % self.slice_size == 0\n",
    "        self.nb_tokens = self.t_dim // self.slice_size\n",
    "        self.target_c_dim = target_c_dim\n",
    "        self.target_t_dim = target_t_dim\n",
    "        self.nhead = nhead\n",
    "        self.nb_layers = nb_layers\n",
    "        self.embedding = Vit_EEG_Embedding(nb_tokens=self.nb_tokens , c_dim = self.c_dim , t_dim = self.t_dim , slice_size = self.slice_size,target_c_dim=self.target_c_dim ,target_t_dim=self.target_t_dim  , emb_dim = self.emb_dim )\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim , nhead=self.nhead , dim_feedforward=emb_dim*4 , batch_first=True , norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer , num_layers=self.nb_layers ,enable_nested_tensor=False)\n",
    "        self.mask_decorder = nn.Sequential(\n",
    "        nn.LayerNorm(emb_dim),\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(emb_dim * 4, self.c_dim * self.slice_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _make_mask(self, x , mask_ratio=0.2):\n",
    "        # x: (B, C, T)\n",
    "        k = max(1, int(self.nb_tokens * mask_ratio))\n",
    "\n",
    "        scores = torch.rand(x.shape[0] , self.nb_tokens , device=x.device)\n",
    "        _ , masked_indexs = torch.topk(scores , k=k , dim=1)\n",
    "\n",
    "        return   masked_indexs\n",
    "\n",
    "    def extract_features(self,x):   \n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "    def forward(self,x  ,task = None ,  mask_index =None):\n",
    "\n",
    "        if task == \"demasking\":\n",
    "            emb_x , targets=  self.embedding(x ,   mask_index)\n",
    "\n",
    "            transformed = self.transformer_encoder(emb_x)\n",
    "            # (B , N+1 , EMB_DIM)\n",
    "            features_without_cls = transformed[:, 1:, :]\n",
    "            # (b , N , EMB_DIM)\n",
    "            result = self.mask_decorder(features_without_cls)\n",
    "            # (B , N , C * slice)\n",
    "            return result , targets\n",
    "\n",
    "\n",
    "        else :\n",
    "            return self.extract_features(x)\n",
    "\n",
    "\n",
    "class RT_head (nn.Module):\n",
    "    def __init__(self , emb_dim = 512):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim , self.emb_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.emb_dim * 2 , self.emb_dim ) ,\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.emb_dim , 1)\n",
    "\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- End-of-epoch: NRMSE over data (val set) & best save ---\n",
    "class _RTWrapper(nn.Module):\n",
    "    def __init__(self, encoder, head):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)         # (B, N+1, E)\n",
    "        cls = feats[:, 0]\n",
    "        return self.head(cls).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_encoder_path    = os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_encoder.pt\")\n",
    "best_rt_head_path    = os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_rt_head.pt\")\n",
    "encoder = Vit_EEG_Encoder()\n",
    "head = RT_head()\n",
    "\n",
    "encoder.load_state_dict(torch.load(best_encoder_path))\n",
    "head.load_state_dict(torch.load(best_rt_head_path))\n",
    "model = _RTWrapper(encoder, head).cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_RTWrapper(\n",
       "  (encoder): Vit_EEG_Encoder(\n",
       "    (embedding): Vit_EEG_Embedding(\n",
       "      (time_projection): Linear(in_features=10, out_features=6, bias=True)\n",
       "      (channel_projection): Linear(in_features=129, out_features=64, bias=True)\n",
       "      (token_projection): Linear(in_features=384, out_features=512, bias=True)\n",
       "      (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_decorder): Sequential(\n",
       "      (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=2048, out_features=1290, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head): RT_head(\n",
       "    (regressor): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 118/183 [00:13<00:07,  8.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_nrmse = \u001b[43mnrmse_over_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_ccd_dataloader\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m val_nrmse = nrmse_over_data(model, val_ccd_dataloader , device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest NRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_nrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mnrmse_over_data\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     10\u001b[39m     index = \u001b[32m0\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\disque d\\ai_stuff\\projects\\pytorchtraining\\deeplearning_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mEEGDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     24\u001b[39m raw = X[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m     25\u001b[39m rt = Y[index % \u001b[38;5;28mself\u001b[39m.shard_size]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tensor_raw = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     27\u001b[39m tensor_rt = torch.tensor(rt , dtype=torch.float32).to(device)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_raw , tensor_rt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test_nrmse = nrmse_over_data(model,test_ccd_dataloader  , device)\n",
    "val_nrmse = nrmse_over_data(model, val_ccd_dataloader , device)\n",
    "print(f\"Test NRMSE: {test_nrmse:.4f}\")\n",
    "print(f\"Val NRMSE: {val_nrmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_encoder_path    = os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_encoder_943.pt\")\n",
    "best_rt_head_path    = os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_rt_head943.pt\")\n",
    "best_model_path = os.path.join(MODELS_AND_CHECKPOINTS_PATH, \"best_model_943.pt\")\n",
    "torch.save(model.state_dict(), best_model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
